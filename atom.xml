<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>落雨乄天珀夜</title>
  
  
  <link href="https://conqueror712.github.io/atom.xml" rel="self"/>
  
  <link href="https://conqueror712.github.io/"/>
  <updated>2023-07-11T16:22:34.264Z</updated>
  <id>https://conqueror712.github.io/</id>
  
  <author>
    <name>落雨乄天珀夜</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>文本提取、文本分类、语言筛选，有什么好用的工具？丨学习记录</title>
    <link href="https://conqueror712.github.io/post/FastText.html"/>
    <id>https://conqueror712.github.io/post/FastText.html</id>
    <published>2023-07-11T06:06:45.000Z</published>
    <updated>2023-07-11T16:22:34.264Z</updated>
    
    <content type="html"><![CDATA[<p>📕环境：macOS or Linux</p><p>🍀本文代码已开源：</p><p><a href="https://github.com/Conqueror712/Data-Cleaning-Practice">https://github.com/Conqueror712/Data-Cleaning-Practice</a></p><h1 id="Trafilature文本提取"><a href="#Trafilature文本提取" class="headerlink" title="Trafilature文本提取"></a>Trafilature文本提取</h1><p>目的：更有效地提取网页中的有用的内容，重点关注有意义的、结构化的文本，有助于模型更好地理解和生成具有特定格式和结构的文本</p><p>工具：trafilatura（RefinedWeb使用的）</p><p>官方文档：<a href="https://trafilatura.readthedocs.io/en/latest/">https://trafilatura.readthedocs.io/en/latest/</a></p><blockquote><p>下面给出trafilatura的使用demo（命令行版本）：</p></blockquote><p>安装：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install trafilature</span><br></pre></td></tr></table></figure><p>使用：以<a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM">https://github.com/HqWu-HITCS/Awesome-Chinese-LLM</a>举例</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trafilatura -u &quot;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&quot;</span><br></pre></td></tr></table></figure><p>输出：可以看到有一些不重要的内容被忽略掉了</p><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aeb578dc8663473285232c40c91616de~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>非命令行版本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import trafilatura</span><br><span class="line"></span><br><span class="line">downloaded = trafilatura.fetch_url(&#x27;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&#x27;)</span><br><span class="line"></span><br><span class="line">print(trafilatura.extract(downloaded))</span><br></pre></td></tr></table></figure><h1 id="Langdetect语言筛选"><a href="#Langdetect语言筛选" class="headerlink" title="Langdetect语言筛选"></a>Langdetect语言筛选</h1><p>目的：构造一个纯英文的数据集</p><p>工具：langdetect（C4使用的）、CCNet的分类器（RefinedWeb使用的）</p><p>官方文档：<a href="https://pypi.org/project/langdetect/">https://pypi.org/project/langdetect/</a></p><blockquote><p>下面给出langdetect的使用demo：</p></blockquote><p>安装：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langdetect</span><br></pre></td></tr></table></figure><p>使用：</p><p>To detect the language of the text:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from langdetect import detect</span><br><span class="line">from langdetect import detect_langs</span><br><span class="line">print(detect(&quot;War doesn&#x27;t show who&#x27;s right, just who&#x27;s left.&quot;))</span><br><span class="line">print(detect(&quot;Ein, zwei, drei, vier&quot;))</span><br></pre></td></tr></table></figure><p>To find out the probabilities for the top languages:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(detect_langs(&quot;Otec matka syn.&quot;))</span><br></pre></td></tr></table></figure><p>输出：注意，这个概率的结果每次都不尽相同</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b537b44b5aac4cfc8988d5e4af86adf7~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><h1 id="FastText文本分类"><a href="#FastText文本分类" class="headerlink" title="FastText文本分类"></a>FastText文本分类</h1><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a65f02cd1a2441d8b66853ea2fd92158~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><h2 id="一、词表示模型Word-Representation-Model"><a href="#一、词表示模型Word-Representation-Model" class="headerlink" title="一、词表示模型Word Representation Model"></a>一、词表示模型Word Representation Model</h2><p>之前说了很多内容，但是理论总要和实践相结合，我们不妨来测试一下词表示模型Word Representation Model：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import fasttext</span><br><span class="line"></span><br><span class="line">Skipgram model:</span><br><span class="line"></span><br><span class="line">model1 = fasttext.train_unsupervised(&#x27;data.txt&#x27;, model=&#x27;skipgram&#x27;)</span><br><span class="line"></span><br><span class="line">cbow model:</span><br><span class="line"></span><br><span class="line">model2 = fasttext.train_unsupervised(&#x27;data.txt&#x27;, model=&#x27;cbow&#x27;)</span><br><span class="line">print(model1.words)  # list of words in dictionary</span><br><span class="line">print(model2[&#x27;care&#x27;])  # get the vector of the word &#x27;care&#x27;</span><br></pre></td></tr></table></figure><p>这时候，有趣的事情出现了，当我的data.txt的数据量很小的时候，会出现报错：</p><p>Empty vocabulary. Try a smaller -minCount value.</p><p>此时，我的data.txt里面的内容只有一行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hi, this is a test, don&#x27;t be care!</span><br></pre></td></tr></table></figure><p>当我增加到两行，三行…无济于事；但就当我增加到五行的时候，事情发生了转变，结果被正确的输出出来了，如图所示：</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/64ae6d2dc9a4406b906ef14fe1a446c0~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>另外，使用<code>model1.save_model(“ListModel_demo.bin”)</code>可以将训练出来的模型保存起来，以便下次使用：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model3 = fasttext.load_model(&#x27;ListModel_demo.bin&#x27;)</span><br><span class="line">print(model3.words)</span><br></pre></td></tr></table></figure><hr><h2 id="二、文本分类模型Text-Classfication-Model"><a href="#二、文本分类模型Text-Classfication-Model" class="headerlink" title="二、文本分类模型Text Classfication Model"></a>二、文本分类模型Text Classfication Model</h2><p>接下来测试一下文本分类模型Text Classification Model，有如下代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import fasttext</span><br><span class="line">model = fasttext.train_supervised(&#x27;data.train.txt&#x27;)</span><br><span class="line">print(model.words)</span><br><span class="line">print(model.labels)</span><br></pre></td></tr></table></figure><p>此时我的data.train.txt是这样的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__label__ apple pineapple</span><br><span class="line">__label__ peer</span><br><span class="line">watermelon</span><br></pre></td></tr></table></figure><p>而输出结果如下：</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cc38c05d534949e9a40ed3b17d6b9bd4~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>更进一步地，我们可以看一看指标：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import fasttext</span><br><span class="line">model = fasttext.train_supervised(&#x27;data.train.txt&#x27;)</span><br><span class="line">print(model.words)</span><br><span class="line">print(model.labels)</span><br><span class="line">def print_results(N, p, r):</span><br><span class="line">    print(&quot;N\t&quot; + str(N))</span><br><span class="line">    print(&quot;P@&#123;&#125;\t&#123;:.3f&#125;&quot;.format(1, p))</span><br><span class="line">    print(&quot;R@&#123;&#125;\t&#123;:.3f&#125;&quot;.format(1, r))</span><br></pre></td></tr></table></figure><p>To evaluate our model by computing the precision at 1 (P@1) and the recall on a test set, we use the test function</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print_results(*model.test(&#x27;test.txt&#x27;))</span><br><span class="line"></span><br><span class="line">predict labels for a specific text</span><br><span class="line"></span><br><span class="line">print(model.predict(“Which baking dish is best to bake a banana bread ?”))</span><br><span class="line"></span><br><span class="line">predict more than one label by specifying the parameter k</span><br><span class="line"></span><br><span class="line">print(model.predict(“Which baking dish is best to bake a banana bread ?&quot;, k=3))</span><br></pre></td></tr></table></figure><p>predict more than one sentence</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(model.predict([“Which baking dish is best to bake a banana bread ?&quot;, &quot;Why not put knives in the dishwasher?&quot;], k=3))</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a6073458a8b44643a1c405cf3de08dde~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><hr><h2 id="三、压缩模型Quantize"><a href="#三、压缩模型Quantize" class="headerlink" title="三、压缩模型Quantize"></a>三、压缩模型Quantize</h2><p>如果按照上面的方法，做出来的.bin模型文件超级大，怎样缩小又不失准确性呢？我们使用quantize方法，只需要添加上这些代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">with the previously trained `model` object, call</span><br><span class="line"></span><br><span class="line">model.quantize(input=&#x27;data.train.txt&#x27;, retrain=True)</span><br><span class="line"></span><br><span class="line">then display results and save the new model</span><br><span class="line"></span><br><span class="line">print_results(*model.test(&#x27;test.txt&#x27;))</span><br><span class="line">model.save_model(&quot;CompressedModel_demo.ftz&quot;)</span><br></pre></td></tr></table></figure><p>这里有一点需要注意的是，如果数据集过小，就会失败，有如下报错：</p><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/162262eda655416d9f49c14b6a807943~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>为此，我们需要更大一点data.train.txt，而不能仅仅是手打的两行字。</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/382050b0a8fb40ce83c81f52885d1479~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>很好，这次我们成功了！</p><p>我们不妨来对比一下压缩前后的模型大小区别：</p><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2d3ae2edc62840c79c8cbb18b3950a54~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>如此悬殊的空间差距！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;📕环境：macOS or Linux&lt;/p&gt;
&lt;p&gt;🍀本文代码已开源：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/Conqueror712/Data-Cleaning-Practice&quot;&gt;https://github.com/Conquero</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>CUDA性能分析丨ltrace和nvprof工具如何使用？</title>
    <link href="https://conqueror712.github.io/post/nvprof.html"/>
    <id>https://conqueror712.github.io/post/nvprof.html</id>
    <published>2023-07-09T11:15:02.000Z</published>
    <updated>2023-07-09T11:17:06.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="零、前言："><a href="#零、前言：" class="headerlink" title="零、前言："></a>零、前言：</h1><p>此demo的Github仓库（持续更新）：<a href="https://github.com/Conqueror712/CUDA-Practice">https://github.com/Conqueror712/CUDA-Practice</a></p><p>📕<strong>欢迎访问</strong>：</p><blockquote><p>个人博客：<a href="https://link.juejin.cn/?target=https://conqueror712.github.io/">conqueror712.github.io&#x2F;</a></p><p>知乎：<a href="https://link.juejin.cn/?target=https://www.zhihu.com/people/soeur712/posts">www.zhihu.com/people/soeu…</a></p><p>Bilibili：<a href="https://link.juejin.cn/?target=https://space.bilibili.com/57089326">space.bilibili.com&#x2F;57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">juejin.cn&#x2F;user&#x2F;129787…</a></p></blockquote><p>有任何疏忽和错误欢迎各位读者指出！</p><h1 id="一、CUDA是什么"><a href="#一、CUDA是什么" class="headerlink" title="一、CUDA是什么"></a>一、CUDA是什么</h1><p>CUDA是一种通用的并行计算平台和编程模型，是在C语言基础上扩展的。</p><img src="https://cdnjson.com/images/2023/06/23/image508394046c4e1268.png" alt="avatar" style="zoom:67%;" /><p>CUDA程序的执行过程：</p><ol><li><strong>分配</strong>CPU内存，并进行数据<strong>初始化</strong>；</li><li><strong>分配</strong>GPU内存，把数据从CPU内存<strong>拷贝</strong>到GPU内存；</li><li>调用<strong>核函数</strong>对存储在GPU内存中的数据进行运算；</li><li>将数据s从GPU内存<strong>拷贝</strong>到CPU内存；</li><li><strong>释放</strong>CPU和GPU上分配的内存。</li></ol><p>GPU需要与CPU协同工作才行，所以说GPU并行计算，事实上指的是基于CPU+GPU的异构计算架构，GPU和CPU通过PCIe总线连接在一起来协同工作，示意图如下：</p><p><img src="https://pic3.zhimg.com/v2-df49a98a67c5b8ce55f1a9afcf21d982_r.jpg" alt="avatar"></p><p>我们发现，GPU有着更多的ALU（绿色部分），所以很适合数据并行的计算密集型任务，例如张量运算等。</p><p>CPU没有很多ALU，但是可以实现复杂的逻辑运算适合控制密集型任务。</p><h1 id="二、CUDA编程模型的两大特色"><a href="#二、CUDA编程模型的两大特色" class="headerlink" title="二、CUDA编程模型的两大特色"></a>二、CUDA编程模型的两大特色</h1><h2 id="1-通过层次结构组织线程"><a href="#1-通过层次结构组织线程" class="headerlink" title="1 通过层次结构组织线程"></a>1 通过层次结构组织线程</h2><p>说到线程，我们先来明确一下进程、线程和协程的区别：</p><blockquote><p>进程：是并发执行的程序在执行过程中分配和管理资源的基本单位，是一个动态概念，竞争计算机系统资源的基本单位。</p><p>线程：是进程的一个执行单元，是进程内科调度实体。比进程更小的独立运行的基本单位。线程也被称为轻量级进程。</p><p>协程：是一种比线程更加轻量级的存在。一个线程也可以拥有多个协程。其执行过程更类似于子例程，或者说不带返回值的函数调用。</p></blockquote><p>如果是对于CPU而言，一个核大概只能支持1~2个硬件线程，但是对于GPU而言，在硬件层面上就能同时支持千百个并发线程，在GPU编程的时候需要考虑到这一点，以提高运行效率。</p><p>在CUDA编程中，线程的管理层次有如下4种：</p><ul><li>线程网络Grid</li><li>线程块Block</li><li>线程束Warp</li><li>线程Thread</li></ul><p>当核函数在主机端也就是CPU启动时，其执行会移动到设备GPU上，此时设备GPU中会产生大量的线程并且每个线程都执行由核函数指定的语句。</p><h2 id="2-通过层次结构组织内存"><a href="#2-通过层次结构组织内存" class="headerlink" title="2 通过层次结构组织内存"></a>2 通过层次结构组织内存</h2><p>如何组织内存和使用内存，是提高效率的关键，GPU中的各级缓存以及各种内存是可以软件控制的，在编程时我们可以手动指定变量存储的位置。</p><p>内存会有很多种，主要有：</p><ul><li>寄存器</li><li>共享内存</li><li>常量内存</li><li>全局内存</li></ul><p>为了发挥这些不同的内存的功效，在CUDA编程中会有很多的little tips，例如：</p><ul><li>尽量使用寄存器</li><li>尽量将数据声明为局部变量</li><li>在重复利用某一数据时将其放入共享内存中</li><li>注意合并访问某些全局数据以减少访问需要的次数</li></ul><blockquote><p>GPU内存层次结构的示意图如下：</p><p><img src="https://pic2.zhimg.com/80/v2-6456af75530956da6bc5bab7418ff9e5_720w.webp" alt="avatar"></p><p>一些说明：</p><ul><li>寄存器Registers，带宽&#x3D;8TB&#x2F;s，delay&#x3D;1时钟周期，特点是快。</li><li>共享内存Shared Memory，带宽&#x3D;1.5TB&#x2F;s，delay&#x3D;1~32时钟周期，特点是可受用户控制。</li><li>全局内存Global Memory，是GPU中最大、延迟最高并且最常使用的内存。</li></ul></blockquote><h1 id="三、Kernel核函数"><a href="#三、Kernel核函数" class="headerlink" title="三、Kernel核函数"></a>三、Kernel核函数</h1><ul><li>核函数用<code>__global__</code>符号声明；</li><li>核函数在调用时要用<code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>来指定Kernel要执行的线程数量；</li><li>每个线程都要执行Kernel，并且会分配到一个唯一的线程号<code>thread ID</code>，其值可通过核函数的内置变量<code>threadIdx</code>来获得。</li></ul><p>因为GPU是异构模型，所以在CUDA中通过函数类型限定词区分host和device上的函数：</p><ul><li><code>__global__</code>：在device上执行，从host中调用，返回类型必须是<code>void</code>，不支持可变参数，不能成为类成员函数。注意用<code>__global__</code>定义的kernel是异步的，这意味着host不会等待kernel执行完就执行下一步；</li><li><code>__device__</code>：在device上执行，仅可以从device中调用，不可以和<code>__global__</code>同时用；</li><li><code>__host__</code>：在host上执行，仅可以从host上调用，一般省略不写，不可以和<code>__global__</code>同时用，但可和<code>__device__</code>，此时函数会在device和host都编译。</li></ul><blockquote><p>2-dim线程的组织结构示意图如下：</p></blockquote><p><img src="https://pic1.zhimg.com/80/v2-aa6aa453ff39aa7078dde59b59b512d8_720w.webp" alt="avatar"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 缺省的值初始化为1，此处由于使用dim3，包含x, y, z三个维度，故有一个维度缺省</span><br><span class="line">dim3 grid(3, 2);</span><br><span class="line">dim3 block(5, 3);</span><br><span class="line">kernel_fun&lt;&lt;&lt; grid, block &gt;&gt;&gt;(prams...);</span><br></pre></td></tr></table></figure><blockquote><p>一些说明：</p></blockquote><ul><li><p>同一网格Grid中的所有线程共享相同的全局内存空间；</p></li><li><p>线程网格和线程块从逻辑上代表了一个核函数的线程层次结构，这种组织方式可以帮助我们有效地利用资源，优化性能。</p></li><li><p>一个线程需要两个内置的坐标变量<code>blockIdx</code>&amp;<code>threadIdx</code>来唯一标识，它们都是<code>dim3</code>类型变量，其中<code>blockIdx</code>指明线程所在<code>grid</code>中的位置，而<code>threaIdx</code>指明线程所在<code>block</code>中的位置，如图中的<code>Thread(1,1)</code>满足：</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">threadIdx.x = 1</span><br><span class="line">threadIdx.y = 1</span><br><span class="line">blockIdx.x = 1</span><br><span class="line">blockIdx.y = 1</span><br></pre></td></tr></table></figure></li></ul><h1 id="四、SM流式多处理器"><a href="#四、SM流式多处理器" class="headerlink" title="四、SM流式多处理器"></a>四、SM流式多处理器</h1><p>一个Block上的线程是放在同一个流式多处理器SM上的，但是单个SM的资源有限，这导致线程块中的线程数是有限制的，现代GPUs的线程块可支持的线程数可达1024个。</p><p>SM是GPU硬件的一个核心组件，SM的核心组件又包括CUDA核心，共享内存，寄存器等，SM可以并发地执行数百个线程，并发能力就取决于SM所拥有的资源数。</p><p>当一个kernel被执行时，它的gird中的线程块被分配到SM上。</p><p>要注意的是，一个线程块只能在一个SM上被调度，但SM一般可以调度多个线程块。有可能一个kernel的各个线程块被分配多个SM，所以说grid只是逻辑层，而SM才是执行的物理层。</p><blockquote><p>逻辑层和物理层的示意图：</p><p><img src="https://pic1.zhimg.com/80/v2-dcc0f678850d5bf1683753c34ca4b308_720w.webp" alt="avatar"></p></blockquote><p>由于SM的基本执行单元是包含32个线程的线程束，所以block大小一般要设置为32的倍数。</p><h1 id="五、ltrace-amp-strace简介"><a href="#五、ltrace-amp-strace简介" class="headerlink" title="五、ltrace &amp; strace简介"></a>五、ltrace &amp; strace简介</h1><h2 id="1-ltrace"><a href="#1-ltrace" class="headerlink" title="1 ltrace"></a>1 ltrace</h2><blockquote><p>Linux环境：</p></blockquote><p>功能：跟踪进程的库函数调用。</p><p>与strace的对比：</p><ul><li>ltrace会显现出哪个<strong>库函数被调用</strong>，而strace则是跟踪程序的每个<strong>系统调用</strong>。</li><li>ltrace与strace使用的技术大体相同，但ltrace在对支持fork和clone方面，不如strace。strace在收到fork和clone等系统调用后，做了相应的处理，而ltrace没有。</li></ul><h2 id="2-strace"><a href="#2-strace" class="headerlink" title="2 strace"></a>2 strace</h2><blockquote><p>Linux环境：</p></blockquote><ul><li>strace命令是一个集诊断、调试、统计与一体的工具，可用来追踪调试程序，能够与其他命令搭配使用。</li><li>Linux系统管理员可以在不需要源代码的情况下即可跟踪系统的调用。</li><li>strace显示有关进程的系统调用的信息，这可以帮助确定一个程序使用的哪个函数，当然在系统出现问题时可以使用 strace定位系统调用过程中失败的原因，这是定位系统问题的很好的方法。</li></ul><h2 id="为什么要用strace？"><a href="#为什么要用strace？" class="headerlink" title="为什么要用strace？"></a>为什么要用strace？</h2><ul><li>在操作系统运维中会出现程序或系统命令运行失败，通过报错和日志无法定位问题根因。</li><li>如何在没有内核或程序代码的情况下查看系统调用的过程。</li></ul><h1 id="六、ltrace-amp-nvprof实战demo"><a href="#六、ltrace-amp-nvprof实战demo" class="headerlink" title="六、ltrace &amp; nvprof实战demo"></a>六、ltrace &amp; nvprof实战demo</h1><h2 id="1-nvprof简介"><a href="#1-nvprof简介" class="headerlink" title="1 nvprof简介"></a>1 nvprof简介</h2><p>nvprof是NVIDIA提供的一种性能分析工具，用于在Linux系统上分析CUDA应用程序的性能。它可以收集应用程序在GPU上执行时的各种指标，如内存带宽、计算吞吐量、指令执行时间等，以帮助开发人员对应用程序进行性能分析和优化。</p><p>使用nvprof工具可以获取以下信息：</p><ul><li>GPU 活动摘要：可以查看 GPU 活动的总体概述，如 GPU 利用率、内存带宽、计算吞吐量等；</li><li>CUDA 核心指标：可以查看 CUDA 核心执行时间、指令执行次数、内存带宽和缓存效率等指标；</li><li>内存分配指标：可以查看内存分配和释放的次数和时间；</li><li>API 指标：可以查看 CUDA API 调用的时间和次数；</li><li>OpenACC 指标：可以查看 OpenACC 指令的执行时间和指令执行次数等。</li></ul><h2 id="2-ltrace实战"><a href="#2-ltrace实战" class="headerlink" title="2 ltrace实战"></a>2 ltrace实战</h2><blockquote><p>安装：</p></blockquote><p><code>sudo apt-get install ltrace</code></p><blockquote><p>执行：</p></blockquote><p><code>ltrace -e cuda* ./hello</code></p><h2 id="3-nvprof实战"><a href="#3-nvprof实战" class="headerlink" title="3 nvprof实战"></a>3 nvprof实战</h2><blockquote><p>安装：</p></blockquote><p><code>sudo apt install nvidia-profiler</code></p><blockquote><p>执行：</p></blockquote><p><code>nvprof -o profile.json ./hello</code></p><p>注意，我们要把C++代码里面的<code>new</code>和<code>delete</code>给删掉，换成CUDA专用的一些内存管理方式，否则不会加速。</p><p>另外，我们如果想充分利用CUDA设备的计算资源，可以使用多个线程块和多个线程，从而达到并行计算。</p><p>例如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> blockSize = <span class="number">256</span>;</span><br><span class="line"><span class="type">int</span> numBlocks = (N + blockSize - <span class="number">1</span>) / blockSize;</span><br><span class="line">add&lt;&lt;&lt;numBlocks, blockSize&gt;&gt;&gt;(N, x, y);</span><br></pre></td></tr></table></figure><p>当然，如果还有一个点不注意的话还是会寄，就像这样：</p><p><img src="https://cdnjson.com/images/2023/07/09/EBDOK_FNKVAWHZJM.png" alt="avatar"></p><p>收到信号139通常表示程序因为内存访问错误而崩溃。</p><p>如果通过调用cudaMallocManaged函数动态分配了x和y数组的内存，但是在初始化数组时，又重新定义了x和y数组，这样就导致cudaMallocManaged分配的内存被泄漏了。</p><p>要解决这个问题，您需要将初始化数组的代码删除或注释掉，只使用cudaMallocManaged函数来分配和释放内存。</p><p>修改后的代码已经放入Github仓库<a href="https://github.com/Conqueror712/CUDA-Practice">https://github.com/Conqueror712/CUDA-Practice</a></p><p>随后可以正确运行，虽然还有Error，但是不影响结果，我们将在后续解决：</p><p><img src="https://cdnjson.com/images/2023/07/09/E31FPO3B2OHFBT.png" alt="avatar"></p><p><img src="https://cdnjson.com/images/2023/07/09/I_23376WK32DBB3Y17O9.png" alt="avatar">s</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;零、前言：&quot;&gt;&lt;a href=&quot;#零、前言：&quot; class=&quot;headerlink&quot; title=&quot;零、前言：&quot;&gt;&lt;/a&gt;零、前言：&lt;/h1&gt;&lt;p&gt;此demo的Github仓库（持续更新）：&lt;a href=&quot;https://github.com/Conqueror</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>ZLUDA丨如何在Intel的GPU上运行CUDA代码？</title>
    <link href="https://conqueror712.github.io/post/ZLUDA.html"/>
    <id>https://conqueror712.github.io/post/ZLUDA.html</id>
    <published>2023-07-02T13:07:09.000Z</published>
    <updated>2023-07-02T13:09:29.767Z</updated>
    
    <content type="html"><![CDATA[<h1 id="零、前言："><a href="#零、前言：" class="headerlink" title="零、前言："></a>零、前言：</h1><p>此demo的Github仓库（持续更新）：<a href="https://github.com/Conqueror712/CUDA-Practice">https://github.com/Conqueror712/CUDA-Practice</a></p><p>📕<strong>欢迎访问</strong>：</p><blockquote><p>个人博客：<a href="https://link.juejin.cn/?target=https://conqueror712.github.io/">conqueror712.github.io&#x2F;</a></p><p>知乎：<a href="https://link.juejin.cn/?target=https://www.zhihu.com/people/soeur712/posts">www.zhihu.com/people/soeu…</a></p><p>Bilibili：<a href="https://link.juejin.cn/?target=https://space.bilibili.com/57089326">space.bilibili.com&#x2F;57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">juejin.cn&#x2F;user&#x2F;129787…</a></p></blockquote><p>有任何疏忽和错误欢迎各位读者指出！</p><h1 id="一、一切从C-开始！"><a href="#一、一切从C-开始！" class="headerlink" title="一、一切从C++开始！"></a>一、一切从C++开始！</h1><p>首先我们要得到一份C++代码，功能很简单，就是做加法：</p><p>注意这是Linux版本</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/time.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// function to add the elements of two arrays</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> n, <span class="type">float</span> *x, <span class="type">float</span> *y)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">      y[i] = x[i] + y[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="type">int</span> N = <span class="number">1</span>&lt;&lt;<span class="number">25</span>; <span class="comment">// 30M elements</span></span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> *x = <span class="keyword">new</span> <span class="type">float</span>[N];</span><br><span class="line">  <span class="type">float</span> *y = <span class="keyword">new</span> <span class="type">float</span>[N];</span><br><span class="line">  <span class="comment">// initialize x and y arrays on the host</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">    x[i] = <span class="number">1.0f</span>;</span><br><span class="line">    y[i] = <span class="number">2.0f</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">timeval</span> t1,t2;</span><br><span class="line">  <span class="type">double</span> timeuse;</span><br><span class="line">  <span class="built_in">gettimeofday</span>(&amp;t1,<span class="literal">NULL</span>);</span><br><span class="line">  <span class="comment">// Run kernel on 30M elements on the CPU</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(N, x, y)</span></span>;</span><br><span class="line">  <span class="built_in">gettimeofday</span>(&amp;t2,<span class="literal">NULL</span>);</span><br><span class="line">  timeuse = (t2.tv_sec - t1.tv_sec) + (<span class="type">double</span>)(t2.tv_usec - t1.tv_usec)/<span class="number">1000.0</span>;</span><br><span class="line"></span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;add(int, float*, float*) time: &quot;</span> &lt;&lt; timeuse &lt;&lt; <span class="string">&quot;ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">  <span class="comment">// Check for errors (all values should be 3.0f)</span></span><br><span class="line">  <span class="type">float</span> maxError = <span class="number">0.0f</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    maxError = <span class="built_in">fmax</span>(maxError, <span class="built_in">fabs</span>(y[i]<span class="number">-3.0f</span>));</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;Max error: &quot;</span> &lt;&lt; maxError &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Free memory</span></span><br><span class="line">  <span class="keyword">delete</span> [] x;</span><br><span class="line">  <span class="keyword">delete</span> [] y;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>添加一些有趣的东西就可以变成CUDA代码：</p><ul><li><code>__global__</code></li><li><code>cudaMallocManaged(&amp;x, N*sizeof(float));</code><br>  <code>cudaMallocManaged(&amp;y, N*sizeof(float));</code></li><li><code>add&lt;&lt;&lt;1, 1&gt;&gt;&gt;(N, x, y);</code></li><li><code>cudaFree(x);</code><br>  <code>cudaFree(y);</code></li></ul><h2 id="1-Linux版本："><a href="#1-Linux版本：" class="headerlink" title="1. Linux版本："></a>1. Linux版本：</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/time.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// function to add the elemsents of two arrays</span></span><br><span class="line"><span class="function">__global__</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> n, <span class="type">float</span> *x, <span class="type">float</span> *y)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">      y[i] = x[i] + y[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="type">int</span> N = <span class="number">1</span>&lt;&lt;<span class="number">25</span>; <span class="comment">// 30M elements</span></span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> *x = <span class="keyword">new</span> <span class="type">float</span>[N];</span><br><span class="line">  <span class="type">float</span> *y = <span class="keyword">new</span> <span class="type">float</span>[N];</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>(&amp;x, N*<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>(&amp;y, N*<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">  <span class="comment">// initialize x and y arrays on the host</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">    x[i] = <span class="number">1.0f</span>;</span><br><span class="line">    y[i] = <span class="number">2.0f</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">timeval</span> t1,t2;</span><br><span class="line">  <span class="type">double</span> timeuse;</span><br><span class="line">  <span class="built_in">gettimeofday</span>(&amp;t1,<span class="literal">NULL</span>);</span><br><span class="line">  <span class="comment">// Run kernel on 30M elements on the CPU</span></span><br><span class="line">  add&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;(N, x, y);</span><br><span class="line">  <span class="built_in">gettimeofday</span>(&amp;t2,<span class="literal">NULL</span>);</span><br><span class="line">  timeuse = (t2.tv_sec - t1.tv_sec) + (<span class="type">double</span>)(t2.tv_usec - t1.tv_usec)/<span class="number">1000.0</span>;</span><br><span class="line"></span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;add(int, float*, float*) time: &quot;</span> &lt;&lt; timeuse &lt;&lt; <span class="string">&quot;ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">  <span class="comment">// Check for errors (all values should be 3.0f)</span></span><br><span class="line">  <span class="type">float</span> maxError = <span class="number">0.0f</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    maxError = <span class="built_in">fmax</span>(maxError, <span class="built_in">fabs</span>(y[i]<span class="number">-3.0f</span>));</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;Max error: &quot;</span> &lt;&lt; maxError &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Free memory</span></span><br><span class="line">  <span class="keyword">delete</span> [] x;</span><br><span class="line">  <span class="keyword">delete</span> [] y;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaFree</span>(x);</span><br><span class="line">  <span class="built_in">cudaFree</span>(y);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-Windows版本："><a href="#2-Windows版本：" class="headerlink" title="2. Windows版本："></a>2. Windows版本：</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;windows.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// function to add the elements of two arrays</span></span><br><span class="line"><span class="function">__global__</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> n, <span class="type">float</span> *x, <span class="type">float</span> *y)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">      y[i] = x[i] + y[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="type">int</span> N = <span class="number">1</span>&lt;&lt;<span class="number">25</span>; <span class="comment">// 30M elements</span></span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> *x = <span class="keyword">new</span> <span class="type">float</span>[N];</span><br><span class="line">  <span class="type">float</span> *y = <span class="keyword">new</span> <span class="type">float</span>[N];</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>(&amp;x, N*<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>(&amp;y, N*<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">  <span class="comment">// initialize x and y arrays on the host</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">    x[i] = <span class="number">1.0f</span>;</span><br><span class="line">    y[i] = <span class="number">2.0f</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ULONGLONG t1, t2, freq;</span><br><span class="line">  <span class="type">double</span> timeuse;</span><br><span class="line">  <span class="built_in">QueryPerformanceFrequency</span>((LARGE_INTEGER*)&amp;freq);</span><br><span class="line">  <span class="built_in">QueryPerformanceCounter</span>((LARGE_INTEGER*)&amp;t1);</span><br><span class="line">  <span class="comment">// Run kernel on 30M elements on the CPU</span></span><br><span class="line">  add&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;(N, x, y);</span><br><span class="line">  <span class="built_in">QueryPerformanceCounter</span>((LARGE_INTEGER*)&amp;t2);</span><br><span class="line">  timeuse = (<span class="type">double</span>)(t2 - t1) / (<span class="type">double</span>)freq * <span class="number">1000.0</span>;</span><br><span class="line"></span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;add(int, float*, float*) time: &quot;</span> &lt;&lt; timeuse &lt;&lt; <span class="string">&quot;ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">  <span class="comment">// Check for errors (all values should be 3.0f)</span></span><br><span class="line">  <span class="type">float</span> maxError = <span class="number">0.0f</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    maxError = <span class="built_in">fmax</span>(maxError, <span class="built_in">fabs</span>(y[i]<span class="number">-3.0f</span>));</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;Max error: &quot;</span> &lt;&lt; maxError &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Free memory</span></span><br><span class="line">  <span class="keyword">delete</span> [] x;</span><br><span class="line">  <span class="keyword">delete</span> [] y;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaFree</span>(x);</span><br><span class="line">  <span class="built_in">cudaFree</span>(y);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="二、一份CUDA代码如何在CPU上跑起来？"><a href="#二、一份CUDA代码如何在CPU上跑起来？" class="headerlink" title="二、一份CUDA代码如何在CPU上跑起来？"></a>二、一份CUDA代码如何在CPU上跑起来？</h1><p>这个很简单，直接运行就可以了。</p><p><img src="https://cdnjson.com/images/2023/07/02/RLBHC1A79UNE645CHGQ.jpg" alt="avatar"></p><h1 id="三、一份CUDA代码如何在NVidia-GPU上跑起来？"><a href="#三、一份CUDA代码如何在NVidia-GPU上跑起来？" class="headerlink" title="三、一份CUDA代码如何在NVidia-GPU上跑起来？"></a>三、一份CUDA代码如何在NVidia-GPU上跑起来？</h1><p>首先要保证电脑内的CUDA环境，这里我们只说1个Linux下配置CUDA环境的细节：</p><ul><li>下载CUDA的时候如果说有几个软件包无法下载，不妨加上<code>-- fix-missing</code>试一试，即<code>sudo apt install nvidia-cuda-toolkit --fix-missing</code>；</li></ul><p>然后按照如图所示的方式就可以跑起来啦！可以看到明显快了很多（笔者是GTX1650，附上截图）。</p><p><img src="https://cdnjson.com/images/2023/07/02/WR4OPCZDJZXNSU6B.png" alt="avatar"></p><p><img src="https://cdnjson.com/images/2023/07/02/imagea80bb0bd9e073ba0.png" alt="avatar"></p><h1 id="四、一份CUDA代码如何在ZLUDA上跑起来？"><a href="#四、一份CUDA代码如何在ZLUDA上跑起来？" class="headerlink" title="四、一份CUDA代码如何在ZLUDA上跑起来？"></a>四、一份CUDA代码如何在ZLUDA上跑起来？</h1><p>ZLUDA仓库：<a href="https://github.com/vosen/ZLUDA">https://github.com/vosen/ZLUDA</a></p><p>这个比前面的复杂一些，网上的资料也比较少，但是经过一番探索还是摸索出来了方法：</p><p>我们需要的前置环境：</p><ul><li>Visual Studio 2019（对没错，必须是2017~2019的版本才行，笔者一开始下了一个2022的结果不行）</li><li>Rust（下载最简单的版本就可以了，可以用<code>cargo --version</code>和<code>rustc --version</code>来检查是否安装成功）</li><li>Visual Studio的<code>cl.exe</code>需要添加至环境变量，具体这个文件在哪可以使用Everything搜索</li><li>Clone上述ZLUDA仓库到本地并编译，编译过程下文会展示</li></ul><p><img src="https://cdnjson.com/images/2023/07/02/CST4_0EOPOSW8GYUUG6.png" alt="avatar"></p><blockquote><p>如上这种编译错误就是没有VS工具链导致的👆</p></blockquote><h2 id="1-ZLUDA前置环境如何编译？"><a href="#1-ZLUDA前置环境如何编译？" class="headerlink" title="1. ZLUDA前置环境如何编译？"></a>1. ZLUDA前置环境如何编译？</h2><p>首先要进入ZLUDA安装目录，打开终端后执行<code>cargo build --release</code></p><h2 id="2-如何把-cu文件编译成-exe文件呢？"><a href="#2-如何把-cu文件编译成-exe文件呢？" class="headerlink" title="2. 如何把.cu文件编译成.exe文件呢？"></a>2. 如何把.cu文件编译成.exe文件呢？</h2><p>只需要进入我们的项目目录下执行：<code>nvcc -o my_app.exe my_app.cu</code></p><p>例如笔者就是：<code>nvcc -o hello.exe windows-hello.cu</code></p><blockquote><p>Linux：</p></blockquote><p><code>LD_LIBRARY_PATH=&lt;ZLUDA_DIRECTORY&gt; &lt;APPLICATION&gt; &lt;APPLICATIONS_ARGUMENTS&gt;</code></p><h2 id="3-如何运行我们的代码？"><a href="#3-如何运行我们的代码？" class="headerlink" title="3. 如何运行我们的代码？"></a>3. 如何运行我们的代码？</h2><p>按理来说随便进入一个目录就行，不过如果出错的话还是在ZLUDA目录下执行如下代码：</p><blockquote><p>Windows：</p></blockquote><p><code>&lt;ZLUDA_DIRECTORY&gt;\zluda_with.exe -- &lt;APPLICATION&gt; &lt;APPLICATIONS_ARGUMENTS&gt;</code></p><p>最后一项参数实测<strong>可以为空</strong>，<code>&lt;APPLICATION&gt;</code>是你代码的<code>.exe</code>可执行文件，例如笔者就是：</p><p><code>D:\My_Files\Coding-Project-2023\OSPP\OSPP-THU-CUDA\Start\ZLUDA\ZLUDA\target\release\deps\zluda_with.exe -- hello.exe</code></p><p><img src="https://cdnjson.com/images/2023/07/02/image.png" alt="avatar"></p><p>比单纯的GPU还快！虽然这里没有控制变量，要控制的话应该在Linux下去测试，但是还是肉眼可见的快。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;零、前言：&quot;&gt;&lt;a href=&quot;#零、前言：&quot; class=&quot;headerlink&quot; title=&quot;零、前言：&quot;&gt;&lt;/a&gt;零、前言：&lt;/h1&gt;&lt;p&gt;此demo的Github仓库（持续更新）：&lt;a href=&quot;https://github.com/Conqueror</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>再回首划时代的技术 - Transformer丨论文解读</title>
    <link href="https://conqueror712.github.io/post/Paper-Transformer.html"/>
    <id>https://conqueror712.github.io/post/Paper-Transformer.html</id>
    <published>2023-06-06T09:25:30.000Z</published>
    <updated>2023-06-06T09:26:56.436Z</updated>
    
    <content type="html"><![CDATA[<h1 id="零、前言："><a href="#零、前言：" class="headerlink" title="零、前言："></a>零、前言：</h1><p><strong>📕欢迎访问</strong>：</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><hr><p><strong>🍬观前小剧场</strong>：</p><blockquote><p>Q：现在网上也有很多解读Transformer的文章，你这个优势在哪里？</p><p>A：事实上，我并不是因为有优势而去写的，写文章的目的其实也是自己学习和理解的一个过程。</p><p>A：不仅如此，我觉得可能我的一些解释和例子，在大牛看起来可能有些小儿科，可能是没必要的内容，但是，如果能够帮助到像我一样的初学者，或者仅仅是给通勤路上的你带来一点点乐趣，那也就值得了。</p><p>A：原文链接：<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p></blockquote><hr><h1 id="一、变压器？变形金刚？Transformer到底在变什么鬼？"><a href="#一、变压器？变形金刚？Transformer到底在变什么鬼？" class="headerlink" title="一、变压器？变形金刚？Transformer到底在变什么鬼？"></a>一、变压器？变形金刚？Transformer到底在变什么鬼？</h1><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/82d40c2a99694af39f689385fd5afae4~tplv-k3u1fbpfcp-zoom-1.image" alt="avatar" style="zoom:67%;"><h2 id="0-Transformer名字解释："><a href="#0-Transformer名字解释：" class="headerlink" title="0. Transformer名字解释："></a>0. Transformer名字解释：</h2><p>事实上，Transformer这个名字在技术上体现了模型中的自注意力机制。在Transformer中，自注意力机制允许模型在输入序列中的每个位置上关注序列中其他位置的信息，从而能够有效地将输入序列中的不同部分进行转换和组合。</p><blockquote><p>现在还不太理解？没关系，继续往后看，会逐一解释。</p></blockquote><p>类比于电气工程中的变压器，变压器可以将输入电压转换为输出电压，并且能够在输入和输出之间实现电气隔离。Transformer模型可以将输入序列中的信息转换为输出序列中的信息，并且能够在输入和输出之间实现信息隔离。因此，这种类比可以帮助人们更好地理解Transformer模型中自注意力机制的作用和优势。</p><p>另外，类比于变形金刚，变形金刚可以通过变形来适应不同的环境和任务，类似地，Transformer模型也可以通过调整不同的超参数和结构来适应不同的自然语言处理任务。此外，变形金刚还可以将不同部分进行组合，形成新的机器人，Transformer模型也可以通过自注意力机制将输入序列中不同部分进行组合，形成新的特征表示，从而更好地完成自然语言处理任务。</p><h2 id="1-摘要和结论："><a href="#1-摘要和结论：" class="headerlink" title="1. 摘要和结论："></a>1. 摘要和结论：</h2><p>对于主流的序列转录（由序列生成序列）模型，一般是通过复杂的循环，或者是CNN，一般是用一个Encoder和Decoder的架构。在性能比较好的模型里面，通常会在Encoder和Decoder之间使用一个注意力机制。</p><p>Transformer这篇文章提出了一个比较简单的架构，也就是仅仅依赖于注意力机制，而没有用循环或者卷积。Transformer做了两个机器翻译的实验，达到了更好的并行度、更少的训练时间，达到了28.4的BLEU（机器翻译的一个衡量标准）。</p><p>一开始，Transformer其实只是想做一个机器翻译的任务，但后来随着BERT和GPT把它用到了更多的NLP任务上时，Transformer就火了，还用在了CV方面，几乎什么东西都能用。</p><ul><li><p>Transformer是第一个仅仅使用注意力机制做序列转录的模型，主要就是用Multi-Headed Self-Attention。</p></li><li><p>Transformer在机器翻译上的训练速度比其他架构快很多，而且效果比较好。</p></li><li><p>Transformer用于其他的任务上可以使得生成不那么时序化。</p></li></ul><h2 id="2-导言："><a href="#2-导言：" class="headerlink" title="2. 导言："></a>2. 导言：</h2><p>2017年，在时序模型里面有LSTM，GRU等等。这里面主要有两个模型：语言模型，Encoder+Decoder模型（当输出的结构化信息比较多的时候）。对于RNN，假设输入是一个句子，那么就会从左往右一个词一个词的看过去。</p><p>具体来说：对于第$t$个词，会计算一个输出$h_t$（隐藏状态），而$h_t$是由$h_{t-1}$和第$t$个词来决定的，这就是为什么RNN能够有效处理时序信息。</p><p>但同时，问题也出现在这里，因为是时序的过程，所以比较难去并行（依赖关系），不能很好的利用现在的硬件加速设备GPU，TPU等。</p><p>不仅如此，当序列比较长的时候，前面的信息可能传到后面就被丢掉了，如果要保留下来的话就需要比较大的$h_t$，这样会导致很大的内存开销，显然也是划不来的。</p><p>RNN的Encoder+Decoder其实已经用上了注意力机制了，主要是用在如何有效把Encoder的内容传给Decoder。但是Transformer因为是用的纯注意力机制，所以可以达到比较高的并行度，可以在更短的时间里面获得更好的结果。</p><h2 id="3-相关工作："><a href="#3-相关工作：" class="headerlink" title="3. 相关工作："></a>3. 相关工作：</h2><p>ByteNet和ConvS2S这些工作已经说了如何使用卷积神经网络替换循环神经网络，使得减少时序的计算，但是这存在一些问题，就是说，利用CNN难以对比较长的序列进行建模。这是因为CNN在做计算的时候，每次会看一个比较小的窗口，如果说有两个比较远的像素，那就需要很多层卷积来最后把它给融合起来。</p><p>但是如果使用Transformer的注意力机制的话，一层就可以看到所有的像素。不过CNN的好处就是可以多通道的输出，所以说Transformer在普通的注意力机制上提出了多头注意力机制，即Multi-Head Attention。这个会在后面讲到。</p><p>Transformer还提到了自注意力机制，虽然这个东西并不是Transformer的独创就是了，自注意力机制和注意力机制有什么区别呢？</p><p>自注意力机制是一种用于计算输入序列中不同位置之间关联度的机制，而注意力机制是一种用于计算不同位置之间权重关系的机制。</p><p>可以说，自注意力机制是注意力机制的一种特殊形式。</p><blockquote><p>注意力机制是一种用于计算不同位置之间的权重关系的机制，用于在输入序列中找到与特定查询相关的信息。具体来说，注意力机制将查询向量和一组键向量进行比较，并使用softmax函数将每个键向量的权重计算出来，最终将加权的值向量作为注意力机制的输出。这种机制常用于自然语言处理中的任务，如机器翻译和问答系统中。</p><p>自注意力机制是Transformer模型中的一种机制，它是一种特殊的注意力机制，用于计算输入序列中不同位置之间的关联性。与传统的注意力机制不同，自注意力机制中的查询向量、键向量和值向量都来自于同一个位置的输入向量，通过计算不同向量之间的相似度来计算关注程度。这种机制能够捕捉输入序列中的长距离依赖关系，避免了传统序列模型中的信息瓶颈问题，因此被广泛应用于自然语言处理任务中。</p></blockquote><p>文章还提到了Memory Network的东西，在当时（2017年）是一个热点，不过现在已经不温不火了。</p><h1 id="二、模型架构——让我们来拆开变形金刚看一看"><a href="#二、模型架构——让我们来拆开变形金刚看一看" class="headerlink" title="二、模型架构——让我们来拆开变形金刚看一看"></a>二、模型架构——让我们来拆开变形金刚看一看</h1><h2 id="1-模型概述："><a href="#1-模型概述：" class="headerlink" title="1. 模型概述："></a>1. 模型概述：</h2><p>要了解Transformer的模型，首先就需要解释一下Encoder+Decoder：</p><p>对于Encoder而言，会将一个输入$[x_1, x_2, …, x_n]$表示成，也就是输出为$[z_1, z_2, …, z_n]$，每一个$z_t$都是对应着$x_t$的一个向量表示。这就是把原始的输入转换成机器学习可以理解的向量，比如说从句子到向量。</p><p>对于Decoder而言，拿到了Encoder的输出，会生成一个长为$m$的序列$[y_1, y_2, …y_m]$（$m$不一定等于$n$），需要注意的是，Decoder是一个一个生成的，而不是像Encoder一样一口气可以看过去，这个一个一个生成的模型叫做自回归，即<code>auto-regressive</code>。</p><p>自回归：要想获得$y_t$，就需要把$y_1$到$y_{t-1}$全部都得到，即过去时刻的输出，也会作为当前时刻的输入。</p><blockquote><p>如图所示，我们接下来就来说一下这张典中典的图：</p></blockquote><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3ea3c04f31f54f5f812bd1f6dd796403~tplv-k3u1fbpfcp-zoom-1.image" alt="avatar" style="zoom:67%;"><p>这是一个Encoder-Decoder的架构，左边一坨是Encoder，右边一坨是Decoder。</p><p>左下角的<code>input</code>就是Encoder的输入，而右下角的<code>Output</code>实际上是Decoder的输入，也就是上一时刻的Decoder的输出，<code>shifted right</code>就是输入和输出不断右移的过程。</p><p>输入<code>input</code>会先进入一个嵌入层<code>Embedding</code>，作用是把一个一个的词表示成一个一个的向量。</p><p><code>Positional Encoding</code>是一种将位置信息嵌入到输入序列中的技术，以便模型能够感知输入序列中各个位置之间的相对距离关系。</p><p>再往上就是Encoder的部分了，左边的<code>N×</code>代表的是，像这样的Encoder层，有$N$个；</p><ul><li><code>Multi-Head Attention</code>顾名思义就是多头注意力机制；</li><li><code>Add</code>就是残差的连接；</li><li><code>Norm</code>是一个层归一化的操作；</li><li><code>Feed Forward</code>是前馈神经网络；</li></ul><p>随后，Encoder的输出进入到Decoder的中间作为输入。</p><p>对于Decoder的部分，我们可以看到上面的部分和Encoder是完全一样的，</p><p>只是下面多了一个<code>Masked Multi-Head Attention</code>，同样的，<code>N×</code>的意思还是一样。</p><p>最后，输出会进入一个线性的输出层<code>Lineat</code>，然后进行一个<code>Softmax</code>，就会得到最终的输出。</p><h2 id="2-Encoder："><a href="#2-Encoder：" class="headerlink" title="2. Encoder："></a>2. Encoder：</h2><p>默认的Encoder一共有$N &#x3D; 6$层，的每个层<code>layers</code>有两个子层<code>sub-layers</code>：</p><ul><li><code>Multi-Head Self-Attention</code></li><li><code>MLP: Positionwise fully connected feed-forward network</code></li></ul><p>用残差连接每个<code>sub-layers</code>，最后使用<code>layer normalization</code>。</p><p>每个<code>layer</code>的公式：$LayerNorm(x+Sublayer(x))$</p><p>因为残差需要输入输出一样大，Transformer为了简单起见（不做投影）就都设置了$d_{model}&#x3D;512$。</p><p>由此，Transformer的可调参数就只有两个了：$N$和$d_{model}$；</p><p>后续基于它做的BERT和GPT等等，都是只需要调这两个参数。</p><h2 id="3-LayerNormalization："><a href="#3-LayerNormalization：" class="headerlink" title="3. LayerNormalization："></a>3. LayerNormalization：</h2><p><code>LayerNormalization</code>是一种在深度神经网络中用于归一化每个输入特征的技术。</p><p>在Encoder层中，<code>LayerNormalization</code>被用于对每个位置的隐藏层向量进行归一化，以确保每个位置的向量在同一尺度上。</p><p>如果要解释呢，就是说<code>LayerNormalization</code>会对每个位置的隐藏层向量进行如下的归一化操作：</p><ol><li>首先，计算该向量在每个特征维度上的均值和方差。</li><li>然后，将该向量在每个特征维度上减去均值并除以方差的平方根。</li><li>最后，将结果乘以一个可学习的缩放因子和加上一个可学习的偏移量。</li></ol><p>这样做的话，层归一化可以使得每个位置的向量更加稳定，减少了不同输入之间的差异，有助于提高模型的泛化性能。</p><p>至于为什么使用<code>LayerNorm</code>而不是<code>BatchNorm</code>，事实上是因为在变长的任务中（文本分类、机器翻译等），输入序列的长度可能会不同，因此每个batch的样本数量和长度都可能不同。</p><h2 id="4-Decoder："><a href="#4-Decoder：" class="headerlink" title="4. Decoder："></a>4. Decoder：</h2><p>Decoder的结构与Encoder类似，但是在处理输入序列时需要注意避免自身的信息泄露。</p><p>默认的Dncoder也有$N &#x3D; 6$层，每个层<code>layers</code>有三个子层<code>sub-layers</code>：</p><ul><li><code>Masked Multi-Head Self-Attention</code>：在生成目标序列时，需要避免将当前位置之后的信息泄露给模型。因此，Decoder的第一个子层使用<code>Mask</code>来限制每个位置<strong>只能看到之前的位置</strong>，并使用多头自注意力来计算每个位置的编码表示。</li><li><code>Multi-Head Attention</code>：Decoder需要将编码表示与Encoder的输出进行交互，以获取源序列的信息。因此，Decoder的第二个子层使用多头注意力来计算每个位置的上下文向量。</li><li><code>MLP: Positionwise fully connected feed-forward network</code>：与Encoder类似，Decoder的第三个子层使用全连接前馈网络来对每个位置的编码表示进行非线性变换。</li></ul><h1 id="三、Attention-is-all-you-need"><a href="#三、Attention-is-all-you-need" class="headerlink" title="三、Attention is all you need?"></a>三、Attention is all you need?</h1><h2 id="1-Scaled-Dot-Product-Attention："><a href="#1-Scaled-Dot-Product-Attention：" class="headerlink" title="1. Scaled Dot-Product Attention："></a>1. Scaled Dot-Product Attention：</h2><p>注意力函数是一个将<code>query</code>和一些<code>key-value</code>对映射成<code>ouptut</code>的函数。</p><p><code>output</code>是<code>value</code>的一个加权和，所以他们的维度是一样的。</p><p>对于每个<code>value</code>的权重，是通过这个<code>value</code>对应的<code>key</code>和<code>query</code>的相似度计算而来的。</p><p>当然，这个相似度计算的算法对于每个任务而言是不一样的（但是道理是一样的），Transformer的版本：</p><p>$Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V$</p><p>名字叫做<code>Scaled Dot-Product Attention</code>，其<code>query</code>和<code>key</code>的长度都是$d_k$，对它们做内积，</p><p>而<code>value</code>是$d_v$（显然输出也就是$d_v$了）。</p><p>用内积表示相似度这个事情很好理解，向量嘛，看一看它们的夹角。</p><p>在实际过程中，这样算可能会有点慢，所以其实真正的方法是把很多<code>query</code>写成一个矩阵，同时对一组查询进行注意力计算，这些查询被打包成一个矩阵$Q$，键和值也被打包成矩阵$K$和$V$，就如上式啦。</p><p>至于为什么要除以一个$\sqrt{d_k}$，是因为当$d_k$的值比较小时，这种点乘方法的注意力机制和另一种加性方法的注意力机制的表现相似。但是，对于较大的$d_k$值，加性注意力的表现要优于点积注意力（没有缩放因子）。这大概是因为在较大的$d_k$值下，点积的值会变得很大，将<code>softmax</code>函数推到具有极小梯度的区域。为了抵消这种影响，Transformer就使用了缩放因子$\frac{1}{\sqrt{d_k}}$来调整点积的大小。</p><p>整个过程的计算图如下：</p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f0aebbf87323480b81faca444cad8c80~tplv-k3u1fbpfcp-zoom-1.image" alt="avatar" style="zoom: 80%;"><p>其中的<code>Mask</code>操作是为了防止$t$时刻看到后面的内容，影响输出（因为注意力机制不是会看到所有的嘛），挡上之后，后面的那些内容的权值就会变成负的一个绝对值很大很大的数，比如<code>1e-10</code>，从而让它们在<code>softmax</code>的时候变成$0$。</p><h2 id="2-Multi-Head-Attention："><a href="#2-Multi-Head-Attention：" class="headerlink" title="2. Multi-Head Attention："></a>2. Multi-Head Attention：</h2><p><code>Multi-Head Attention</code>是一种用于计算输入序列中不同位置之间关联度的机制。</p><p>基本原理是将输入序列中的每个元素都映射为多个不同的向量表示，然后计算不同向量之间的相似度来计算关联度。这个过程涉及三个步骤：</p><ol><li>线性映射：对输入序列进行线性映射，将其转换为一个包含不同维度的向量表示。具体来说，将输入序列乘以三个权重矩阵，分别表示查询、键和值的线性映射。</li><li>注意力计算：对于每个查询向量，在所有键向量上计算其与之对应的注意力得分，表示该查询向量对不同键向量的关注程度。这个计算过程可以使用点积注意力、加性注意力等方法。</li><li>加权求和：将每个值向量与对应的注意力得分相乘并加权求和，得到一个表示该查询向量的加权键向量。这个过程可以重复多次，得到多个加权键向量，最终将它们拼接在一起生成一个多头注意力表示。</li></ol><p>这样一来，模型就能够同时关注输入序列中不同位置的信息，并且学习到它们之间的关联性了。</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c970ab2e20ff41a4a1af81b270701084~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ea8c7b14d77f4787b21b4812eb4e0a6c~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><h2 id="3-不只是Attention！"><a href="#3-不只是Attention！" class="headerlink" title="3. 不只是Attention！"></a>3. 不只是Attention！</h2><p>实际上，Transformer模型不仅仅依赖于自注意力机制，还使用了MLP、Mask、残差连接、层归一化等关键技术，这些技术也缺一不可。</p><p>这些技术都是Transformer模型中不可或缺的组成部分，它们共同作用于模型中，使得Transformer能够在自然语言处理任务中取得了很好的效果。</p><h1 id="四、站在过去，展望未来"><a href="#四、站在过去，展望未来" class="headerlink" title="四、站在过去，展望未来"></a>四、站在过去，展望未来</h1><p>当我们回顾过去，会发现，Transformer模型对NLP领域的影响是深远的。Transformer模型的提出，不仅仅是一种新的神经网络模型，更是一种全新的思路和范式，它开创了自注意力机制的新时代，成为NLP领域的一股强劲推动力量，就如几年前的CNN之于CV一样。</p><p>站在过去的2017年，Transformer模型在机器翻译、文本生成、问答系统等任务中取得了突破性进展，赋予了NLP模型更强的处理语义和上下文的能力，使得NLP应用更加普及和高效。此外，Transformer模型也为NLP领域的研究提供了新的思路，推动了NLP乃至所有的AI领域的发展和创新。</p><p>展望未来的20年代中后期，Transformer模型大概仍将继续发挥重要作用。在多模态、现实交互等挑战面前，Transformer模型需要进一步优化和改进，以适应更加复杂和多样化的应用场景。同时，我们也需要探索更多新的技术和思路，以进一步提升模型的性能和效率。</p><blockquote><p>THE END.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;零、前言：&quot;&gt;&lt;a href=&quot;#零、前言：&quot; class=&quot;headerlink&quot; title=&quot;零、前言：&quot;&gt;&lt;/a&gt;零、前言：&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;📕欢迎访问&lt;/strong&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;个人博客：&lt;a href=</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读</title>
    <link href="https://conqueror712.github.io/post/Paper-BigBird.html"/>
    <id>https://conqueror712.github.io/post/Paper-BigBird.html</id>
    <published>2023-05-29T04:32:00.000Z</published>
    <updated>2023-05-29T04:33:38.266Z</updated>
    
    <content type="html"><![CDATA[<h1 id="零、前言："><a href="#零、前言：" class="headerlink" title="零、前言："></a>零、前言：</h1><p><strong>📕欢迎访问</strong>：</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><hr><p><strong>🍬观前小剧场</strong>：</p><blockquote><p>Q：既然上次我们知道了ELMo、BERT都是芝麻街的东西，那还有什么吗？我觉得这样的命名好有趣！</p><p>A：当然！不妨来看看2020年的一篇大鸟，呃不对，是论文——BigBird吧！</p></blockquote><hr><h1 id="一、这是什么鸟？看一下："><a href="#一、这是什么鸟？看一下：" class="headerlink" title="一、这是什么鸟？看一下："></a>一、这是什么鸟？看一下：</h1><h2 id="一句话解释「大鸟」BigBird："><a href="#一句话解释「大鸟」BigBird：" class="headerlink" title="一句话解释「大鸟」BigBird："></a>一句话解释「大鸟」BigBird：</h2><p>通过<strong>稀疏的注意力机制</strong>，将对序列长度的二次依赖<strong>减少到线性</strong>。</p><p>基于此，相较于BERT，可以<strong>处理更长的上下文信息</strong>，大大提高了各种NLP任务的性能。</p><hr><h2 id="BigBird出现的目的和意义："><a href="#BigBird出现的目的和意义：" class="headerlink" title="BigBird出现的目的和意义："></a>BigBird出现的目的和意义：</h2><p>虽然BERT是近年来NLP中最成功的深度学习模型之一，但是由于其全注意力机制，会存在<strong>对序列长度的二次依赖</strong>，从而<strong>不能很好地处理较长的上下文信息</strong>。</p><p>而BigBird采用<strong>稀疏的注意力机制</strong>，突破了全注意力机制的<strong>局限</strong>，使得可以<strong>处理更长的上下文信息</strong>。（怎么感觉和上面说的一样）</p><hr><h2 id="BigBird背景铺垫与介绍："><a href="#BigBird背景铺垫与介绍：" class="headerlink" title="BigBird背景铺垫与介绍："></a>BigBird背景铺垫与介绍：</h2><p>Transformer的广泛应用使得现代NLP模型性能大大优于以前的序列模型，如LSTM，主要是因为<strong>自注意力机制</strong>的引入，这使得我们可以对输入序列的每个令牌去<strong>并行</strong>的评估，而不是像LSTM等循环神经网络的<strong>顺序</strong>依赖性。这样做的好处是可以充分利用GPU和TPU这种现代SIMD硬件加速器的全部功能，从而可以让我们在空前规模的大数据集上训练NLP模型，而这一过程往往是通过预训练以及对下游任务的微调进行的。</p><p>但是，正如矛盾是对立而统一的一样，这种改进并不是没有代价的，完全自注意力机制的计算和内存开销是<strong>序列长度的平方</strong>，这降低了它们对更大上下文的任务的直接适用性，最典型的大上下文例子就是QA和文档分类与摘要。如果非要做的话，就会将大字符串拆分更小的段，这种内容碎片化也会导致上下文的严重丢失，从而使其应用程序受到限制。</p><p>那么，我们有没有一种可能，可以通过<strong>更少的内积</strong>实现一个完全二次型的自注意力机制呢？这样<strong>稀疏的自注意力机制</strong>能否保留原始网络的表现力和灵活性呢？</p><p>答案就是”大鸟”——BigBird！（灵感来源于图的稀疏化方法）</p><p>“大鸟”的稀疏注意力机制可以提高需要较长上下文的任务的性能。其复杂性是线性的而非二次的。</p><p>“大鸟”主要由三个部分组成：</p><ul><li>在序列的所有部分上出现的一组（$g$个）全局<code>token</code>的集合</li><li>所有与$w$个本地相邻<code>token</code>集合相关的<code>token</code></li><li>所有标记都属于$r$个随机标记的集合</li></ul><p>“大鸟”的三个主要贡献：</p><ul><li>BigBird满足所有已知的全变换的理论性质，特别是添加额外的记号，允许将所有连续序列表示为线性个内积的序列函数，并且在标准精度下是图灵完备的。这也就意味着，一切可以计算的问题，BigBird都能计算，理论上，它能够用来解决任何算法。</li><li>BigBird的扩展上下文有利于各种NLP任务。</li><li>一种新的基于注意力机制的模型应用：提取基因组序列（如DNA）的上下文表示。</li></ul><blockquote><p>我们不妨来看一看，各种不同的Transformer变种的效率：</p><p>圈越大，时间复杂度越高；</p><p>纵坐标越小，空间复杂度越高；</p><p>横坐标越大，性能越好；</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2668aa7424b34a929c4b30e2d2200b45~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p></blockquote><hr><h2 id="BigBird相关工作："><a href="#BigBird相关工作：" class="headerlink" title="BigBird相关工作："></a>BigBird相关工作：</h2><p>相关工作这一部分主要概括了两条限制Transformer的依赖关系，的研究方向（加逗号是为了断句，方便理解）。</p><ol><li><p>对于长度限制，最简单的方法是使用滑动窗口；更复杂的方法通常重复调用Transformer块，即每次提供不同的上下文。</p></li><li><p>讨论完全的注意力机制是否是必要的，不完全的注意力机制可以减少内存和计算需求。</p></li></ol><p>除此之外，Longformer引入了局部滑动窗口和少量全局遮罩来减少计算，并将BERT扩展到更长的序列任务。BigBird与Extended Transformers Construction工作密切相关，后者旨在为Transformer编码文本结构。其广泛使用全局令牌来实现目标。</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cf44ce15f914426b8ffb41182ab4ec74~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><blockquote><p>注意力机制的示意图如上所示，白色表示注意力不集中；</p><p>从左到右分别是r &#x3D; 2的随机注意力、w &#x3D; 3的滑动窗口注意力、g &#x3D; 2的全局注意力以及将之合并的”大鸟”。</p></blockquote><p>这三种注意力机制结合起来为什么就能接近BERT的全注意力机制呢？我们以图释之：</p><blockquote><p>BigBird：</p></blockquote><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ae25633798ac480687061916991123b3~tplv-k3u1fbpfcp-watermark.image" alt="graph.gif"></p><blockquote><p>BERT：</p></blockquote><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ece3c3e207df40538c46632ef7326e87~tplv-k3u1fbpfcp-watermark.image" alt="full.png"></p><hr><h1 id="二、林子大了，什么鸟都有："><a href="#二、林子大了，什么鸟都有：" class="headerlink" title="二、林子大了，什么鸟都有："></a>二、林子大了，什么鸟都有：</h1><h2 id="稀疏注意力到底在做什么？"><a href="#稀疏注意力到底在做什么？" class="headerlink" title="稀疏注意力到底在做什么？"></a>稀疏注意力到底在做什么？</h2><p>BigBird使用稀疏注意力机制，这说明注意力机制是逐个<code>token</code>应用的，而不是像BERT那样，只对整个输入进行一次应用。</p><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f16dff0b54f246c2b99e0850431b81a6~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>如图所示，有额外的向左平移和向右平移的句子，就是顶部的<code>them Vasudev is proting BigBird...</code>这些；</p><p>这是计算<strong>滑动注意力</strong>时所需要的，而滑动注意力又是BigBird稀疏注意力的一个重要组成部分，也就是图中黄色的那些。</p><h2 id="蛋白质是牛肉的八倍？"><a href="#蛋白质是牛肉的八倍？" class="headerlink" title="蛋白质是牛肉的八倍？"></a>蛋白质是牛肉的八倍？</h2><p>BigBird能够处理比以前长8倍的序列。利用 BigBird 及其稀疏注意力机制，研究小组将 BERT 的复杂度$O(n^2)$降到$O(n)$。</p><p>这说明，原来的BERT只能处理512个<code>token</code>的输入序列，现在BigBird可以处理4096个<code>token</code>，整整八倍！</p><p>然而事实上，BigBird带给我们的惊喜远不止如此，在BigBird的论文中，使用的是4096这一数值，然而实际上的效果可以达到更大的 16K以上！</p><h2 id="一较高下？"><a href="#一较高下？" class="headerlink" title="一较高下？"></a>一较高下？</h2><p>用GPT-3举例（2020年还没有GPT-3.5，更别说GPT-4了），BigBird的预训练集远不如GPT-3大，因为GPT-3的训练参数为1750亿，但如下表所示，BigBird比很多模型的性能更好。</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9a286fffb71c4e82a4346cb6ca61adc6~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><h2 id="21世纪是生物的世纪？"><a href="#21世纪是生物的世纪？" class="headerlink" title="21世纪是生物的世纪？"></a>21世纪是生物的世纪？</h2><p>深度学习在基因组学数据处理中的应用越来越多。编码器将 DNA 序列的片段作为输入，用于诸如甲基化分析、预测非编码变体的功能效应等任务。这些任务以DNA序列片段作为输入，既然是序列，那么BigBird想必可以派上用场，果不其然，甚至因为DNA中的许多功能效应是高度非局部的，也就是偏向于更长的范围，所以长序列的处理显得尤为重要。</p><h2 id="大显身手！Google搜索引擎："><a href="#大显身手！Google搜索引擎：" class="headerlink" title="大显身手！Google搜索引擎："></a>大显身手！Google搜索引擎：</h2><p>2019年，BERT出现的时候，Google第一时间就把BERT集成到其搜索引擎中，来理解用户的输入，从而为用户呈现更多、更相关的内容了；2020年，”大鸟”飞来，很快啊！Google马上就又放了进去。</p><hr><h1 id="三、鸟师傅，你是做什么工作的："><a href="#三、鸟师傅，你是做什么工作的：" class="headerlink" title="三、鸟师傅，你是做什么工作的："></a>三、鸟师傅，你是做什么工作的：</h1><h2 id="再看稀疏注意力："><a href="#再看稀疏注意力：" class="headerlink" title="再看稀疏注意力："></a>再看稀疏注意力：</h2><p>在一般的完全注意力，例如BERT中，序列为$X&#x3D;x_1, x_2,…,x_n$，由稠密向量$Q,K,V$计算的注意力为$Z&#x3D;Softmax(QK^T)$；</p><p>在BigBird注意力计算中，这样的过程只在个别的query向量和key向量之间计算。</p><p>那么，更多的是什么呢？</p><p>设：<code>b, r, s, g</code>分别为<code>block_size, num_random_blocks, num_sliding_blocks, num_global_blocks</code>；</p><p>当<code>b = 4, r = 1, s = 3, g = 2</code>时，<code>Q</code>和<code>V</code>的块如下所示：</p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d8d363c4d8e4329b1a5e22eaba7b995~tplv-k3u1fbpfcp-zoom-1.image" alt="avatar" style="zoom:50%;"><p>注意力得分$q_1,q_2,q_3,q_{n-2},q_{n-1},q_n$计算过程：</p><p>$q_1$由$a_1$来表示：$a_1&#x3D;Softmax(q_1 × K^T)$，$q_1$代表第一个块，$g_i$代表第$i$个块</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9f2fd4486f8b46f8ac8ee4611ed77a74~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>当计算$a_2$时，事情就变得没那么简单了，需要加上另外两种块，即$a_2&#x3D;Softmax(q2×concat(k1, k2, k3, k5, k7))$</p><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/401e904ae01547ee84f7a3bf2c1421ef~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>类似地，来到$q_3$：</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a590cbe45d224c2da609cace940a4a11~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>倒数第二个，$q_{n-1}$：</p><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/35ea6b7a23cd45a68ea936c1a96295d0~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>最后，也如初见，$a_n&#x3D;Softmax(q_n × K^T)$：</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b5ecc98a9f384a6a9fad15041580fde7~tplv-k3u1fbpfcp-zoom-1.image" alt="avatar"></p><blockquote><p>整体效果图：</p></blockquote><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fe6ad1ee41764801a7911c3f5051c7ff~tplv-k3u1fbpfcp-watermark.image" alt="block-sparse-attn.gif"></p><p>这就是稀疏注意力中最难的部分了。</p><blockquote><p>实事求是：</p></blockquote><p>纸上谈兵显然是行不通的，我们需要数据支撑，稀疏注意力到底为什么好：</p><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/11a9a72862534034bc76be7ca2099324~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>嗯！显然好！</p><hr><h2 id="大鸟大鸟，实战见分晓！"><a href="#大鸟大鸟，实战见分晓！" class="headerlink" title="大鸟大鸟，实战见分晓！"></a>大鸟大鸟，实战见分晓！</h2><blockquote><p>像这样，就可以使用BigBird模型了！</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BigBirdModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从预先训练的检查点装载BigBird</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>)</span><br><span class="line"><span class="comment"># 这将初始化模型的默认配置，即attention_type = &quot;block_sparse&quot;, num_random_blocks = 3, block_size = 64</span></span><br><span class="line"><span class="comment"># 但是您可以使用任何检查点自由地更改这些参数</span></span><br><span class="line"><span class="comment"># 这3个参数只会更改每个查询token将要参加的token数量</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>, num_random_blocks=<span class="number">2</span>, block_size=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过将attention_type设置为&quot;original_full&quot;</span></span><br><span class="line"><span class="comment"># BigBird将依赖于n^2复杂度的全部注意力</span></span><br><span class="line"><span class="comment"># 这样，BigBird与BERT的相似度达到99.9%</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>, attention_type=<span class="string">&quot;original_full&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>一个问答任务的例子：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BigBirdForQuestionAnswering, BigBirdTokenizer</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从预训练的权重中初始化BigBird模型，并在其顶部随机初始化头部</span></span><br><span class="line">model = BigBirdForQuestionAnswering.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>, block_size=<span class="number">64</span>, num_random_blocks=<span class="number">3</span>)</span><br><span class="line">tokenizer = BigBirdTokenizer.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">dataset = <span class="string">&quot;torch.utils.data.DataLoader object&quot;</span></span><br><span class="line">optimizer = <span class="string">&quot;torch.optim object&quot;</span></span><br><span class="line">epochs = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 极小的训练循环</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> dataset:</span><br><span class="line">        model.train()</span><br><span class="line">        batch = &#123;k: batch[k].to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = model(**batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        output[<span class="string">&quot;loss&quot;</span>].backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将最终权重保存在本地目录中</span></span><br><span class="line">model.save_pretrained(<span class="string">&quot;&lt;YOUR-WEIGHTS-DIR&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推至Hub</span></span><br><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> ModelHubMixin</span><br><span class="line">ModelHubMixin.push_to_hub(<span class="string">&quot;&lt;YOUR-WEIGHTS-DIR&gt;&quot;</span>, model_id=<span class="string">&quot;&lt;YOUR-FINETUNED-ID&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用微调模型进行推理</span></span><br><span class="line">question = [<span class="string">&quot;How are you doing?&quot;</span>, <span class="string">&quot;How is life going?&quot;</span>]</span><br><span class="line">context = [<span class="string">&quot;&lt;some big context having ans-1&gt;&quot;</span>, <span class="string">&quot;&lt;some big context having ans-2&gt;&quot;</span>]</span><br><span class="line">batch = tokenizer(question, context, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">batch = &#123;k: batch[k].to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch&#125;</span><br><span class="line"></span><br><span class="line">model = BigBirdForQuestionAnswering.from_pretrained(<span class="string">&quot;&lt;YOUR-FINETUNED-ID&gt;&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    start_logits, end_logits = model(**batch).to_tuple()</span><br><span class="line">    <span class="comment"># 用你想要的策略解码start_logits, end_logits</span></span><br></pre></td></tr></table></figure><h2 id="BigBird的架构的细节部分："><a href="#BigBird的架构的细节部分：" class="headerlink" title="BigBird的架构的细节部分："></a>BigBird的架构的细节部分：</h2><p>注：这一部分推荐阅读原论文，笔者的翻译水平有限（笑）。</p><p>原文链接：<a href="https://arxiv.org/abs/2007.14062">https://arxiv.org/abs/2007.14062</a></p><h3 id="1-使用广义注意力机制来描述BigBird模型："><a href="#1-使用广义注意力机制来描述BigBird模型：" class="headerlink" title="1. 使用广义注意力机制来描述BigBird模型："></a>1. 使用<strong>广义注意力机制</strong>来描述BigBird模型：</h3><p>BigBird使用广义注意力机制和稀疏随机图概念描述模型，以减少自注意机制的复杂度。</p><ol><li><p>用广义注意力机制描述BigBird模型，该机制用于Transformer的每个层。广义注意力机制由有向图<code>D</code>表示：</p><ul><li><code>D</code>的节点集是输入序列中的记号；</li><li><code>D</code>的弧表示注意力机制将考虑的内积。</li></ul></li><li><p>如果<code>D</code>是完全有向图，可以恢复完全二次注意力机制。为简化表达，BigBird操作<code>D</code>的邻接矩阵<code>A</code>，即使<code>D</code>可能是稀疏的。<code>A(i，j) = 1</code>表示查询<code>i attend</code>到键<code>j</code>，否则为0。当<code>A</code>是全1矩阵时，导致二次复杂度，因为所有记号都<code>attend</code>每个其他记号。</p></li><li><p>将自注意视为完全连接图允许利用现有图论来减少其复杂度。减少自注意的二次复杂度问题可以视为图稀疏问题。</p></li><li><p>稀疏随机图注意机制应具有两个理想条件：节点间平均路径长度小和局部性概念。</p></li><li><p>随机图是展开器，可以在许多不同的上下文中近似完全图，包括在其谱属性方面。</p></li></ol><h3 id="2-Erdos-Renyi随机图模型："><a href="#2-Erdos-Renyi随机图模型：" class="headerlink" title="2. Erdos-Rényi随机图模型："></a>2. Erdos-Rényi随机图模型：</h3><p>在这个模型中，每条边都是独立选定的，出现的概率是固定的。在只有<code>Θ(n)</code>条边的随机图中，任意两个节点之间的最短路径是对数级的。因此，这样的随机图在谱上近似完全图，其第二特征值远离第一个特征值。这一属性导致随机游走在图中的混合时间很快，这暗示信息可以在任意一对节点之间快速流动。因此，BigBird提出了稀疏注意力，其中每个查询仅关注<code>r</code>个随机键。</p><h3 id="3-相邻内积与聚类系数："><a href="#3-相邻内积与聚类系数：" class="headerlink" title="3. 相邻内积与聚类系数："></a>3. 相邻内积与聚类系数：</h3><p>大多数NLP和计算生物学中的上下文都具有较高的局部性参考。这意味着一个记号可以从其相邻记号中获取大量信息。BigBird研究自注意模型时得出结论，相邻内积对于NLP任务极其重要。在图论术语中，聚类系数是局部连接性的度量，当图包含许多完全图或近完全图时，聚类系数较高。简单的Erdos-Rényi随机图没有高的聚类系数，但小世界图具有高聚类系数。因此，BigBird定义了滑动窗口注意力，使查询在宽度为<code>w</code>的自注意中attend到<code>i - w/2</code>到<code>i + w/2</code>之间的键。</p><hr><h1 id="四、结论与尾声："><a href="#四、结论与尾声：" class="headerlink" title="四、结论与尾声："></a>四、结论与尾声：</h1><p>BigBird是一个线性相关的稀疏注意机制，有理论保证和实践检验，是序列到序列函数的通用近似器，并且是图灵完备的。</p><p><strong>在理论上：</strong></p><ul><li>BigBird使用额外的全局令牌来保留模型的表达能力。</li><li>BigBird通过移动到稀疏注意机制来减少计算，但同时确实也会产生一些其他的成本。</li></ul><p><strong>在实际上：</strong></p><p>BigBird在许多NLP任务上取得了最先进的性能，如问答和长文档分类。</p><p>BigBird进一步引入了基于注意机制的DNA上下文语言模型，并对下游任务进行调优，如启动子区域预测和预测非编码变异的效果。</p><p>最后，BigBird虽好，但是正如其名，太Big了，这里是指代码量，实现起来实在是比较复杂。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;零、前言：&quot;&gt;&lt;a href=&quot;#零、前言：&quot; class=&quot;headerlink&quot; title=&quot;零、前言：&quot;&gt;&lt;/a&gt;零、前言：&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;📕欢迎访问&lt;/strong&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;个人博客：&lt;a href=</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>2023了，再来看看NLP经典之作 - BERT丨论文解读</title>
    <link href="https://conqueror712.github.io/post/Paper-BERT.html"/>
    <id>https://conqueror712.github.io/post/Paper-BERT.html</id>
    <published>2023-05-20T13:48:01.000Z</published>
    <updated>2023-05-20T13:49:03.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="零、前言："><a href="#零、前言：" class="headerlink" title="零、前言："></a>零、前言：</h1><p><strong>📕欢迎访问</strong>：</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><hr><p><strong>🍬观前小剧场</strong>：</p><blockquote><p>Q：2023年了，如果还没看过BERT，或者是刚入门的小白，该怎么办？</p><p>A：没关系，那就现在开始看吧，<strong>只有先开始才能走向新的开始</strong>！</p><p>Q：我觉得你说的有道理，不过话说回来，正确的阅读姿势是什么？</p><p>A：我想，你需要准备一杯咖啡或者茶，安静地坐下来，以及<strong>30~45分钟的时间</strong>，慢下来阅读，这可能会比急急忙忙地看一遍的收获更大，印象更深。</p><p>Q：如果我只是想了解个大概，并不想知道具体的细节怎么办，我现在就在通勤的地铁上，我也没有位置坐，很急！</p><p>A：没关系，笔者考虑到了这种情况，只需要阅读<strong>前两节或者前三节</strong>的内容即可。</p><p>A：事不宜迟，如果没有其他问题的话我们就开始吧！</p></blockquote><hr><h1 id="一、BERT简介："><a href="#一、BERT简介：" class="headerlink" title="一、BERT简介："></a>一、BERT简介：</h1><blockquote><p>BERT开源仓库：<a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></p></blockquote><h2 id="一句话解释BERT："><a href="#一句话解释BERT：" class="headerlink" title="一句话解释BERT："></a>一句话解释BERT：</h2><p>是一个预训练的语言表征模型，是一个<strong>深度的、双向的Transformer</strong>，用来做<strong>预训练</strong>，针对一般的语言理解任务。</p><blockquote><p>Q：预训练是什么？</p><p>A：预训练就是，事先在一个较大的数据集上面训练好一个模型的这次训练过程（在这之后，再把这个模型拿到别的任务上面继续训练）。</p></blockquote><hr><h2 id="BERT出现的目的和意义："><a href="#BERT出现的目的和意义：" class="headerlink" title="BERT出现的目的和意义："></a>BERT出现的目的和意义：</h2><p>BERT的目的是通过<strong>在所有层中对左右上下文</strong>进行联合条件反射，从<strong>未标记的文本</strong>中预训练深度双向表示。</p><p>普适性：预训练的BERT模型可以通过一个<strong>额外的输出层</strong>进行<strong>微调</strong>，从而应用于更多样的任务，如问答和语言推理，并且无需对特定的任务架构进行大量的修改。</p><p>BERT出现的意义：让我们能够从一个大数据集上训练好一个比较深的神经网络，用于很多不同的NLP任务上面，既简化了训练，又提升了性能。</p><blockquote><p>Q：BERT名字的由来？</p><p>A：B&#x3D;Bidirectional, E&#x3D;Encoder, R&#x3D;Representations, T&#x3D;Transformers</p></blockquote><p>乍一看，怎么BERT这个名字和他的含义不是特别沾边啊？</p><p>事实上，这是因为BERT基于了ELMo的一些工作，而这两个名字都是美国的一个老少皆宜的少儿英语学习节目——<strong>芝麻街</strong>（笑）。</p><blockquote><p>芝麻街中文官网：<a href="https://www.sesamestreetchina.com.cn/">https://www.sesamestreetchina.com.cn/</a></p></blockquote><p>于是乎，一发不可收拾，NLP领域的文章就把芝麻街中人物的名字用了个遍。</p><p><img src="https://cdnjson.com/images/2023/05/19/image79f52a4bd5c81654.png" alt="avatar"></p><hr><h1 id="二、BERT与GPT、ELMo的关系"><a href="#二、BERT与GPT、ELMo的关系" class="headerlink" title="二、BERT与GPT、ELMo的关系"></a>二、BERT与GPT、ELMo的关系</h1><blockquote><p>由于BERT是基于<strong>GPT</strong>和<strong>ELMo</strong>这两个工作的，我们就先来看一下他们之间有什么区别。</p></blockquote><p><img src="https://cdnjson.com/images/2023/05/20/image2decf2d7bd0b6324.png" alt="avatar"></p><h2 id="BERT与GPT的区别："><a href="#BERT与GPT的区别：" class="headerlink" title="BERT与GPT的区别："></a>BERT与GPT的区别：</h2><p>在2018年，GPT是<strong>单向的</strong>，基于左边的上下文信息，来生成新的文段。（2023年有待考究）；</p><p>而BERT不仅用了左侧的信息，还用了右侧的信息，所以说是一个<strong>双向的</strong>操作，即Bidirectional。</p><h2 id="BERT与ELMo的区别："><a href="#BERT与ELMo的区别：" class="headerlink" title="BERT与ELMo的区别："></a>BERT与ELMo的区别：</h2><p>ELMo用的是基于RNN的架构，在用于一些特定的下游任务时，需要<strong>对架构做一定的调整</strong>；</p><p>BERT用的是Transformer，只需要<strong>改最上层</strong>的就行了，这里指的是BERT的那个额外的输出层。</p><blockquote><p>之后文章列举了自己在11个NLP任务上面的更好的表现，并且科学地给出了绝对值和相对值，这是非常优秀的一种写文细节，我们在写论文的时候也可以借鉴这种写法。</p></blockquote><hr><h1 id="三、BERT预训练"><a href="#三、BERT预训练" class="headerlink" title="三、BERT预训练"></a>三、BERT预训练</h1><h2 id="预训练在NLP的应用："><a href="#预训练在NLP的应用：" class="headerlink" title="预训练在NLP的应用："></a>预训练在NLP的应用：</h2><p>采用预训练的两大类自然语言任务：</p><ul><li><strong>句子层面</strong>的任务：通过整体分析来预测句子之间的关系，如对句子情绪的识别，或者两个句子之间的关系 ；</li><li><strong>词语层面</strong>的任务：标记级任务，如命名实体识别和问答（比如”张三”这个词是不是人名？还是一种食物？）。</li></ul><p>事实上，预训练在计算机视觉领域已经用了很多年了，BERT也不是自然语言处理领域第一个用预训练的，只是说，因为BERT的效果太好了，以至于**”抢了风头”**。</p><p>预训练的表示，减少了对许多高度工程化的任务的<strong>特定架构的需求</strong>。</p><p>BERT是第一个<strong>基于调优</strong>的表示模型，在大量句子级和记号级的任务上实现了最先进的性能，甚至优于很多特定于某种任务的架构。</p><hr><h2 id="NLP中两种预训练的策略："><a href="#NLP中两种预训练的策略：" class="headerlink" title="NLP中两种预训练的策略："></a>NLP中两种预训练的策略：</h2><p>语言模型预训练应用于下游任务，或者说特征表示，有两种现成的策略：基于特征、基于微调。</p><ul><li>基于特征：如ELMo——使用<strong>特定于任务</strong>的RNN架构，把学到的特征和输入一起作为特征的表达。</li><li>基于微调：如GPT——引入了最小的任务特定参数，并通过简单的<strong>一点点微调</strong>所有预训练参数，来对下游任务进行训练。</li></ul><p>在当时，这些的技术限制了预训练表征的力量，主要是因为标准语言模型是单向的。</p><p>传统的单向语言模型，或者是浅层拼接两个单向语言模型，只能获取单方向的上下文信息。</p><p>如果要获取句子层面，或者说句子之间的信息的话，就显得有些吃力了。</p><blockquote><p>那么，BERT是如何来解决这一问题的呢？</p></blockquote><hr><h2 id="MLM——来自远古的秘宝："><a href="#MLM——来自远古的秘宝：" class="headerlink" title="MLM——来自远古的秘宝："></a>MLM——来自远古的秘宝：</h2><p>BERT的使用Transformer的双向编码器表示来改进基于微调的方法；</p><p>使用带<strong>掩码的语言模型MLM</strong>, Masked Language Model对双向的Transformer进行预训练，缓解了单向性约束，生成深度的双向语言表征。</p><p>MLM使得能<strong>融合左右上下文</strong>，所以才能预训练深度双向Transformer，收到了Cloze的启发（这个竟然是1953年的论文）。</p><p>那么MLM在做什么呢？其实就是每一次<strong>随机选择一些字</strong>，然后**”盖住”<strong>，目标函数显而易见了，就是</strong>预测那些被盖住的字**。</p><p>有没有**”完形填空”**的感觉？事实上就是这样。</p><p>有了这个概念，我们就方便理解了，做完形填空的时候，就是不仅要看左边的文段，还要看右边的文段才行。</p><p>MLM除了完形填空之外，对于**”句子匹配”**也是有帮助的（怎么都是英语考试的题型）。</p><p>句子匹配的意思是说，<strong>随机的选两个句子</strong>，目标函数是要判断这两个句子在原文中<strong>是不是相邻的</strong>。</p><p>（有点像句子匹配的题目吧？虽然我也不知道那个题目具体叫什么名字）</p><hr><h2 id="有标注的数据集一定更好吗："><a href="#有标注的数据集一定更好吗：" class="headerlink" title="有标注的数据集一定更好吗："></a>有标注的数据集一定更好吗：</h2><p>事实上，BERT和它之后的一些工作证明了，有些时候，在NLP任务上，通过没有标号的、大量的数据集训练的模型，效果比有标注的、规模小一些的数据集上，效果更好。</p><p>这样的想法也在被计算机视觉等其他领域所采用。</p><hr><h2 id="BERT预训练与微调："><a href="#BERT预训练与微调：" class="headerlink" title="BERT预训练与微调："></a>BERT预训练与微调：</h2><h3 id="预训练Pre-Training："><a href="#预训练Pre-Training：" class="headerlink" title="预训练Pre-Training："></a>预训练Pre-Training：</h3><p>让我们再次回顾一下预训练在干什么，事实上，就是<strong>在一个没有标注的数据集上训练BERT模型</strong>。</p><p>有两个关键点：目标函数、预训练数据。</p><h3 id="微调Fine-Tuning："><a href="#微调Fine-Tuning：" class="headerlink" title="微调Fine-Tuning："></a>微调Fine-Tuning：</h3><p>同样是用的BERT模型，但是<strong>初始化权重为预训练过程中得到的权重</strong>。</p><p><strong>所有的权重</strong>，在微调的过程中都会参与训练，并且用的是<strong>有标号的数据</strong>。</p><p>基于预训练得到的模型，在每个后续的下游任务中都会训练出<strong>适用于当前任务的模型</strong>。</p><blockquote><p>以下是预训练和微调的图示：（后续还会介绍其细节，先大致浏览一下即可）</p></blockquote><p><img src="https://cdnjson.com/images/2023/05/19/image4ba18c7a6d70362c.png" alt="avatar"></p><blockquote><p>论文的这一部分对这些前置的技术做了简要的介绍，这是非常好的习惯。倘若是对于大家可能熟知的任务一笔带过，会显得整个文章不那么得体，不那么自洽。</p></blockquote><hr><h1 id="四、深入BERT模型"><a href="#四、深入BERT模型" class="headerlink" title="四、深入BERT模型"></a>四、深入BERT模型</h1><h2 id="BERT模型架构："><a href="#BERT模型架构：" class="headerlink" title="BERT模型架构："></a>BERT模型架构：</h2><p>简而言之，就是一个多层的、双向的Transformer编码器。</p><p>BERT调整了Transformer的三个参数：L（Transformer块的个数）、H（隐藏层维数）、A（自注意力机制的多头的头的数目）。</p><p>$BERT_{BASE}$：$L &#x3D; 12, H &#x3D; 768, A &#x3D; 12, Total\ Parameters&#x3D;110M$</p><p>$BERT_{LARGE}$：$L &#x3D; 24, H &#x3D; 1024, A &#x3D; 16, Total\ Parameters&#x3D;340M$</p><blockquote><p>如何把超参数换算成可学习参数的大小呢？</p></blockquote><h2 id="BERT模型可学习参数计算："><a href="#BERT模型可学习参数计算：" class="headerlink" title="BERT模型可学习参数计算："></a>BERT模型可学习参数计算：</h2><p>可学习参数主要分为两块：嵌入层、Transformer块，过程有图简述之：</p><p><img src="https://cdnjson.com/images/2023/05/19/imagef1add2ac16e1587b.png" alt="avatar"><br>自注意力机制本身并没有可学习参数，但是对于<strong>多头注意力</strong>，会把进入的所有K(Key)、V(Value)、Q(Query)都做一次投影，每次投影的维度为64，而且有$A × 64 &#x3D; H$；</p><p>如上，前面的自注意力块有$4H^2$个参数，MLP有$H×4H×2&#x3D;8H^2$个参数，则每个Transformer块中有$12H^2$个参数；</p><p>最后再乘$L$和字典大小，得到$Sum &#x3D; 30000H+12LH^2$，代入$BERT_{BASE}$可以获得确实是大约$110M$个参数。</p><hr><h2 id="BERT输入输出："><a href="#BERT输入输出：" class="headerlink" title="BERT输入输出："></a>BERT输入输出：</h2><p>输入：既可以是”句子”，也可以是”句子对”，不过事实上，这里的句子和句子对都不是狭义上的概念，事实上是一个**”序列”**。</p><p>因为BERT不同于一般的Transformer，<strong>BERT只有编码器</strong>，而Transformer有编码器和解码器，所以为了能够处理两个句子的情况，需要把两个句子变成一个序列。</p><h3 id="切词方法：WordPiece"><a href="#切词方法：WordPiece" class="headerlink" title="切词方法：WordPiece"></a>切词方法：WordPiece</h3><p>如果说按照空格来切词的话，也就是<strong>一个词对应一个Token</strong>，那样会导致<strong>字典大小特别大</strong>，根据上文的可学习参数计算方法，这会导致参数太多而且集中在嵌入层上面。</p><p>而WordPiece的想法是：如果一个词在整个文段里出现的<strong>概率不大</strong>的话，应该切开，看其<strong>子序列</strong>（有可能是一个词根）。</p><p>如果一个词根出现的概率比较大的话，就<strong>只保留词根</strong>就好了，这样的话会让<strong>字典大小缩小不少</strong>。</p><h3 id="合并句子的方法："><a href="#合并句子的方法：" class="headerlink" title="合并句子的方法："></a>合并句子的方法：</h3><p>每一个序列的<strong>第一个词</strong>都是<code>[cls]</code>，代表classification，因为最后的输出要代表整个序列的一个信息，这是因为BERT使用了Transformer的编码器，所以自注意力层里的每一个词都会看输入的所有词的关系，所以可以放在第一个位置。</p><p>需要对句子进行一定的区分，以从句子层面进行分类，这里有两个办法：</p><ul><li>为每个<strong>句子的末尾</strong>添加了一个<code>[SEP]</code>表示separate；</li><li>学习一个嵌入层，来知道某个句子是第一个还是第二个。</li></ul><p>这个时候我们再回去看一下上面给出过的图，就很好理解了：</p><p><img src="https://cdnjson.com/images/2023/05/19/image4ba18c7a6d70362c.png" alt="avatar"></p><p>如上图，每一个Token进入BERT，得到这个Token的Embedding表示。</p><p>整体上的效果就是：输入一个序列，得到一个序列，最后再添加额外的输出层来得到想要的结果。</p><p>对于每一个词元，进入BERT时的向量表示的是：</p><p>这个词元本身的Embedding + 它在哪个位置的Embedding + 位置的Embedding，如下图所示：</p><p><img src="https://cdnjson.com/images/2023/05/19/image856058d1358a11da.png" alt="avatar"></p><ul><li><strong>Token Embedding</strong>：词元的Embedding层，对于每一个词元，输出一个对应的向量；</li><li><strong>Segment Embedding</strong>：A表示第一句话，B表示第二句话；</li><li><strong>Position Embedding</strong>：位置的嵌入层，输入的大小是序列最长有多长，输入的是每一个词元的位置信息（0base）</li></ul><p>这些都是通过学习得来的，而不同于Transformer的手动构造。</p><hr><h2 id="MLM预训练的细节："><a href="#MLM预训练的细节：" class="headerlink" title="MLM预训练的细节："></a>MLM预训练的细节：</h2><p>知道了切词的方法，我们再来看所谓的”英语题”。</p><h3 id="完形填空："><a href="#完形填空：" class="headerlink" title="完形填空："></a>完形填空：</h3><p>对一个输入的词元序列，如果一个词元是<strong>由WordPiece生成</strong>的话（也就是切开来的），那就会有15%的概率<strong>随机替换成一个掩码</strong><code>[MASK]</code>。</p><p>（当然，对于<code>[CLS]</code>和<code>[SEP]</code>就不做替换了，这属于功能性的）</p><p>整体来看，如果输入序列是1000的话，那就要预测大约150个词。</p><p>当然，这一过程是MLM预训练独有的，在微调的过程中是看不到这一个过程的，更看不到<code>[MASK]</code>。</p><p>但是，这个<strong>替换</strong>的过程，也不是100%替换成<code>[MASK]</code>的，具体来说是这样的方案：</p><ul><li>80%：替换为<code>[MASK]</code></li><li>10%：替换为一个随机的词元</li><li>10%：什么都不做，但是用来做预测</li></ul><blockquote><p>附录的一个例子：</p></blockquote><img src="https://cdnjson.com/images/2023/05/20/imagec7866559f6d03693.png" alt="avatar" style="zoom: 80%;" /><h3 id="句子匹配："><a href="#句子匹配：" class="headerlink" title="句子匹配："></a>句子匹配：</h3><p>无论是在QA还是语言推理层面，都是一个句子对，所以最好让其学习一些句子层面的信息。</p><p>对于句子A和B，有可能是相邻，也有可能不是，那么相邻的就是正例，不相邻的就是负例，各占50%。</p><blockquote><p>附录的一个例子，简单读一下Input和Label就明白了：</p></blockquote><img src="https://cdnjson.com/images/2023/05/20/imagedb613b6793064687.png" alt="avatar" style="zoom: 80%;" /><p>值得一提的是，可以看到有一个<code>##</code>，这个意思是说，flightless这个词在原文中不常见，所以砍成了两个词，<code>##</code>的作用是表明这原本是一个词。</p><blockquote><p>两种不同策略的预训练精度结果（MLM和LR）：</p></blockquote><img src="https://cdnjson.com/images/2023/05/20/image4c8380af809bea06.png" alt="avatar" style="zoom:67%;" /><hr><h2 id="再看微调："><a href="#再看微调：" class="headerlink" title="再看微调："></a>再看微调：</h2><p>由于选择了单一个编码器的架构，而不是像传统的Transformer一样有编码器和解码器，所以会有一些缺点：</p><p>与GPT比，BERT用的是编码器，GPT用的是解码器。BERT做机器翻译、文本的摘要，也就是生成类的任务并不好做。</p><p>不过事实上，分类问题在NLP中更常见。</p><blockquote><p>如下图是BERT在分类任务上的表现情况：</p></blockquote><p><img src="https://cdnjson.com/images/2023/05/20/image20c21b2fae29114d.png" alt="avatar"></p><hr><h1 id="五、结论与尾声"><a href="#五、结论与尾声" class="headerlink" title="五、结论与尾声:"></a>五、结论与尾声:</h1><p>在本文发布的当时，语言模型迁移学习的经验改进表明，丰富的<strong>无监督预训练</strong>是许多<strong>语言理解系统</strong>的组成部分，这个效果是非常好的。</p><p>使得训练集比较少的任务也能够使用深度神经网络，获得比较好的效果。</p><p>特别是，这些结果使<strong>低资源任务</strong>也能从深度单向架构中受益。BERT的主要贡献是进一步将这些发现<strong>推广到深度双向架构</strong>，允许相同的预训练模型成功地处理<strong>广泛的</strong>NLP任务。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;零、前言：&quot;&gt;&lt;a href=&quot;#零、前言：&quot; class=&quot;headerlink&quot; title=&quot;零、前言：&quot;&gt;&lt;/a&gt;零、前言：&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;📕欢迎访问&lt;/strong&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;个人博客：&lt;a href=</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>计网 -  路由算法丨学习记录</title>
    <link href="https://conqueror712.github.io/post/Computer-Network-4-2.html"/>
    <id>https://conqueror712.github.io/post/Computer-Network-4-2.html</id>
    <published>2023-05-15T11:19:06.000Z</published>
    <updated>2023-05-31T00:17:43.892Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><p>注：本文图片的水印均是笔者自己的。</p><p>参考资料：《自顶向下的计算机网络》《王道计算机网络考研复习指导》</p><blockquote><p>本节概述：</p></blockquote><p>网络层的路由算法主要解决<strong>数据包从源地址到达目的地址的路径选择问题</strong>。主要的路由算法有：</p><ol><li><strong>距离 - 向量路由算法</strong>：每个路由器维护到各个网络的距离和方向信息，并周期性交换这个信息来更新自己的路由表，如RIP协议。</li><li><strong>链路状态路由算法</strong>：每个路由器发布自己与相邻路由器相连的链路信息，所有路由器据此构建整个网络拓扑结构图，并计算最短路径，如OSPF协议。</li><li><strong>层级路由算法</strong>：网络划分为多个层级，每个层级内部采用某种路由算法，层级之间采用另一种路由算法，如IS-IS协议。<br> 以上几种常见的路由算法根据网络规模和结构的不同，在网络层有着不同的应用，用以确定最优的数据传输路径。</li></ol><hr><h1 id="静态路由与动态路由："><a href="#静态路由与动态路由：" class="headerlink" title="静态路由与动态路由："></a>静态路由与动态路由：</h1><p>前面我们只知道，路由器转发分组这一过程，是通过<strong>路由表</strong>来转发的，但是并没有具体说路由表是怎么来的。</p><p>事实上，路由表是通过各种<strong>算法</strong>得到的。</p><p>从<strong>能否</strong>随网络的<strong>通信量</strong>或<strong>拓扑结构</strong>，来<strong>自适应地进行调整变化</strong>来划分，路由算法可以分为<strong>静态</strong>和<strong>动态</strong>两类。</p><h2 id="静态路由算法："><a href="#静态路由算法：" class="headerlink" title="静态路由算法："></a>静态路由算法：</h2><p><strong>非自适应的</strong>；</p><p>需要网络管理员<strong>手工配置</strong>的路由信息。</p><p>不能及时适应网络状态的变化，适用于简单的、变化少的小型网络。</p><p>特点：</p><ul><li>简便</li><li>开销小</li></ul><h2 id="动态路由算法："><a href="#动态路由算法：" class="headerlink" title="动态路由算法："></a>动态路由算法：</h2><p><strong>自适应的</strong>；</p><p>路由器上的<strong>路由表项</strong>是通过相互连接的路由器之间<strong>彼此交换信息</strong>，然后按照一定的<strong>算法</strong>优化而来的。</p><p>这些路由信息会在一定<strong>时间间隔</strong>里不断<strong>更新</strong>，以，适应不断变化的网络，从而随时获得最优的效果。</p><p>特点：</p><ul><li>可以改善网络性能</li><li>有助于流量控制</li><li>会增加网络负担</li><li>有可能因为反应太快而引起振荡</li><li>有可能因为反应太慢而影响网络路由的一致性</li></ul><p>接下来我们介绍两种常用的动态路由算法。</p><hr><h1 id="距离-向量路由算法："><a href="#距离-向量路由算法：" class="headerlink" title="距离 - 向量路由算法："></a>距离 - 向量路由算法：</h1><h2 id="算法描述："><a href="#算法描述：" class="headerlink" title="算法描述："></a>算法描述：</h2><p>所有节点都<strong>定期地</strong>将它们的<strong>整个路由选择表</strong>传送给所有与之<strong>直接相邻</strong>的节点（保证路由的有效性和一致性）。</p><p>这种路由选择表的组成：</p><ul><li>每条路径的<strong>目的地</strong></li><li>路径的**”代价”**（通常采用RIP算法，即采用”跳数”作为距离的度量）</li></ul><h2 id="更新路由表的两种情况："><a href="#更新路由表的两种情况：" class="headerlink" title="更新路由表的两种情况："></a>更新路由表的两种情况：</h2><ul><li>有一条<strong>新的路由</strong></li><li>某一条路由有<strong>更小的代价</strong></li></ul><h2 id="算法实质："><a href="#算法实质：" class="headerlink" title="算法实质："></a>算法实质：</h2><p><strong>迭代计算</strong>一条路由中的站段数或延迟时间，从而得到到达一个目标的最小代价通路。</p><p>更新报文的大小与通信子网的节点数<strong>成正比</strong>。</p><p>该算法更有可能遇到<strong>路由环路</strong>的问题。</p><hr><h1 id="链路状态路由算法："><a href="#链路状态路由算法：" class="headerlink" title="链路状态路由算法："></a>链路状态路由算法：</h1><p>每个参与该算法的节点具有<strong>完全的网络拓扑信息</strong>，它们执行以下两项任务：</p><ol><li><strong>主动测试</strong>所有<strong>邻接节点</strong>的状态</li><li><strong>定期地</strong>将链路状态传播给<strong>所有其他节点</strong></li></ol><p>典型的链路状态算法：OSPF算法。</p><h2 id="算法描述：-1"><a href="#算法描述：-1" class="headerlink" title="算法描述："></a>算法描述：</h2><p>对于一个节点，先检查所有<strong>直接链路</strong>的状态，并将所得的状态信息发送给网上的<strong>所有其他节点</strong>。</p><p>每当链路状态报文到达的时候，路由节点就会使用这些状态信息去试图更新自己的<strong>网络拓扑</strong>和<strong>状态视野图</strong>；</p><p>顺带一提，更新路由使用的是<strong>Dijkstra</strong>算法，因为是计算单源最短路。</p><h2 id="算法特征："><a href="#算法特征：" class="headerlink" title="算法特征："></a>算法特征：</h2><p>向所有的节点发送状态信息时，使用的是<strong>泛洪算法</strong>。</p><p>发送的信息是什么？事实上是<strong>链路状态</strong>，而且是与当前路由器<strong>相邻的</strong>所有路由器的链路状态，那么链路状态包括什么呢？</p><ul><li>代价</li><li>距离</li><li>时延</li><li>带宽</li></ul><p>只有当链路状态发生变化的时候，路由器才会泛洪。</p><h2 id="算法优势："><a href="#算法优势：" class="headerlink" title="算法优势："></a>算法优势：</h2><p>因为每个路由节点都使用干同样的原始状态数据，<strong>独立</strong>地计算路径，并且链路状态报文<strong>不加改变</strong>地传播，所以<strong>易于查找故障</strong>。</p><p>因为链路状态报文仅运载来自单个节点关于直接链路的信息，其大小与节点个数<strong>无关</strong>，所以有着比距离 - 向量算法更好的<strong>规模可扩展性</strong>。</p><hr><h1 id="层次路由："><a href="#层次路由：" class="headerlink" title="层次路由："></a>层次路由：</h1><h2 id="层次路由的必要性："><a href="#层次路由的必要性：" class="headerlink" title="层次路由的必要性："></a>层次路由的必要性：</h2><p>为什么需要层次路由呢？</p><p>我们以一张图来解释：</p><p><img src="https://cdnjson.com/images/2023/05/15/image1b76e277c2e2052d.png" alt="avatar"></p><h2 id="两大类路由选择协议："><a href="#两大类路由选择协议：" class="headerlink" title="两大类路由选择协议："></a>两大类路由选择协议：</h2><p>自治系统来源于<strong>被因特网划分的互联网</strong>。</p><p>两个自治系统之间需要通信，就需要一种协议来<strong>屏蔽</strong>两个自治系统之间的<strong>差异</strong>，所以有了路由选择协议的两大划分：</p><ul><li>IGP内部网关协议：一个<strong>自治系统内部</strong>使用的，具体的协议有RIP和OSPF等。</li><li>EGP外部网关协议：<strong>自治系统之间</strong>所使用的，也可以为分组在不同自治系统之间<strong>选择最优的路径</strong>，具体的协议有BGP。</li></ul><h2 id="关于OSPF的更多："><a href="#关于OSPF的更多：" class="headerlink" title="关于OSPF的更多："></a>关于OSPF的更多：</h2><p>使用层次路由时，OSPF将一个自治系统<strong>再划分为若干个区域</strong>，每个路由器都知道<strong>在本区域内如何把分组路由到目的地的细节</strong>，但是不用知道<strong>其他区域的内部结构</strong>。</p><p>采用分层次划分区域的方法，虽然会让交换信息的种类增多，也会使OSPF协议更复杂，但是！这样做却能使每个区域内部交换路由信息的<strong>通信量大大减小</strong>，所以说，OSPF协议能够用于<strong>规模很大的自治系统</strong>中。</p><hr><h1 id="附加-本节易错知识点习题："><a href="#附加-本节易错知识点习题：" class="headerlink" title="[附加] 本节易错知识点习题："></a>[附加] 本节易错知识点习题：</h1><blockquote><p>慢收敛，是导致发生路由回路的根本原因。</p></blockquote><p>慢收敛：在距离 - 向量路由协议中，好消息传得快，坏消息传得慢，这就导致了路由信息发生变化时，各路由器接收信息是异步的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;个人博客：&lt;a href=&quot;https://conqueror712.github.io/&quot;&gt;https://conqueror712.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;知乎：&lt;a href=&quot;https://www.zhihu.com/</summary>
      
    
    
    
    <category term="408" scheme="https://conqueror712.github.io/categories/408/"/>
    
    
  </entry>
  
  <entry>
    <title>计网 - 网络层的功能丨学习记录</title>
    <link href="https://conqueror712.github.io/post/Computer-Network-4-1.html"/>
    <id>https://conqueror712.github.io/post/Computer-Network-4-1.html</id>
    <published>2023-05-14T02:57:03.000Z</published>
    <updated>2023-05-31T00:17:50.317Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><p>注：本文图片的水印均是笔者自己的。</p><p>参考资料：《自顶向下的计算机网络》《王道计算机网络考研复习指导》</p><blockquote><p>本节概述：</p></blockquote><p>网络层：<strong>向上只提供简单灵活的、无连接的、尽力而为的数据报服务</strong>。</p><p>特点：传送的分组可能会出错、丢失、重复、乱序以及超时。</p><p>原因：降低网络构建的成本，灵活其运行方式以适用于多种应用，在今天看来，这是正确的。</p><hr><h1 id="异构网络互联："><a href="#异构网络互联：" class="headerlink" title="异构网络互联："></a>异构网络互联：</h1><h2 id="网络互连的概念："><a href="#网络互连的概念：" class="headerlink" title="网络互连的概念："></a>网络互连的概念：</h2><p>将两个以上的计算机网络，通过一定的方法，用一些<strong>中继系统</strong>相互连接起来，以构成更大的网络系统。</p><p>更准确地说，是指用<strong>路由器进行网络互连和路由选择</strong>。</p><p>（路由器是一台专用的计算机，用于在互联网中进行路由选择）</p><p>那中继系统又是什么呢？</p><h2 id="中继系统："><a href="#中继系统：" class="headerlink" title="中继系统："></a>中继系统：</h2><p>中继系统分类（按所在层次）：</p><ul><li>物理层：转发器、集线器</li><li>数据链路层：网桥、交换机</li><li><strong>网络层：路由器</strong></li><li>网络层以上：网关</li></ul><p>就这样，许多计算机网络就可以通过一些路由器进行互连；</p><h2 id="IP网："><a href="#IP网：" class="headerlink" title="IP网："></a>IP网：</h2><p>又因为参加互连的计算机网络都使用相同的<strong>IP协议</strong>，所以可以整体视为一个<strong>虚拟IP网络</strong>（逻辑互连网络）。</p><p>更进一步地说，就是说，互连起来的各种物理网络的<strong>异构性</strong>本来是客观存在的，但是通过<strong>IP协议</strong>，就可以使这些性能各异的网络在网络层上看起来好像是一个统一的网络，我们可以理解为<strong>封装</strong>。</p><p>使用IP网的<strong>好处</strong>：</p><p>当IP网上的主机进行通信的时候，其内部复杂的路由转发过程被掩盖，就像<strong>黑盒</strong>一样，简单明确。</p><hr><h1 id="路由与转发："><a href="#路由与转发：" class="headerlink" title="路由与转发："></a>路由与转发：</h1><p>路由器主要有两个功能：路由选择、分组转发。</p><h2 id="路由选择："><a href="#路由选择：" class="headerlink" title="路由选择："></a>路由选择：</h2><p>根据特定的<strong>路由选择协议</strong>，构造出<strong>路由表</strong>，同时经常或定期地和相邻路由器<strong>交换路由信息</strong>，从而不断地<strong>更新和维护路由表</strong>。</p><p>更进一步地说，是按照复杂的<strong>分布式算法</strong>，根据从各相邻路由器得到的关于整个<strong>网络拓扑的变化情况</strong>，<strong>动态</strong>地改变所选择的路由。</p><h2 id="分组转发："><a href="#分组转发：" class="headerlink" title="分组转发："></a>分组转发：</h2><p>处理<strong>通过路由器的数据流</strong>。关键操作是<strong>转发表查询、转发和相关的队列管理和任务调度</strong>等。</p><p>更进一步地说，是路由器<strong>根据转发表</strong>，将用户的<strong>IP数据报</strong>从合适的端口<strong>转发</strong>出去。</p><p>（在讨论路由选择原理的时候，往往不去区分转发表和路由表，都统称为路由表）</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d52ded8e99aa492597ff6e65322eceea~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><hr><h1 id="SDN的基本概念："><a href="#SDN的基本概念：" class="headerlink" title="SDN的基本概念："></a>SDN的基本概念：</h1><p>因为网络层的主要任务就是<strong>转发</strong>和<strong>路由选择</strong>，所以可以将网络层抽象地划分为：</p><ul><li>数据平面（转发）</li><li>控制平面（选择）</li></ul><p>SDN——软件定义网络，是近年流行的一种创新网络架构，采用<strong>集中式的控制平面</strong>和<strong>分布式的数据平面</strong>（互相分离）。</p><p>其中，其控制平面利用<strong>控制-数据接口</strong>，对数据平面上的路由器进行<strong>集中式的控制</strong>，从而方便软件来控制网络。</p><p>这使得路由器<strong>变得简单</strong>，砍掉了相互交换路由信息这一功能，集成在远程控制器中；</p><p>路由器只需要进行<strong>收到分组、查找转发表、转发分组</strong>这三项工作。</p><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/08280287cdde4863a663beb21474cd1d~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><h2 id="SDN的应用："><a href="#SDN的应用：" class="headerlink" title="SDN的应用："></a>SDN的应用：</h2><p>在某些具体条件下，例如一些<strong>大型的数据中心之间的广域网</strong>，使用SDN模式进行改造，就可以让网络的<strong>运行效率变高</strong>。</p><h2 id="SDN的可编程性："><a href="#SDN的可编程性：" class="headerlink" title="SDN的可编程性："></a>SDN的可编程性：</h2><p>SDN具有可编程性，为开发者们提供强大的编程接口（<strong>北向接口</strong>），使得网络具有很好的编程性。</p><p>SDN控制器和转发设备建立双向会话的接口称为<strong>南向接口</strong>；</p><p>通过不同的南向接口协议（如Openflow），SDN控制器就可以<strong>兼容不同的硬件设备</strong>，同时可以在设备中实现上层应用的逻辑。</p><p>SDN控制器集群内部控制器之间的通信接口称为<strong>东西向接口</strong>，用于增强整个控制平面的<strong>可靠性</strong>和<strong>可拓展性</strong>。</p><h2 id="SDN的优缺点："><a href="#SDN的优缺点：" class="headerlink" title="SDN的优缺点："></a>SDN的优缺点：</h2><p>优点：</p><ul><li>集中式与分布式的并行——既利于<strong>控制平面的全局优化</strong>，又利于<strong>高性能的网络转发</strong>；</li><li>两大功能分离——灵活可编程与性能的<strong>平衡</strong>；</li><li>两大功能分离——<strong>降低成本</strong>。</li></ul><p>缺点：</p><ul><li>集中管理——<strong>安全风险</strong>（更易攻击）</li><li>集中化的控制器——<strong>瓶颈问题</strong>（性能）</li></ul><hr><h1 id="拥塞控制："><a href="#拥塞控制：" class="headerlink" title="拥塞控制："></a>拥塞控制：</h1><p>概念：在通信子网中，因出现<strong>过量的分组</strong>而引起网络<strong>性能的下降</strong>的现象——拥塞。</p><h2 id="判断网络是否进入拥塞状态的方法："><a href="#判断网络是否进入拥塞状态的方法：" class="headerlink" title="判断网络是否进入拥塞状态的方法："></a>判断网络是否进入拥塞状态的方法：</h2><p>观察网络的<strong>吞吐量</strong>与<strong>网络负载</strong>的关系。</p><p>随着<strong>网络负载的增加</strong>：</p><ul><li>网络的吞吐量<strong>明显小于</strong>正常的吞吐量——<strong>轻度拥塞状态</strong>；</li><li>网络的吞吐量快速下降——<strong>拥塞状态</strong>；</li><li>网络的吞吐量降为零——<strong>死锁状态</strong>。</li></ul><p>这时，拥塞控制就来解决这一问题。</p><h2 id="流量控制与拥塞控制的区别："><a href="#流量控制与拥塞控制的区别：" class="headerlink" title="流量控制与拥塞控制的区别："></a>流量控制与拥塞控制的区别：</h2><p>流量控制：发送端与接收端的<strong>点对点</strong>的通信量控制，抑制发送速率，以便接收。</p><p>拥塞控制：必须确保通信子网能够传送待传送的数据，<strong>全局性</strong>问题。</p><h2 id="拥塞控制的两种方法："><a href="#拥塞控制的两种方法：" class="headerlink" title="拥塞控制的两种方法："></a>拥塞控制的两种方法：</h2><ul><li>开环控制：在设计时实现考虑到——<strong>静态方法</strong>。</li><li>闭环控制：采用监测网络系统，及时检测哪里发生了拥塞——<strong>动态方法</strong>。</li></ul><hr><h1 id="附加-本节易错知识点习题："><a href="#附加-本节易错知识点习题：" class="headerlink" title="[附加] 本节易错知识点习题："></a>[附加] 本节易错知识点习题：</h1><blockquote><p>路由器连接的异构网络是指：数据链路层和物理层均不同。</p></blockquote><p>网络的异构性指的是传输介质、数据编码方式、链路控制协议、不同的数据单元格式、不同的转发机制。</p><p>而这些特点分别在物理层和数据链路层中定义。</p><hr><blockquote><p>在路由器互联的多个局域网的结构中，要求每个局域网的物理层、数据链路层、网络层的协议可以不同，但是网络层以上的高层协议必须相同。</p></blockquote><p>路由器只能隐藏网络层及以下的具体细节，但是以上不可以。</p><hr><blockquote><p>可以分隔广播域的是路由器。</p></blockquote><p>路由器工作在网络层，不转发广播包，因此可以分隔广播域，抑制网络风暴。</p><p>交换机工作在数据链路层，能够分隔冲突域，但不能分隔广播域。</p><p>集线器和中继器是物理层设备，都不能分隔。</p><hr><blockquote><p>一个路由器的路由表通常包含：目的网络和到达该目的网络路径上的下一个路由器的IP地址。</p></blockquote><hr><blockquote><p>路由器在能够开始向输出链路传输分组的第一位之前，必须先接收到整个分组——存储转发机制。</p></blockquote><hr><blockquote><p>在因特网中，IP分组的传输需要经过源主机和中间路由器到达目的主机；</p><p>通常来说，源主机和中间路由器都不知道IP分组到达目的主机需要经过的完整路径。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;个人博客：&lt;a href=&quot;https://conqueror712.github.io/&quot;&gt;https://conqueror712.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;知乎：&lt;a href=&quot;https://www.zhihu.com/</summary>
      
    
    
    
    <category term="408" scheme="https://conqueror712.github.io/categories/408/"/>
    
    
  </entry>
  
  <entry>
    <title>MLC-LLM教程丨无需科技，浏览器就能用的大模型</title>
    <link href="https://conqueror712.github.io/post/MLC-LLM.html"/>
    <id>https://conqueror712.github.io/post/MLC-LLM.html</id>
    <published>2023-05-01T12:14:34.000Z</published>
    <updated>2023-05-01T13:00:31.339Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><blockquote><p>本文预计阅读时长：10分钟</p></blockquote><p>你是否还在为ChatGPT的注册、网络、封号等问题头疼？</p><p>别担心，人人都能<strong>本地使用</strong>的语言模型来了！</p><p>本文就来介绍<strong>MLC-LLM</strong>的浏览器使用方法。</p><p>特别说明：<strong>进行对话无需科技</strong>！</p><p>当然，它不仅可以通过浏览器来使用，还可以通过手机或电脑（不同于浏览器）来使用，</p><p>笔者认为，<strong>浏览器方法是最简单的一种</strong>，故撰写此文。</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><hr><h1 id="教程："><a href="#教程：" class="headerlink" title="教程："></a>教程：</h1><h2 id="官方文档："><a href="#官方文档：" class="headerlink" title="官方文档："></a>官方文档：</h2><p>首先，给出官方文档，英文阅读能力较强的读者可以自行前往进行参考，当然，笔者就是根据官方文档来学习的使用方法，只看官方文档显然也可以，不过可能会有一些解释不清的地方，不妨继续阅读本文以明确。</p><p>文档主页：<a href="https://mlc.ai/mlc-llm/">https://mlc.ai/mlc-llm/</a></p><p>浏览器方法页面：<a href="https://mlc.ai/web-llm/">https://mlc.ai/web-llm/</a></p><blockquote><p>心急的读者可能会直接在自己的浏览器上进行对话了，但是很遗憾，这大概并不能成功。</p></blockquote><hr><h2 id="环境配置："><a href="#环境配置：" class="headerlink" title="环境配置："></a>环境配置：</h2><p>读者不要看到环境配置就大惊失色，事实上，这并没有多么困难，甚至可以说是傻瓜式，一键式操作。</p><p>我们只需要下载一个<strong>开发者专用的Chrome浏览器</strong>，俗称金丝雀版Chrome。</p><p>下载地址：<a href="https://www.google.com/chrome/canary/%EF%BC%88%E9%9C%80%E8%A6%81%E7%A7%91%E6%8A%80%EF%BC%89">https://www.google.com/chrome/canary/（需要科技）</a></p><blockquote><p>笔者也为不方便使用科技的读者准备了下载链接：</p><p>链接：<code>https://pan.baidu.com/s/1X9RuHQkRYA07tCjimYIU4Q?pwd=8guf </code></p><p>提取码：<code>8guf</code></p></blockquote><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/70ceebe112e44907bcbf47cd5d145332~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>傻瓜式安装后，我们就可以通过金丝雀版Chrome打开上面的浏览器方法页面<a href="https://mlc.ai/web-llm/%E3%80%82">https://mlc.ai/web-llm/。</a></p><blockquote><p>此时，我们才可以真正的进行对话，不过在此之前，我们需要等待浏览器初始化，这大概需要两分钟。</p></blockquote><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1c72d1e8619a4b078f092d6771570168~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><blockquote><p>PS：如果没有进行初始化，那就输入一段话然后按一下<code>Send</code>试一试。</p></blockquote><hr><h2 id="效果演示："><a href="#效果演示：" class="headerlink" title="效果演示："></a>效果演示：</h2><p>初始化结束后，我们就可以愉快地对话了！</p><blockquote><p>值得一提的是，中文可能会出现乱码的情况，不过他会自己修复一些。</p></blockquote><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8852074afe154433854685e060a207f4~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><blockquote><p>需要注意的是，响应速度可能并不尽如人意，这大概与本机的配置有关，但这也不失为一种本地化使用AI的办法，当你的其他AI崩溃的时候，不妨来试一试MLC-LLM！</p></blockquote><p>这张任务管理器界面截图证明了这个确实是跑在本地上了，笔者的渣机内存和GPU占用率很高：</p><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ff639722a1ec459abf24b188dbba3784~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><hr><p>THE END.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;本文预计阅读时长：10分钟&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;你是否还在为ChatGPT的注册、网</summary>
      
    
    
    
    <category term="Experience" scheme="https://conqueror712.github.io/categories/Experience/"/>
    
    
  </entry>
  
  <entry>
    <title>面经丨BUPT第三届模拟面试大赛</title>
    <link href="https://conqueror712.github.io/post/Interview-BUPT.html"/>
    <id>https://conqueror712.github.io/post/Interview-BUPT.html</id>
    <published>2023-04-26T15:32:52.000Z</published>
    <updated>2023-04-26T16:08:19.349Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5e849f5a76724b0f8dad347fe57d8aff~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>笔者这次是作为决赛的选手参加的此次面试，人生中的第一次面试经历，收获颇丰，特此记录与分享。</p><p>本次活动是从面向整个人工智能学院的海选中选出的9位同学进行决赛，有数位领导参会，个人认为规格还是很高的。</p><hr><h1 id="无领导面试："><a href="#无领导面试：" class="headerlink" title="无领导面试："></a>无领导面试：</h1><p>以下是我在此次面试前做的一些准备资料：</p><h2 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h2><p>无领导小组讨论，俗称群面Group Interview.</p><p>小组成员以讨论的方式，共同应对一个需要解决的问题，经过各种观点和思想的碰撞和提炼，共同找出一个最合适的答案或结果。</p><p>面试官则在一旁对应聘者在讨论中的发言内容及左右局势的能力进行评估。</p><h2 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h2><ul><li>节约时间</li><li>让应聘者较为放松</li></ul><h2 id="评分标准："><a href="#评分标准：" class="headerlink" title="评分标准："></a>评分标准：</h2><ul><li><strong>语言方面的考核</strong>，通常包括应聘者的语言表达能力、辩论说服能力、组织协调能力、发言主动性、论点的正确性等。</li><li><strong>非语言方面的考核</strong>，包括应聘者的面部表情、身体姿势、语调、语速和手势等等。</li><li><strong>应聘者个性特点的考核</strong>，通常包括自信心、进取心、责任心、情绪稳定性、反应灵活性等等。在小组面试中，应聘者的所有举动都在面试官的视线里，所有的细节都有可能决定应聘者在小组面试中的成败。</li></ul><h2 id="注意事项："><a href="#注意事项：" class="headerlink" title="注意事项："></a>注意事项：</h2><ul><li>仔细听规则，适当记录</li><li>成败源于细节，态度决定一切</li><li>自我介绍不要紧张，声音要洪亮，语言清晰流畅，避免机械式的背书，多肢体语言，表情控制，自信</li><li>听一听别人的自我介绍，对对方有一个第一印象，方便后续的小组讨论与合作</li><li>拿到题目之后仔细审题，要有广度有深度</li><li>观点陈述要有条理，语言简洁凝练</li><li>小组讨论要有礼貌，注意发言顺序，要面向小组成员而不是面试官</li></ul><h2 id="角色划分："><a href="#角色划分：" class="headerlink" title="角色划分："></a>角色划分：</h2><ul><li>破冰者</li><li>领导者</li><li>时间控制者</li><li>总结汇报者</li><li>组织协调者</li><li>其他参与者</li></ul><hr><h1 id="面经分享："><a href="#面经分享：" class="headerlink" title="面经分享："></a>面经分享：</h1><h2 id="自我介绍环节："><a href="#自我介绍环节：" class="headerlink" title="自我介绍环节："></a>自我介绍环节：</h2><p>面试官提出的一些共性的问题：</p><ul><li>时间把握，给出的要求是90s，那么显然不能超过90s，也不能低于70s</li><li>讲述要有逻辑</li><li>注意语速，上台前可以先做深呼吸</li><li>注意一定要脱稿完成</li><li>第一印象很重要，一定要自信</li><li>归纳总结性的自我介绍为上，而不是流水账</li><li>项目经历和专业能力是很重要的，要多说</li></ul><h2 id="小组面试环节："><a href="#小组面试环节：" class="headerlink" title="小组面试环节："></a>小组面试环节：</h2><p><em>这部分记的内容不多，因为当时在台上没有可以记录的东西</em></p><ul><li>一定要做好自己是哪个角色的准备</li><li>尽可能展现自我的个性，展示自己的优势之处，让HR记住你</li><li>无领导小组面试很展现个性，一定要注意表现出来</li></ul><h2 id="随机应变环节："><a href="#随机应变环节：" class="headerlink" title="随机应变环节："></a>随机应变环节：</h2><ul><li><p>如果是作为领导的话，考虑员工的能力比性格更重要</p></li><li><p>避免口语化，注意逻辑性（上面也提到了这一点）</p></li><li><p>如果有一些棘手的背景设计，记得要为自己想办法争取时间</p></li><li><p>如果有多方案，要考虑能不能并行的做，两手或者多手抓</p></li><li><p>要灵活的面对问题，而不是被局限在问题里面，比如跳出二择的问题（因为两个可能都不是好的解决方法）</p></li><li><p>回答的时候要面向HR，而不是面向屏幕或者其他的什么东西</p></li><li><p>语速要适中，注意语音语调的变化</p></li><li><p>注意听问题的弦外之音，不懂就问（如果遇到领导的指示不明白的情况）</p></li><li><p>对于问题的情况，大概率背景是不那么明确的，注意要情况分类，类比二叉树</p></li><li><p>要注意更加侧重于解决方案而不是在背景分析里面打转</p></li><li><p>对于非常狗血的问题或者追问，一定要在适当的时候说拒绝回答，这样才是一个鲜活的人</p></li><li><p>面对问题，要想办法在侧面体现出自己的优势</p></li><li><p>注意每个问题是偏向于深度回答还是广度回答，一般来说，二者兼得会很不错，一个重要的指标是要合逻辑、通情理，这样才能够说服面试官</p></li><li><p>准备和经验要充分，多参加这种面试，多锻炼，各种场合的，不要害怕</p></li><li><p>可以自己录制自己的演讲视频，会发现很多问题</p></li></ul><hr><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/63f8287b88984234897487b2bc029f00~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>THE END</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h1 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5</summary>
      
    
    
    
    <category term="Interview" scheme="https://conqueror712.github.io/categories/Interview/"/>
    
    
  </entry>
  
  <entry>
    <title>计网 - Ep3 - 传输层「万字长文」丨学习记录</title>
    <link href="https://conqueror712.github.io/post/Computer-Network-3.html"/>
    <id>https://conqueror712.github.io/post/Computer-Network-3.html</id>
    <published>2023-04-20T06:39:28.000Z</published>
    <updated>2023-05-31T00:17:59.889Z</updated>
    
    <content type="html"><![CDATA[<p><em>Computer Networking A Top-Down Approach Learning Note Part 3</em></p><p><strong>前言</strong>：</p><p>本文是笔者初学计算机网络的笔记和一些心得，难免会有部分疏漏和错误，还请各位读者积极指出，不吝赐教。</p><p>有一些内容是笔者认为对自己暂时没那么重要的部分，就没有放上去，具体的内容可以查看相关的书籍。</p><p>观前提醒：本文篇幅较长，若您只是想看其中的某一小节的知识，直接点击目录进行跳转即可！</p><p>事不宜迟，我们开始吧！</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><p>注：本文图片的水印均是笔者自己的。</p><p>参考资料：《自顶向下的计算机网络》《王道计算机网络考研复习指导》</p><hr><h1 id="传输层概述"><a href="#传输层概述" class="headerlink" title="传输层概述"></a>传输层概述</h1><p>传输层，为<strong>运行在不同主机上的应用进程</strong>，提供<strong>直接的通信服务</strong>，起着至关重要的作用。</p><blockquote><p>Learning Target:</p></blockquote><p>传输层的工作原理：</p><ul><li>多路复用 &#x2F; 多路分解</li><li>可靠数据传输</li><li>流量控制</li><li>拥塞控制</li></ul><p>Internet的传输层协议：</p><ul><li>UDP：无连接传输</li><li>TCP：面向连接的可靠传输</li><li>TCP的拥塞控制</li></ul><hr><h2 id="概述和传输层服务："><a href="#概述和传输层服务：" class="headerlink" title="概述和传输层服务："></a>概述和传输层服务：</h2><blockquote><p>传输服务与协议</p></blockquote><p><strong>传输服务和协议</strong>目的：为运行在不同主机上的应用进程提供<strong>逻辑通信</strong>。</p><p>传输协议运行位置：端系统。</p><ul><li>发送方：将应用层来的报文分成<strong>报文段</strong>，然后继续传递给网络层。</li><li>接收方：将报文段重组成报文，然后继续传递给应用层。（相反的过程）</li></ul><p>还需注意的是，有很多传输层协议可以使用，对于Internet来说，就是TCP和UDP。</p><blockquote><p>我们来对比一下传输层和网络层的区别</p></blockquote><p>网络层服务：主机之间的逻辑通信。</p><p>传输层服务：进程之间的逻辑通信（更细）。</p><ul><li>传输层服务<strong>依赖于</strong>网络层的服务（延时和带宽，这两个不可以被加强）</li><li>传输层服务对网络层的服务进行<strong>增强</strong>（数据丢失、数据混乱、加密）</li></ul><p>笔者的理解：网络层建立起主机之间的逻辑通信之后，传输层才能在之上进行颗粒度更细的进程上的逻辑通信。</p><blockquote><p>Internet传输层协议</p></blockquote><p>TCP：</p><ul><li>多路复用、多路分解</li><li>建立连接</li><li>拥塞控制</li><li>流量控制</li></ul><p>UDP：</p><ul><li>多路复用、多路分解</li><li>没有为尽力而为的IP服务添加更多的额外服务</li></ul><hr><h2 id="传输层的寻址与端口："><a href="#传输层的寻址与端口：" class="headerlink" title="传输层的寻址与端口："></a>传输层的寻址与端口：</h2><h3 id="端口："><a href="#端口：" class="headerlink" title="端口："></a>端口：</h3><p>端口能够让应用层的各种应用进程将其数据通过端口<strong>向下交付</strong>给传输层，</p><p>以及让传输层知道应当将其报文段中的数据通过端口<strong>向上交付</strong>给应用层的相应进程。</p><p>端口是传输层服务访问点（T-SAP）；</p><p>（类比）端口在传输层的作用类似于IP地址在网络层的作用或MAC地址在数据链路层的作用；</p><p>不同的是，IP和MAC地址标识的是主机，而端口表示的是应用进程。</p><h3 id="端口号："><a href="#端口号：" class="headerlink" title="端口号："></a>端口号：</h3><p>应用进程通过端口号进行标识，端口号长度为16bits，能表示65536个不同的端口号。</p><p>应当注意的是，端口号只具有<strong>本地意义</strong>。</p><p><strong>端口号分类</strong>：</p><p>$[0, 1023]$的端口号称之为<strong>周知端口号</strong>，保留给HTTP(80)等周知应用层协议，由IANA指派。</p><p>$[1024, 49151]$的端口号称之为<strong>登记端口号</strong>，供没有周知端口号的应用程序使用，也需要在IANA上登记。</p><p>$[49152, 65535]$的端口号称之为<strong>临时端口</strong>，通信结束后，这些就会释放掉。</p><blockquote><p>IANA，互联网地址指派机构</p></blockquote><h3 id="套接字："><a href="#套接字：" class="headerlink" title="套接字："></a>套接字：</h3><p>套接字$Socket&#x3D;(IP地址:端口号)$</p><p>在网络中，采用发送方和接收方的套接字来识别端点，<strong>唯一地标识网络中的一台主机和其上的一个应用进程</strong>。</p><hr><h2 id="多路复用与多路分解："><a href="#多路复用与多路分解：" class="headerlink" title="多路复用与多路分解："></a>多路复用与多路分解：</h2><p>人话解释：就是将，由网络层提供的主机到主机的交付服务，延伸到，为运行在主机上的应用程序提供进程到进程的交付服务。</p><p>这是所有的计算机网络都需要的服务，而不仅仅是Internet独有的。</p><p>复习<strong>套接字</strong>：相当于网络和进程之间传递数据的门户。</p><blockquote><p>发送方主机——多路复用：</p></blockquote><p>含义：从多个套接字，接收来自多个进程的报文，根据套接字对应的<strong>IP地址和端口号</strong>等信息，对报文段<strong>用头部</strong>加以封装（该头部信息用于以后的解复用）。</p><blockquote><p>接收方主机——多路分解：</p></blockquote><p>含义：根据报文段的头部信息中的<strong>IP地址和端口号</strong>，将接收到的报文段发给正确的套接字（和对应的应用进程）。</p><ul><li>Port是端口号，用于区分进程</li><li>PID是进程的唯一标识</li></ul><p>如果还没有明白，可以仔细阅读这个例子 ：</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a52712255f34457cb2037b601864c844~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>传输层多路复用的要求：</p><ol start="0"><li>套接字有唯一标识符；</li><li>每个报文段有特殊字段来指示该报文段所要交付到的套接字；</li></ol><hr><h3 id="无连接的多路复用与多路分解："><a href="#无连接的多路复用与多路分解：" class="headerlink" title="无连接的多路复用与多路分解："></a>无连接的多路复用与多路分解：</h3><p>一个UDP套接字是由一个<strong>二元组</strong>来全面标识的：</p><p>该二元组包含一个<strong>目的IP地址</strong>和一个<strong>目的端口号</strong>。</p><p>那么，源端口号还有存在的必要吗？答案是有的，源端口号作为<strong>返回地址</strong>的一部分。</p><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5c493d6e7fb34784b7b5960e1ce8a57f~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><hr><h3 id="面向连接的多路复用与多路分解："><a href="#面向连接的多路复用与多路分解：" class="headerlink" title="面向连接的多路复用与多路分解："></a>面向连接的多路复用与多路分解：</h3><p>TCP套接字和UDP套接字的区别是，TCP套接字是由一个四元组来全面标识的：</p><p>四元组：源IP地址，源端口号，目的IP地址，目的端口号。</p><p>与UDP不同的是：</p><p>两个具有不同源IP地址或源端口号的到达TCP报文段将被定向到两个不同的套接字，除非TCP报文段携带了初始创建连接的请求。</p><blockquote><p>具体的细节并没有在此展开，笔者认为，一方面是因为写在一起过于纷繁复杂，不便于读者观看；另一方面是笔者自己也一时间看不下去那么多细节，所以就先Overview，之后在做题的过程中遇到问题再返回来查漏补缺。</p></blockquote><hr><p>Web服务器与TCP，先以一图以示之：</p><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3025661947be4a31a363b4102e0507b2~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><hr><h1 id="无连接传输-UDP："><a href="#无连接传输-UDP：" class="headerlink" title="无连接传输 - UDP："></a>无连接传输 - UDP：</h1><h2 id="UDP数据报："><a href="#UDP数据报：" class="headerlink" title="UDP数据报："></a>UDP数据报：</h2><p>UDP的全称到底是什么？事实上是：User Datagram Protocol，即用户数据报协议。</p><p>UDP传的是数据报。</p><p>与IP一样，也是<strong>尽力而为</strong>的，除了相比IP，更加细分了进程到进程以外（IP是主机到主机），几乎没有额外的内容了，仅仅增加了两个最基本的服务：<strong>复用和分用以及差错检测</strong>。</p><blockquote><p>UDP的首部格式以及UDP的优点：（其中，左图上面四个部分都是16bits的）</p></blockquote><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/03e2dc0c61a044ae93e73259f6689eb6~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>除此之外，UDP还支持一对一，一对多，多对一和多对多的交互通信。</p><p>UDP常用于一次性传输较少数据的网络应用，如DNS，SNMP等；</p><p>UDP也常用于多媒体应用，这对于实时性是有要求的；</p><p>UDP不保证可靠交付，但这不意味着应用对数据的要求是不可靠的，所有维护可靠性的工作可以由用户在应用层完成。</p><p>UDP是面向报文的，一次交付一个完整的报文，<strong>因此报文是UDP数据包处理的最小单位</strong>。</p><p>UDP的数据报包含两部分：UDP首部和用户数据。UDP的首部有8B，由四个字段组成，如上图所示↑，意义如下：</p><ul><li>源端口号：在需要对方回信时选用，不需要是全置零；</li><li>目的端口：这在终点交付报文时必须使用到；</li><li>长度：UDP数据报的长度，最小值是8（仅有首部）；</li><li>校验和：检测UDP数据报在传输中是否有错，有错就丢弃，该字段是可选的，当源主机不想计算校验和时置零。</li></ul><h3 id="UDP基于端口的分用："><a href="#UDP基于端口的分用：" class="headerlink" title="UDP基于端口的分用："></a>UDP基于端口的分用：</h3><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9c307f8342f24f63ae382ab726389bab~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><hr><h2 id="UDP校验："><a href="#UDP校验：" class="headerlink" title="UDP校验："></a>UDP校验：</h2><p>在计算校验和时，要在UDP数据报之前增加12B的<strong>伪首部</strong>，</p><p><strong>只是</strong>用于在计算校验和时，临时添加在UDP数据报的前面，得到一个临时的UDP数据报。</p><p>校验和需要用这个临时的UDP数据报来计算。</p><p>UDP数据报的首部和伪首部示意图：</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3b50b49c1d4d45f6961f0dcc61dbb079~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><h3 id="校验和的计算方法："><a href="#校验和的计算方法：" class="headerlink" title="校验和的计算方法："></a>校验和的计算方法：</h3><p>UDP校验和的计算方法与IP数据报首部校验和的计算方法类似，</p><p>但不同的是，IP数据报的校验和只检验IP数据报的首部，但是UDP的校验和则检查首部和数据部分。</p><blockquote><p>这部分的题目不多（也有可能是笔者见的比较少），具体的计算方法这里暂略。</p></blockquote><p>以下给出一个计算UDP校验和的例子：</p><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/da6f0e9bc34a4a6da50d5cf95ca872ed~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><h3 id="校验和的目的："><a href="#校验和的目的：" class="headerlink" title="校验和的目的："></a>校验和的目的：</h3><p>快速检测在被传输报文段中的差错（如比特反转）。</p><p>虽然这种方法的校错能力不强，但是有点是简单快捷。</p><hr><h1 id="面向连接的传输-TCP："><a href="#面向连接的传输-TCP：" class="headerlink" title="面向连接的传输 - TCP："></a>面向连接的传输 - TCP：</h1><blockquote><p>TCP是在不可靠的IP层之上实现的可靠的数据传输协议，主要解决传输的可靠、有序、不丢失和不重复的问题。</p></blockquote><h2 id="TCP报文段："><a href="#TCP报文段：" class="headerlink" title="TCP报文段："></a>TCP报文段：</h2><p>报文段是什么：TCP传送的数据单元称为<strong>报文段</strong>。</p><p>TCP报文段用处：</p><ul><li>运载数据</li><li>建立连接、释放连接和应答</li></ul><blockquote><p>组成概述</p></blockquote><p>一个TCP报文段分为首部和数据两部分，整个TCP报文段作为IP数据报的数据部分封装在IP数据报中。</p><p>首部的前20B是固定的，TCP首部最短就为20B，后面有4N字节是根据需要而增加的选项，长度是4B的整数倍。</p><p>如图所示：</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/896c531313c448e2a3cb3a297a1a876d~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><blockquote><p>各个字段的意义：（这部分有点多，但是大概是很重要的部分）</p></blockquote><h3 id="源端口和目的端口："><a href="#源端口和目的端口：" class="headerlink" title="源端口和目的端口："></a>源端口和目的端口：</h3><p>各占2B的大小。</p><p>端口是传输层与应用层的服务接口，传输层的<strong>复用</strong>和<strong>分用</strong>功能都要通过端口实现。</p><h3 id="序号："><a href="#序号：" class="headerlink" title="序号："></a>序号：</h3><p>占4B的大小，范围是$[0, 2^{32}-1]$，共$2^{32}$个序号。</p><p>（有的地方也称之为序列号，反正都是Sequence Number）</p><p>TCP为每个报文段分配一个序列号，<strong>用于报文段的重组和丢失检测</strong>。</p><p>∵TCP是面向字节流的（即TCP传送时时逐个字节传送的）</p><p>∴TCP连接传送的字节流中的<strong>每个字节都按顺序编号</strong>，</p><p>序号字段的值指的是本报文段所发送的数据的第一个字节的序号（类似于首地址）。</p><h3 id="确认号："><a href="#确认号：" class="headerlink" title="确认号："></a>确认号：</h3><p>占4B的大小，是<strong>期望</strong>收到对方<strong>下一个</strong>报文段的<strong>第一个</strong>数据字节的序号。</p><p>若确认号为N则说明序号到N-1为止的所有数据都已正确收到。</p><h3 id="数据偏移（首部长度）："><a href="#数据偏移（首部长度）：" class="headerlink" title="数据偏移（首部长度）："></a>数据偏移（首部长度）：</h3><p>占4bits， 首部长度指出TCP报文段的<strong>数据起始处</strong>距离TCP报文段的<strong>起始处</strong>有多远。</p><p>∵数据偏移的<strong>单位</strong>是32bits，即4B；</p><p>又∵4位二进制数能表示的最大值为15；</p><p>∴TCP首部的最大长度为60B。</p><blockquote><p>这里如果没理解的话可以看如下解释：</p><p>首部长度字段占4比特,最大值为15。但是这个15不是直接表示15个字节,而是表示15 * 4 &#x3D; 60个字节。 这是因为TCP报文段采用32位定界长度的格式,所以所有的字段都必须是4字节对齐的。所以当首部长度字段的值是1时,代表首部4字节;当值为2时,代表首部8字节;以此类推,最大值为15时,代表首部15 * 4 &#x3D; 60字节。</p></blockquote><h3 id="保留："><a href="#保留：" class="headerlink" title="保留："></a>保留：</h3><p>占6bits，目前置零。</p><h3 id="紧急位URG："><a href="#紧急位URG：" class="headerlink" title="紧急位URG："></a>紧急位URG：</h3><p>当<code>URG == 1</code>时，表明<strong>紧急指针字段</strong>有效，它告诉系统此报文段中有紧急数据，应当<strong>尽快传送</strong>。</p><p>不过，需要注意的是，URG需要配合首部中紧急指针字段（后续会介绍）使用，</p><p>即，数据<strong>从第一个字节到紧急指针所指的字节就是紧急数据</strong>。</p><h3 id="确认位ACK："><a href="#确认位ACK：" class="headerlink" title="确认位ACK："></a>确认位ACK：</h3><p>当<code>ACK == 1</code>时，<strong>确认号字段</strong>才有效。</p><p>TCP规定，在连接建立之后，所有传送的报文段都必须把ACK置1。</p><h3 id="推送位PSH："><a href="#推送位PSH：" class="headerlink" title="推送位PSH："></a>推送位PSH：</h3><p>当接收方TCP收到<code>PSH == 1</code>的报文段，就会<strong>尽快地交付</strong>给接收应用进程，而不是等到整个缓存都填满了再向上交付。</p><h3 id="复位位RST："><a href="#复位位RST：" class="headerlink" title="复位位RST："></a>复位位RST：</h3><p>当<code>RST == 1</code>时，表明TCP连接中出现严重差错，必须<strong>释放连接</strong>，然后再重新建立连接。</p><h3 id="同步位SYN："><a href="#同步位SYN：" class="headerlink" title="同步位SYN："></a>同步位SYN：</h3><p>当<code>SYN == 1</code>时，表示这是一个<strong>连接请求</strong> or <strong>连接接受</strong>的报文。</p><p>当<code>SYN == 1 &amp;&amp; ACK == 0</code>时，说明这是一个连接请求报文（还没建立连接），</p><p>若对方同意建立连接，则在响应报文中为<code>SYN == 1 &amp;&amp; ACK == 1</code>。</p><h3 id="终止位FIN："><a href="#终止位FIN：" class="headerlink" title="终止位FIN："></a>终止位FIN：</h3><p>用来<strong>释放一个连接</strong>，当<code>FIN == 1</code>时，说明此报文段的发送方的数据已经发送完毕，并要求释放连接。</p><h3 id="窗口："><a href="#窗口：" class="headerlink" title="窗口："></a>窗口：</h3><p>占2B，范围为$[0, 2^{16}-1]$。</p><p>它指的是，现在允许对方发送的数据量，这是因为接收方的数据缓存空间是有限的，</p><p>这个窗口值可以作为接收方让发送方设置其发送窗口大小的依据。</p><h3 id="校验和："><a href="#校验和：" class="headerlink" title="校验和："></a>校验和：</h3><p>占2B，校验和字段检验的范围包括首部和数据两部分。</p><p>与UDP一样，在计算校验和时，要在TCP报文段的前面加上12B的伪首部，具体来说：</p><ul><li>将UDP伪首部的协议字段的17改成6</li><li>UDP长度字段改成TCP长度</li></ul><p>其他的都和UDP一样。</p><h3 id="紧急指针："><a href="#紧急指针：" class="headerlink" title="紧急指针："></a>紧急指针：</h3><p>占2B，如上文，指出本报文段中紧急数据共多少字节。</p><h3 id="选项："><a href="#选项：" class="headerlink" title="选项："></a>选项：</h3><p>长度可变。</p><p>TCP最初只规定了一种选项，即<strong>最大报文段长度Maximum Segment Size(MSS)</strong> ，</p><p>MSS是TCP报文段中的<strong>数据字段</strong>的最大长度。</p><h3 id="填充："><a href="#填充：" class="headerlink" title="填充："></a>填充：</h3><p>这是为了是整个首部长度是4B的整数倍而设置的。</p><hr><h2 id="TCP连接管理："><a href="#TCP连接管理：" class="headerlink" title="TCP连接管理："></a>TCP连接管理：</h2><h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>因为TCP是面向连接的协议，所以每个TCP连接都要经历三个阶段：</p><ol start="0"><li>建立连接</li><li>传输数据</li><li>释放连接</li></ol><p>在TCP连接的建立过程中，需要解决以下三个问题：</p><ol start="0"><li>要让每一方都能够确知对方的存在</li><li>要允许双方协商一些参数</li><li>能够对运输尸体资源进行分配</li></ol><p>TCP把连接作为最基本的抽象，每条TCP连接有两个端点，但这端点不是主机，不是IP地址，也不是应用进程，而是套接字。</p><p>TCP采用C-S模式。</p><h3 id="TCP三次握手："><a href="#TCP三次握手：" class="headerlink" title="TCP三次握手："></a>TCP三次握手：</h3><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/35a2cf403af8456da067a5b4ce5d4b73~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>连接建立前，服务器进程处于LISTEN状态。</p><h4 id="第一步："><a href="#第一步：" class="headerlink" title="第一步："></a>第一步：</h4><p>如图所示，图上展示出的内容不再赘述，需要指出的是，SYN报文段不能携带数据，但要消耗掉一个序号；</p><p>此时，客户端进程进入SYN-SENT（同步已发送）状态。</p><h4 id="第二步："><a href="#第二步：" class="headerlink" title="第二步："></a>第二步：</h4><p>同意建立连接后，发回确认，并为该TCP连接分配缓存和变量；</p><p>确认报文段也不能携带数据，但是也要消耗掉一个序号；</p><p>此时，TCP服务器进程进入SYN-RCVD（同步收到）状态。</p><p>（与此同时，由于服务器端的资源是在第二次握手时分配的，而客户端则在第三次握手时才分配，这个空档期就可以用来对服务器进行SYN泛洪攻击）</p><h4 id="第三步："><a href="#第三步：" class="headerlink" title="第三步："></a>第三步：</h4><p>当客户端收到确认时，还要向服务器给出确认，并为该TCP连接分配缓存和变量；</p><p>该报文段<strong>可以携带数据</strong>，若不携带数据则不消耗序号；</p><p>此时，TCP客户端进程进入ESTABLISHED（已建立连接）状态。</p><h3 id="TCP四次挥手："><a href="#TCP四次挥手：" class="headerlink" title="TCP四次挥手："></a>TCP四次挥手：</h3><p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/466c9c7df52544b6aab275f15faf709b~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>与三次握手一样，如图所示，图上展示出的内容不再赘述；</p><p>需要指出的一点是，参与TCP连接的两个进程中的任意一方都可以主动终止该连接。</p><h4 id="第一步：-1"><a href="#第一步：-1" class="headerlink" title="第一步："></a>第一步：</h4><p>FIN报文段即使不携带任何数据，也会消耗一个序号；</p><p>此时，TCP客户进程进入FIN-WAIT-1（终止等待1）状态；</p><h4 id="第二步：-1"><a href="#第二步：-1" class="headerlink" title="第二步："></a>第二步：</h4><p>服务器收到连接释放报文段后，发出确认，进入CLOSE-WAIT（关闭等待）状态。</p><p>但我们要注意的是，TCP连接可以视为一个双有向边，即L-&gt;R关闭不影响R-&gt;L的数据传输。</p><h4 id="第三步：-1"><a href="#第三步：-1" class="headerlink" title="第三步："></a>第三步：</h4><p>如果服务器已经没有要向客户端发送的数据，就通知TCP释放连接，这时进入LAST-ACK（最后确认）状态。</p><blockquote><p>你真的要离开我了吗？</p></blockquote><h4 id="第四步："><a href="#第四步：" class="headerlink" title="第四步："></a>第四步：</h4><p>客户端收到连接释放报文段时，类似第二步地，也必须发出确认；</p><p>值得一提的是，这里有一个计时器，必须等待2MSL（最长报文段寿命）之后，客户端才能进入CLOSED（连接关闭）状态。</p><blockquote><p>选择题喜欢考察关于连接和释放的题目，ACK、SYN、FIN一定切记等于1。</p></blockquote><hr><h2 id="TCP流量控制："><a href="#TCP流量控制：" class="headerlink" title="TCP流量控制："></a>TCP流量控制：</h2><blockquote><p>下面以<strong>数据链路层</strong>的流量控制与可靠传输机制进行说明，在传输层也类似，亦会从中介绍。</p></blockquote><p>流量控制涉及对链路上的帧的<strong>发送速率的控制</strong>，以便接收方有足够的缓冲空间来接收每个帧。</p><p>流量控制的基本方法是<strong>由接收方控制发送方发送数据的速率</strong>。</p><p>流量控制的两种常见方式：</p><ul><li>停止-等待协议</li><li>滑动窗口协议</li></ul><h3 id="停止-等待-SW-流量控制基本原理："><a href="#停止-等待-SW-流量控制基本原理：" class="headerlink" title="停止-等待(SW)流量控制基本原理："></a>停止-等待(SW)流量控制基本原理：</h3><blockquote><p>一句话描述Stop-and-Wait：</p></blockquote><p>发送方每发送一帧，都要等待接收方的应答信号，之后<strong>才能</strong>发送下一帧；</p><p>接收方每接收一帧，都要反馈一个应答信号，表示可接收下一帧，若接收方不反馈应答信号，则发送方必须一直等待。</p><p>这样每次只发送一帧，然后陷入等待的这种过程，传输效率很低。</p><p><em>借用我初中年级主任的一句话：人生就是在这样你等我，我等他的过程中被浪费掉了。</em></p><h3 id="滑动窗口流量控制基本原理："><a href="#滑动窗口流量控制基本原理：" class="headerlink" title="滑动窗口流量控制基本原理："></a>滑动窗口流量控制基本原理：</h3><p>滑动窗口分为两个窗口：</p><ul><li>发送窗口：在任意时刻，发送方维持一组连续的，允许发送的帧的序号。</li><li>接收窗口：在任意时刻，接收方维持一组连续的，允许接收的帧的序号。</li></ul><p>$W_T$表示在还未收到对方确认信息的情况下，发送方最多还可以发送多少个数据帧，$W_R$的道理类似。</p><p>有如下例：</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/77f1eb94736a4430a726bd1d4380a846~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><blockquote><p>那么，窗口具体是如何滑动的呢？</p></blockquote><p>发送端每收到一个确认帧，发送窗口就向前滑动一个帧的位置；当发送窗口内没有可以发送的帧（即窗口内的帧全是已发送但未收到确认的帧）时，发送方就会停止发送，直到收到接收方发送的确认帧，使得窗口移动，移动之后，窗口内有了新的可以发送的帧，才开始继续发送。</p><p>接收端收到数据帧后，将窗口向前移动一个位置，并发回确认帧，若收到的数据帧落在接收窗口之外，则一律丢弃。</p><blockquote><p>好，那么滑动窗口有什么重要的特性吗？</p></blockquote><ol start="0"><li><p>只有接收窗口向前滑动，且接收方发送了确认帧时，发送窗口才有可能向前滑动（得收到确认帧）。</p></li><li><p>停止-等待协议、GBN协议和SR协议只在发送窗口与接收窗口的<strong>大小</strong>上有所差别：</p><ul><li>SW：发送窗口大小 &#x3D; 接收窗口大小 &#x3D; 1</li><li>GBN：发送窗口大小 &gt; 1，接收窗口大小 &#x3D; 1</li><li>SR：发送窗口大小 &gt; 1，接收窗口大小 &gt; 1</li></ul></li><li><p>只有接收窗口大小为1时，才能保证帧的<strong>有序接收</strong>。</p></li><li><p>数据链路层的滑动窗口大小在传输过程中时固定的，而传输层的则不是。</p></li></ol><h3 id="TCP如何使用窗口机制实现流量控制："><a href="#TCP如何使用窗口机制实现流量控制：" class="headerlink" title="TCP如何使用窗口机制实现流量控制："></a>TCP如何使用窗口机制实现流量控制：</h3><h4 id="接收窗口rwnd："><a href="#接收窗口rwnd：" class="headerlink" title="接收窗口rwnd："></a>接收窗口rwnd：</h4><p>在通信过程中，<strong>接收方根据自己接收缓存的大小，动态地调整发送方的窗口大小</strong>，</p><p>即调整TCP报文段首部中的<strong>窗口</strong>字段值来限制发送方向网络注入报文的速率。</p><h4 id="拥塞窗口cwnd："><a href="#拥塞窗口cwnd：" class="headerlink" title="拥塞窗口cwnd："></a>拥塞窗口cwnd：</h4><p>发送方根据其<strong>对当前网络的拥塞程度的估计而确定的窗口值</strong>，其大小与网络的带宽和时延密切相关。</p><blockquote><p>我们假设了接收方总是有足够大的缓存空间，故发送窗口的大小由网络的拥塞程度决定，故而等效为拥塞窗口。</p></blockquote><h3 id="传输层和数据链路层的流量控制的区别："><a href="#传输层和数据链路层的流量控制的区别：" class="headerlink" title="传输层和数据链路层的流量控制的区别："></a>传输层和数据链路层的流量控制的区别：</h3><p>传输层定义<strong>端到端</strong>用户之间的流量控制；</p><p>数据链路层定义<strong>两个中间的相邻节点</strong>的流量控制；</p><p>另外，数据链路层的滑动窗口协议和窗口大小不能动态变化，<strong>传输层的则可以动态变化</strong>。</p><hr><h2 id="TCP可靠传输："><a href="#TCP可靠传输：" class="headerlink" title="TCP可靠传输："></a>TCP可靠传输：</h2><h3 id="数据链路层的可靠传输："><a href="#数据链路层的可靠传输：" class="headerlink" title="数据链路层的可靠传输："></a>数据链路层的可靠传输：</h3><p>数据链路层的可靠传输使用<strong>确认</strong>和<strong>超时重传</strong>两种机制来完成。</p><ul><li><p>确认：</p><p>是一种无数据的控制帧，使得接收方可以让发送方知道哪些内容被正确接收；</p><p>有时为了图方便，会将确认捎带在一个回复帧中，称为<strong>捎带确认</strong>。</p></li><li><p>超时重传：</p><p>指发送方在发送某个数据帧后就开启一个计时器⏲，在一定的时间内，如果没有得到发送的数据帧的确认帧，那就重新发送该数据帧，直到发送成功为止。</p></li></ul><blockquote><p>那么问题来了，当数据帧传输到了以后出错了怎么办呢？答案是ARQ。</p></blockquote><p>自动重传请求（Automatic Repeat reQuest, ARQ）：</p><p>通过接收方请求发送方重传出错的数据帧，来恢复出错的帧。</p><p>传统自动重传请求分为三种：</p><ul><li>SW-ARQ</li><li>GBN-ARQ</li><li>SR-ARQ</li></ul><p>后两者是滑动窗口技术与请求重传技术的结合。</p><h3 id="传输层的可靠传输："><a href="#传输层的可靠传输：" class="headerlink" title="传输层的可靠传输："></a>传输层的可靠传输：</h3><p>TCP在IP层的不可靠的、尽力而为的服务的基础上建立了一种可靠数据传输服务。</p><p>TCP通过校验、序号、确认和重传等机制来达到目的，我们分别展开。</p><h4 id="校验："><a href="#校验：" class="headerlink" title="校验："></a>校验：</h4><p>与UDP一样，不再赘述。</p><h4 id="序号：-1"><a href="#序号：-1" class="headerlink" title="序号："></a>序号：</h4><blockquote><p>概括：</p></blockquote><p>TCP首部的<strong>序号</strong>字段，用来保证数据能够<strong>有序</strong>提交给应用层；</p><p>TCP把数据视为一个<strong>无结构但有序的字节流</strong>，序号建立在传送的字节流上，而非报文段。</p><blockquote><p>关于字节流的更多细节：</p></blockquote><p>TCP连接传送的数据流，把每个字节都编上序号，序号字段的值是指本报文段所发送的数据的第一个字节的序号。</p><p>有例图所示：</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d330bc54b0e4ffdbec326e0c6fffdb1~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>第一个报文段的序号是0，第二个报文段的序号是3，第三个是6，第四个是8。</p><h4 id="确认："><a href="#确认：" class="headerlink" title="确认："></a>确认：</h4><p>TCP首部的确认号是期望收到对方的下一个报文段的数据的第一个字节的序号，这想必不难理解。</p><p>此外，发送方的缓存区会缓存<strong>已经发送但是还没收到确认的报文段</strong>，以便在需要时重传。</p><p>很重要的一点是，TCP默认使用<strong>累计确认</strong>，即TCP只确认字节流中<strong>至第一个丢失字节为止的字节</strong>。</p><blockquote><p>有点不好理解？我们以一个例子说明：</p></blockquote><p>在上图中，例如，接收方B收到了发送方A发送的，包含字节<code>0~2</code>及字节6<del>7的报文段，但是，由于种种原因，接收方B并没有收到字节&#96;3</del>5&#96;的报文段，此时B仍在等待字节3及其后面的字节，因此<strong>B到A的下一个报文段将会把确认号字段置为3</strong>，以保证顺序。</p><h4 id="重传："><a href="#重传：" class="headerlink" title="重传："></a>重传：</h4><p>有两种情况会导致TCP对报文段进行重传：超时和冗余ACK。（这部分在后续的拥塞控制里面也有提及，故简要叙述）</p><blockquote><p>超时：</p></blockquote><p>TCP每发送一个报文段，就对这个报文段设置一次计时器。</p><p>当计时器设置的<strong>重传时间到期但还未收到确认时</strong>，就会重传这一报文段。</p><p>这其中有一种自适应的算法来确认加权平均往返时间$RTT_S$，由于这方面的考题不多，故算法不做介绍。</p><blockquote><p>冗余ACK</p></blockquote><p>TCP规定当发送方收到对同一个报文段的<strong>3个冗余ACK</strong>时，就可以认为跟在这个被确认报文段之后的报文段已经丢失。</p><p>简单举个例子：当A收到对于1号报文段的3个冗余ACK时，就可以认为2号报文段已经丢失，这时候，A就可以对2号报文段执行重传，这部分在后续的拥塞控制里的快重传有介绍，这里点到为止。</p><hr><h2 id="TCP拥塞控制："><a href="#TCP拥塞控制：" class="headerlink" title="TCP拥塞控制："></a>TCP拥塞控制：</h2><p>拥塞控制的意义：</p><p>防止过多的数据注入网络，保证网络中的路由器或链路<strong>不至于过载</strong>。</p><h3 id="关于拥塞："><a href="#关于拥塞：" class="headerlink" title="关于拥塞："></a>关于拥塞：</h3><p>当拥塞出现的时候，端点实际上并不了解拥塞发生的细节，对于通信连接的端点来说，只能”观察”到其现象，即通信时延的增加。</p><p>我们考虑以下这个例子：</p><p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/292ee130595e4c68ae1c46a0b17cf64e~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><hr><h3 id="控制与流量控制的区别："><a href="#控制与流量控制的区别：" class="headerlink" title="控制与流量控制的区别："></a>控制与流量控制的区别：</h3><p>拥塞控制：<strong>全局性</strong>的过程，让网络能够承受现有的网络负荷，涉及所有的主机、路由器以及与降低网络传输性能有关的所有因素。</p><p>流量控制：<strong>局部性</strong>的过程，是端到端的问题，与拥塞控制一样，是<strong>通过控制发送方发送数据的速率来达到控制效果</strong>。</p><hr><h3 id="两个窗口："><a href="#两个窗口：" class="headerlink" title="两个窗口："></a>两个窗口：</h3><p>TCP为什么要有两个窗口呢？</p><p>事实上，这是因为发送方在确定发送报文段的速率的时候，既要考虑接收方的接收能力，又要从全局考虑不要使网络发生拥塞。</p><ol start="0"><li><p>接收窗口rwnd：</p><p><strong>接收方</strong>根据<strong>目前接收缓存大小</strong>所许诺的最新窗口值，反映<strong>接收方的容量</strong>。</p><p>由接收方根据其放在TCP报文的首部的窗口字段，通知对方。</p></li><li><p>拥塞窗口cwnd：</p><p><strong>发送方</strong>根据自己估算的<strong>网络拥塞程度</strong>而设置的窗口值，反映<strong>网络的当前容量</strong>。</p><p>只要网络未出现拥塞，拥塞窗口就可以再大一些，以便把更多的分组发送出去，反之，一旦出现网络拥塞，拥塞窗口就会减小一些，以限制注入网络的分组数。</p></li></ol><p>这两者有一个上限：</p><p>发送窗口的上限值 &#x3D; $min[rwnd, cwnd]$</p><p>然而，接收窗口的大小是根据TCP报文首部的窗口字段通知发送方的，但是发送方该如何维护拥塞窗口呢？这时候就需要慢开始和拥塞避免算法了，请看下文。</p><blockquote><p>当然，我们上文中提到，上述条件成立的前提是，我们假设了接收方总是有足够大的缓存空间；</p><p>基于此，发送窗口的大小由网络的拥塞程度决定，故而等效为拥塞窗口。</p></blockquote><hr><h3 id="四种拥塞控制的算法："><a href="#四种拥塞控制的算法：" class="headerlink" title="四种拥塞控制的算法："></a>四种拥塞控制的算法：</h3><p>慢开始、拥塞避免、快重传、快恢复。</p><p>这四种算法是同时运用的。</p><blockquote><p>一句话概括：</p></blockquote><p>在TCP连接建立和网络出现超时的时候，采用慢开始和拥塞避免算法；</p><p>当发送方接收到冗余ACK时，采用快重传和快恢复算法。</p><h4 id="慢开始算法："><a href="#慢开始算法：" class="headerlink" title="慢开始算法："></a>慢开始算法：</h4><blockquote><p>一句话解释算法：</p></blockquote><p>在TCP刚刚连接好并开始发送TCP报文段时，先令拥塞窗口cwnd &#x3D; 1，即一个最大报文段长度MSS；</p><p>每当接收到一个对新报文段的确认后，cwnd++，即增大一个MSS；</p><p>这样逐步增大发送方的cwnd，可以使分组注入网络的速率更加合理。</p><blockquote><p>慢开始的慢到底是什么意思</p></blockquote><p>其实，这个慢的意思是一开始先不着急用很大的窗口来发送报文段，初始时，cwnd &#x3D; 1的效果类似于”试探”。</p><p>这就是慢的含义——花费一点时间来慢慢的进入正题。</p><p>使用慢开始算法后，每经过一个传输轮次RTT，cwnd就会加倍，也就是说是指数规律增长，这样一来，慢开始就会一直把cwnd增大到一个规定的<strong>慢开始门限ssthresh（阈值）</strong> ，然后改用拥塞避免算法。</p><blockquote><p>所谓RTT，就是往返时延Round-Trip Time</p><p>是指网络通信中一个数据包从发送端发送到接收端，再从接收端返回到发送端所需的时间。</p><p>在计算机网络中，往返时延是一个重要的网络性能指标，可以反映出网络的响应速度和稳定性。当我们在浏览网页、发送邮件、进行在线游戏等网络操作时，RTT 往返时延会直接影响我们的体验。较低的 RTT 往返时延可以提高网络响应速度和传输效率，较高的 RTT 往返时延则会导致网络传输变慢、卡顿、延迟等问题。</p></blockquote><h4 id="拥塞避免算法："><a href="#拥塞避免算法：" class="headerlink" title="拥塞避免算法："></a>拥塞避免算法：</h4><blockquote><p>一句话解释算法：</p></blockquote><p>每经过一个RTT，就让cwnd++，而不是加倍，使得cwnd线性缓慢增长。</p><p>据此，我们可以知道：</p><ul><li>if (cwnd &lt; ssthresh): use 慢开始算法</li><li>if (cwnd &gt; ssthresh): use 拥塞避免算法</li><li>if (cwnd &#x3D;&#x3D; ssthresh): use 慢开始算法 or 拥塞避免算法（一般来说是用拥塞避免算法）</li></ul><h4 id="网络拥塞的处理过程："><a href="#网络拥塞的处理过程：" class="headerlink" title="网络拥塞的处理过程："></a>网络拥塞的处理过程：</h4><p>无论是哪个阶段，只要发送方判断网络出现拥塞（未按时收到确认ACK），</p><p>就要把慢开始门限ssthresh设置为<strong>出现拥塞时的发送方的cwnd值的一半</strong>（但不能小于2），</p><p>然后，把cwnd重新设置为1，继续执行慢开始算法。</p><blockquote><p>特别地，在慢开始阶段，若2cwnd&gt;ssthresh，则令下一个RTT后的cwnd&#x3D;ssthresh而不是2cwnd。</p></blockquote><p>如例图所示：</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d0a5472f16c14e7eb4891778a84daeab~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><p>然而，需要注意的是，这两种算法都不能完全避免拥塞，只是可以缓解拥塞。</p><blockquote><p>所谓ACK，是指确认（Acknowledgment）的缩写，它是一种用于确认收到数据的信号。在TCP&#x2F;IP协议中，当一个计算机接收到另一个计算机发送的数据包时，会向发送方发送一个ACK信号，表示已经成功接收到数据。这个ACK信号是由接收方发送给发送方的，以告诉发送方数据已经被正确接收。在TCP协议中，发送方会等待一个ACK信号来确认数据已经被成功接收，如果在一定时间内没有收到ACK信号，就会认为数据包丢失或损坏，然后重新发送数据。因此，ACK在计算机网络中非常重要，它能够确保数据的可靠传输。</p></blockquote><hr><p>快重传和快恢复算法是对慢开始和拥塞避免算法的改进。</p><h4 id="快重传："><a href="#快重传：" class="headerlink" title="快重传："></a>快重传：</h4><blockquote><p>一句话解释算法：</p></blockquote><p>我们已经知道的是，TCP可靠传输机制中，利用了快重传技术，通过冗余ACK来检测丢包的发生。</p><p>在这里，我们利用冗余ACK来检测网络的拥塞，其实是同一套道理（因为丢了包不就意味着网络可能出现了拥塞嘛）。</p><p>快重传并不是取消了重传计时器，而是在某些情况下更早地重传丢失的报文段。</p><p>当发送方<strong>连续收到三个重复的ACK</strong>时，<strong>直接重传</strong>对方尚未接收到的报文段，而<strong>不必等待</strong>那个报文段设置的重传计时器超时。</p><h4 id="快恢复："><a href="#快恢复：" class="headerlink" title="快恢复："></a>快恢复：</h4><blockquote><p>一句话解释算法：</p></blockquote><p>当发送方连续收到三个冗余ACK时，把慢开始门限ssthresh设置为此时cwnd的一半，这是为了预防网络发生拥塞。</p><p>这一过程与慢开始有点类似，但是不同的是，快恢复会把cwnd值设置为慢开始门限ssthresh改变后的数值，而不是1，然后直接开始进行拥塞避免算法。如下图例子所示：</p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/db292a83e0ce429083e80b39bc2a520d~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p><hr><blockquote><p>后续，笔者会给出章节相关的重难点和做题经验，敬请期待。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;Computer Networking A Top-Down Approach Learning Note Part 3&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;本文是笔者初学计算机网络的笔记和一些心得，难免会有部分疏漏和错误</summary>
      
    
    
    
    <category term="408" scheme="https://conqueror712.github.io/categories/408/"/>
    
    
  </entry>
  
  <entry>
    <title>Neo4j - Ep3丨学习记录</title>
    <link href="https://conqueror712.github.io/post/Neo4j-3.html"/>
    <id>https://conqueror712.github.io/post/Neo4j-3.html</id>
    <published>2023-04-09T05:21:28.000Z</published>
    <updated>2023-04-16T13:56:16.186Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><blockquote><p>Abstract：本文的主要内容是<strong>图形数据建模、领域知识对建模的重要性以及图数据模型和实例模型之间的区别</strong>。</p></blockquote><p>笔者近日在学习有关知识图谱Knowledge Graph的相关内容。</p><p>Neo4j是项目所需的一款功能强大的应用，特此来学习之，并记录于本文。</p><p>笔者是AI领域的小白，作为初学者，文章中难免会有出错或者不恰当的部分，烦请读者朋友们指出（在以下任意平台）。</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><p><strong>注意：本文不是教程，只是个人学习的记录和心得，可能会对你有帮助，建议配合官方文档一起学习！</strong></p><p>官方文档链接：<a href="https://graphacademy.neo4j.com/courses/modeling-fundamentals/1-getting-started/">https://graphacademy.neo4j.com/courses/modeling-fundamentals/1-getting-started/</a></p><p>再注：原文是英文，若有较好的英文阅读能力可以直接读，本文是对原文中重要的内容进行了翻译，以及加上了笔者自己的理解。</p><hr><h1 id="初窥门径："><a href="#初窥门径：" class="headerlink" title="初窥门径："></a>初窥门径：</h1><h2 id="什么是图形数据建模："><a href="#什么是图形数据建模：" class="headerlink" title="什么是图形数据建模："></a>什么是图形数据建模：</h2><p>首先我们要知道的是Neo4j中，<strong>图的组成部分</strong>有哪些，具体来说有四个组件，分别是：</p><ul><li>节点</li><li>标签</li><li>关系</li><li>属性</li></ul><p>而<strong>创建图形数据模型</strong>的步骤又如下所示：</p><ol><li>了解领域并为应用程序定义特定用例（问题）。</li><li>开发初始图形数据模型：<ul><li>对节点（实体）建模。</li><li>对节点之间的关系建模。</li></ul></li><li>针对初始数据模型测试用例。</li><li>使用 Cypher 使用测试数据创建图形（实例模型）。</li><li>测试用例，包括针对图表的性能。</li><li>由于关键用例的变化或性能原因，重构（改进）图形数据模型。</li><li>在图上实施重构并使用 Cypher 重新测试。</li></ol><p>由此可见，图形数据建模事实上是一个迭代的过程，有些读者可能会觉得那么频繁的重构是很麻烦的，不过事实上，与RDBMS相比，利用Cypher语言构建的Neo4j图形修改起来是十分灵活和方便的。</p><hr><h2 id="模型的目的："><a href="#模型的目的：" class="headerlink" title="模型的目的："></a>模型的目的：</h2><p>在为应用程序执行图形数据建模过程时，至少需要两种类型的模型：</p><ul><li>数据模型</li><li>实例模型</li></ul><blockquote><p>听起来名字差不多？那么，这两种模型分别都是什么呢？</p></blockquote><p>事实上：</p><ul><li>数据模型描述的是图形的<strong>标签、关系和属性</strong>。</li><li>实例模型是用来测试模型用例的，这是图形数据建模的重要一环。</li></ul><p>这是一个数据模型的例子：</p><p><img src="https://cdnjson.com/images/2023/04/09/image.png" alt="avatar"></p><p>而这是一个实例模型的例子：</p><img src="https://cdnjson.com/images/2023/04/09/image77db515ea60ba928.png" alt="avatar" style="zoom:50%;" /><hr><h1 id="建模节点："><a href="#建模节点：" class="headerlink" title="建模节点："></a>建模节点：</h1><p>这一部分主要学习的是：</p><ul><li>从您的用例中识别实体。</li><li>在图中创建节点以支持数据模型。</li></ul><hr><h2 id="定义标签："><a href="#定义标签：" class="headerlink" title="定义标签："></a>定义标签：</h2><p>实体是应用程序用例中的主导名词，比如：<strong>食谱</strong>中主要使用了哪些<strong>成分</strong>？</p><p>加粗的<strong>名词</strong>就是所谓的实体。</p><p>而这些实体就是在图中的节点，我们定义标签也是用名词来定义，也就是节点的”名字”。</p><h2 id="定义属性："><a href="#定义属性：" class="headerlink" title="定义属性："></a>定义属性：</h2><p>节点不仅有名字（标签），还要有属性，属性是用来唯一标识一个节点的，还可以回答应用程序用例的具体细节和返回数据。</p><p>具体问题需要具体分析。</p><hr><h1 id="建模关系："><a href="#建模关系：" class="headerlink" title="建模关系："></a>建模关系：</h1><p>这一部分主要学习的是：</p><ul><li>从用例中识别关系。</li><li>在图中创建关系以支持数据模型。</li></ul><p>首先我们要知道的是，关系是实体之间的联系，一般来说是<strong>动词</strong>。</p><p>比如：食谱中主要<strong>使用了</strong>哪些成分？</p><p>虽然这看上去不是一件很难的事情，但是它们的微观和宏观设计可以说是<strong>图形性能</strong>中最关键的因素。</p><h2 id="命名关系："><a href="#命名关系：" class="headerlink" title="命名关系："></a>命名关系：</h2><p>如上例，”使用了”就是一种命名，不能随便命名，以防混淆。</p><h2 id="关系方向："><a href="#关系方向：" class="headerlink" title="关系方向："></a>关系方向：</h2><p>在Neo4j中创建关系的时候，一定是一条有向的边，倘若真的需要无向边，那就正反都创建就好了。</p><p>值得一提的是，关系可以是两个节点之间的，也可以是一个节点的自环。</p><h2 id="Fanout："><a href="#Fanout：" class="headerlink" title="Fanout："></a>Fanout：</h2><p>直接翻译成扇出不知道合不合适，我们就暂时先用Fanout了。</p><p>在这里，我们的实体（Person、Residence）不是表示为单个节点，而是表示为网络或链接节点。</p><img src="https://cdnjson.com/images/2023/04/14/image067c2e266ab99c52.png" alt="avatar" style="zoom:50%;" /><h2 id="关系属性："><a href="#关系属性：" class="headerlink" title="关系属性："></a>关系属性：</h2><p>在这里我们看到我们在MARRIED关系上有一个日期属性来进一步描述 Michael 和 Sarah 之间的关系。</p><p>此外，我们在WORKS_AT关系上有一个角色属性来描述迈克尔在 Graph Inc. 工作时拥有或拥有的角色。</p><img src="https://cdnjson.com/images/2023/04/14/image0a34e05dcbb509f4.png" alt="avatar" style="zoom: 50%;" /><hr><h1 id="测试模型："><a href="#测试模型：" class="headerlink" title="测试模型："></a>测试模型：</h1><p>在本模块中，我们将测试图是否可以用于测试模型的用例。</p><p>简单来说，在测试用例期间，我们需要做的是：</p><ul><li>向图中添加更多数据以测试可扩展性。</li><li>测试和修改用于测试用例的任何 Cypher 代码。</li><li>如果无法回答用例，则重构数据模型。</li></ul><hr><h1 id="重构图："><a href="#重构图：" class="headerlink" title="重构图："></a>重构图：</h1><p>在本模块中，我们将学习：</p><ul><li>为什么我们重构一个图数据模型和图。</li><li>向数据模型添加标签。</li></ul><h2 id="为什么重构："><a href="#为什么重构：" class="headerlink" title="为什么重构："></a>为什么重构：</h2><p>重构是<strong>更改数据模型和图形</strong>的过程。</p><p>重构的三个原因：</p><ul><li>建模的图表并没有回答所有的用例。</li><li>出现了一个新的用例，您必须在数据模型中考虑该用例。</li><li>用例的 Cypher 表现不佳，尤其是当图形缩放时。</li></ul><h2 id="重构步骤："><a href="#重构步骤：" class="headerlink" title="重构步骤："></a>重构步骤：</h2><p>要重构图形数据模型和图形，您必须：</p><ol><li>设计新的数据模型。</li><li>编写 Cypher 代码来转换现有图形以实现新的数据模型。</li><li>重新测试所有用例，可能使用更新的 Cypher 代码。</li></ol><blockquote><p>具体内容待补充</p></blockquote><hr><h1 id="消除重复数据："><a href="#消除重复数据：" class="headerlink" title="消除重复数据："></a>消除重复数据：</h1><h2 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h2><p>我们应该注意避免在图表中重复数据。</p><p>有些数据库需要一种<strong>非规范化形式</strong>来提高一组查询的速度，而图形数据库并不总是这样。</p><p>去重数据给你带来了额外的好处，允许你通过一个节点进行查询。</p><p>例如，找到购买了特定产品的其他客户，或者根据其他用户的评分找到类似的电影。</p><p>此外，在图中复制数据会增加图的大小以及查询可能需要检索的数据量。</p><blockquote><p>具体内容待补充</p></blockquote><hr><h1 id="特殊化关系："><a href="#特殊化关系：" class="headerlink" title="特殊化关系："></a>特殊化关系：</h1><h2 id="为什么要重构图表以特殊化关系："><a href="#为什么要重构图表以特殊化关系：" class="headerlink" title="为什么要重构图表以特殊化关系："></a>为什么要重构图表以特殊化关系：</h2><ul><li>减少需要检索的节点数。</li><li>提高查询性能。</li></ul><p>特殊关系允许您遵循特定的关系类型并避免在单个查询中遍历图的大部分。</p><h2 id="如何建立动态关系："><a href="#如何建立动态关系：" class="headerlink" title="如何建立动态关系："></a>如何建立动态关系：</h2><p>使用Cypher中的APOC库，这是图形引擎在运行时可用的库中的专用过程。</p><blockquote><p>具体内容待补充</p></blockquote><hr><h1 id="中间节点："><a href="#中间节点：" class="headerlink" title="中间节点："></a>中间节点：</h1><p>有时，我们会发现需要将更多数据连接到关系的情况，而不是可以在属性中完全捕获的数据。</p><p>换句话说，您需要连接两个以上节点的关系。</p><p>数学通过<strong>超边</strong>的概念允许这样做。这在 Neo4j 中是不可能的，但我们也有办法，解决方案是创建中间节点。</p><h2 id="流程："><a href="#流程：" class="headerlink" title="流程："></a>流程：</h2><ul><li>在单个上下文中连接两个以上的节点。<ul><li>超边（n 元关系）</li></ul></li><li>将某事与一段关系联系起来。</li><li>在实体之间共享图中的数据。</li></ul><blockquote><p>具体内容待补充</p></blockquote><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Abstract：本文的主要内容是&lt;strong&gt;图形数据建模、领域知识对建模的重要性以及图数据模型和实例</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习 - Ep4 - 卷积神经网络丨学习记录</title>
    <link href="https://conqueror712.github.io/post/DL-CNN.html"/>
    <id>https://conqueror712.github.io/post/DL-CNN.html</id>
    <published>2023-04-08T11:48:55.000Z</published>
    <updated>2023-04-23T03:07:54.906Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>本篇内容记录笔者学习深度学习的学习过程，如果你有任何想询问的问题，欢迎在以下任何平台提问！</p><p>参考书：《动手学深度学习》</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><hr><h1 id="从全连接层到卷积："><a href="#从全连接层到卷积：" class="headerlink" title="从全连接层到卷积："></a>从全连接层到卷积：</h1><p><img src="https://cdnjson.com/images/2023/04/08/image713e2a24b5495899.png" alt="avatar"></p><ul><li>图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。</li><li>局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。</li><li>在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。</li><li>卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。</li><li>多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。</li></ul><hr><h1 id="图像卷积："><a href="#图像卷积：" class="headerlink" title="图像卷积："></a>图像卷积：</h1><h2 id="互相关运算："><a href="#互相关运算：" class="headerlink" title="互相关运算："></a>互相关运算：</h2><p>卷积神经网络的设计更多的用于图像数据，</p><p>而严格来说，”卷积”这个词用的有点问题，事实上，它所做的运算是<strong>互相关运算</strong>而非卷积运算。</p><p>在卷积层中，<strong>输入张量和核张量通过互相关运算产生输出张量</strong>。</p><p>我们以一个例子来说明，话不多说，上图：</p><p><img src="https://cdnjson.com/images/2023/04/19/image.png" alt="avatar"></p><hr><h2 id="卷积层和卷积核的区别："><a href="#卷积层和卷积核的区别：" class="headerlink" title="卷积层和卷积核的区别："></a>卷积层和卷积核的区别：</h2><ul><li>卷积层是深度神经网络中的一种基本层级结构，其目的是从输入数据中提取有用的特征。</li><li><strong>卷积层由多个卷积核组成</strong>，每个卷积核用于对输入数据进行卷积操作，从而生成输出特征图。</li></ul><p>卷积核是卷积层中的一个重要参数，它定义了卷积操作的形式和卷积层的特征提取能力。</p><p>卷积核通常是一个小的、固定大小的矩阵，可以视为卷积运算中的滤波器。</p><p>卷积核中的每个元素都具有一定的权重，用于控制对输入数据中不同位置的响应。</p><p>在具体操作中，卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。</p><hr><h2 id="图像中目标的边缘检测："><a href="#图像中目标的边缘检测：" class="headerlink" title="图像中目标的边缘检测："></a>图像中目标的边缘检测：</h2><p>通过找到像素变化的位置，来检测图像中不同颜色的边缘。</p><p>举个例子（例子来源于《动手学深度学习》）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><p>我们构造一个高度为1、宽度为2的卷积核<code>K</code>。</p><p>当进行互相关运算时，<strong>如果水平相邻的两元素相同，则输出为零，否则输出为非零。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br></pre></td></tr></table></figure><p>现在，我们对参数<code>X</code>（输入）和<code>K</code>（卷积核）执行互相关运算。 如下所示，输出<code>Y</code>中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y = corr2d(X, K)</span><br><span class="line">Y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><p>现在我们将输入的二维图像转置，再进行如上的互相关运算。</p><p>其输出如下，之前检测到的垂直边缘消失了。 不出所料，这个卷积核<code>K</code><strong>只可以检测垂直边缘，无法检测水平边缘</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corr2d(X.t(), K)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><hr><h1 id="学习卷积核："><a href="#学习卷积核：" class="headerlink" title="学习卷积核："></a>学习卷积核：</h1><p>对于更复杂数值的卷积核或者连续的卷积层来说，我们就不太可能去手动设计滤波器了；</p><p>这时候就需要通过”学习”来构造适合的卷积核了。</p><p>先构造一个卷积层，并将其卷积核初始化为随机张量。</p><p>接下来，在每次迭代中，我们比较<code>Y</code>与卷积层输出的平方误差，然后计算梯度来更新卷积核。</p><p>一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><hr><ul><li>二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。</li><li>我们可以设计一个卷积核来检测图像的边缘。</li><li>我们可以从数据中学习卷积核的参数。</li><li>学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。</li><li>当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。</li></ul><hr><h1 id="填充和步幅："><a href="#填充和步幅：" class="headerlink" title="填充和步幅："></a>填充和步幅：</h1><h2 id="填充："><a href="#填充：" class="headerlink" title="填充："></a>填充：</h2><p>在应用多层卷积时，我们常常丢失边缘像素。</p><p>由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。</p><p>但随着我们应用许多连续卷积层，累积丢失的像素数就多了。</p><p>解决这个问题的简单方法即为<strong>填充（padding）</strong>：在输入图像的边界填充元素（通常填充元素是0）。</p><p>如下例，我们将3×3输入填充到5×5，那么它的输出就增加为4×4：</p><p><img src="https://cdnjson.com/images/2023/04/19/imagecd0d3843ecbae19a.png" alt="avatar"></p><hr><h2 id="步幅："><a href="#步幅：" class="headerlink" title="步幅："></a>步幅：</h2><p>在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。</p><p>在前面的例子中，我们默认每次滑动一个元素。</p><p>但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。</p><p>我们将每次滑动元素的数量称为<strong>步幅（stride）</strong>。</p><p><img src="https://cdnjson.com/images/2023/04/19/image698e0910fe9a35f9.png" alt="avatar"></p><hr><ul><li>填充可以增加输出的高度和宽度。这常用来使输出与输入具有相同的高和宽。</li><li>步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的$1&#x2F;n$（n是一个大于1的整数）。</li><li>填充和步幅可用于有效地调整数据的维度。</li></ul><hr><h1 id="多输入多输出通道："><a href="#多输入多输出通道：" class="headerlink" title="多输入多输出通道："></a>多输入多输出通道：</h1><h2 id="多输入通道："><a href="#多输入通道：" class="headerlink" title="多输入通道："></a>多输入通道：</h2><p>当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。</p><p><img src="https://cdnjson.com/images/2023/04/19/image2d706cc4ae16c640.png" alt="avatar"></p><hr><h2 id="多输出通道："><a href="#多输出通道：" class="headerlink" title="多输出通道："></a>多输出通道：</h2><p>在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。</p><p>直观地说，我们可以将每个通道看作对不同特征的响应。</p><p>而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。</p><p>因此，多输出通道并不仅是学习多个单通道的检测器。</p><hr><ul><li>多输入多输出通道可以用来扩展卷积层的模型。</li><li>当以每像素为基础应用时，1×1卷积层相当于<strong>全连接层</strong>。</li><li>1×1卷积层通常用于<strong>调整网络层的通道数量</strong>和<strong>控制模型复杂性</strong>。</li></ul><hr><h1 id="池化层："><a href="#池化层：" class="headerlink" title="池化层："></a>池化层：</h1><h2 id="最大池化层与平均池化层："><a href="#最大池化层与平均池化层：" class="headerlink" title="最大池化层与平均池化层："></a>最大池化层与平均池化层：</h2><p>池化层，也称汇聚层。</p><p>池化层的两个目的：</p><ul><li>降低卷积层对位置的敏感性；</li><li>降低对空间降采样表示的敏感性。</li></ul><p>与卷积层类似，池化层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口遍历的每个位置计算一个输出。</p><p>然而，不同于卷积层中的输入与卷积核之间的互相关计算，池化层不包含参数。</p><p>相反，池运算是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。</p><p>这些操作分别称为<em>最大池化层</em>（maximum pooling）和<em>平均池化层</em>（average pooling）。</p><p><img src="https://img.picgo.net/2023/04/19/image01b296e555bb4e42.png" alt="avatar"></p><hr><h2 id="填充与步幅："><a href="#填充与步幅：" class="headerlink" title="填充与步幅："></a>填充与步幅：</h2><p>类似地，与卷积层一样，池化层也可以改变输出形状，通过改变填充和步幅。</p><hr><h2 id="多个通道："><a href="#多个通道：" class="headerlink" title="多个通道："></a>多个通道：</h2><p>在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 这意味着汇聚层的输出通道数与输入通道数相同。</p><hr><ul><li>对于给定输入元素，最大汇聚层会输出该窗口内的最大值，平均汇聚层会输出该窗口内的平均值。</li><li>汇聚层的主要优点之一是减轻卷积层对位置的过度敏感。</li><li>我们可以指定汇聚层的填充和步幅。</li><li>使用最大汇聚层以及大于1的步幅，可减少空间维度（如高度和宽度）。</li><li>汇聚层的输出通道数与输入通道数相同。</li></ul><hr><h1 id="卷积神经网络之——LeNet："><a href="#卷积神经网络之——LeNet：" class="headerlink" title="卷积神经网络之——LeNet："></a>卷积神经网络之——LeNet：</h1><p>LeNet是一种卷积神经网络，由Yann LeCun等人在1990年代提出，是卷积神经网络的开山鼻祖之一。</p><p>LeNet在当时被广泛应用于手写数字识别等任务，并为后来更复杂的卷积神经网络的设计奠定了基础。</p><p>虽然LeNet已经相对简单，但其基本的卷积神经网络结构已经被广泛应用于现代深度学习模型中。</p><p>因此，<strong>LeNet通常也被用作卷积神经网络的统称</strong>。</p><h2 id="LeNet："><a href="#LeNet：" class="headerlink" title="LeNet："></a>LeNet：</h2><p>总体来看，LeNet（LeNet-5）由两个部分组成：</p><ul><li>卷积编码器：由两个卷积层组成；</li><li>全连接层密集块：由三个全连接层组成。</li></ul><p><img src="https://img.picgo.net/2023/04/19/image7fe28db16e3136e3.png" alt="avatar"></p><p>简化版如下：</p><img src="https://img.picgo.net/2023/04/19/image92cfb05af2c1888f.png" alt="avatar" style="zoom: 67%;" /><ul><li>卷积神经网络（CNN）是一类使用卷积层的网络。</li><li>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层。</li><li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li><li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li><li>LeNet是最早发布的卷积神经网络之一。</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h1&gt;&lt;p&gt;本篇内容记录笔者学习深度学习的学习过程，如果你有任何想询问的问题，欢迎在以下任何平台提问！&lt;/p&gt;
&lt;p&gt;参考书：《动手学深度学习》</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Neo4j - Ep2丨学习记录</title>
    <link href="https://conqueror712.github.io/post/Neo4j-2.html"/>
    <id>https://conqueror712.github.io/post/Neo4j-2.html</id>
    <published>2023-04-02T01:46:08.000Z</published>
    <updated>2023-04-03T03:29:18.093Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><blockquote><p>Abstract：本文的主要内容是<strong>图数据库的入门</strong>以及<strong>Cypher语言的基础知识</strong>。</p></blockquote><p>笔者近日在学习有关知识图谱Knowledge Graph的相关内容。</p><p>Neo4j是项目所需的一款功能强大的应用，特此来学习之，并记录于本文。</p><p>笔者是AI领域的小白，作为初学者，文章中难免会有出错或者不恰当的部分，烦请读者朋友们指出（在以下任意平台）。</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><p><strong>注意：本文不是教程，只是个人学习的记录和心得，可能会对你有帮助，建议配合官方文档一起学习！</strong></p><p>官方文档链接：<a href="https://graphacademy.neo4j.com/courses/cypher-fundamentals/1-reading/">https://graphacademy.neo4j.com/courses/cypher-fundamentals/1-reading/</a></p><p>再注：原文是英文，若有较好的英文阅读能力可以直接读，本文是对原文中重要的内容进行了翻译，以及加上了笔者自己的理解。</p><hr><h1 id="从Neo4j读取数据："><a href="#从Neo4j读取数据：" class="headerlink" title="从Neo4j读取数据："></a>从Neo4j读取数据：</h1><blockquote><p>Cypher语言可以帮助在图中检索数据，包括节点，关系（也就是边），还可以过滤查询Filter queries。</p></blockquote><h2 id="Cypher简介："><a href="#Cypher简介：" class="headerlink" title="Cypher简介："></a>Cypher简介：</h2><p>Cypher是一种为图而设计的<strong>查询语言</strong>。</p><p>一个很重要的概念是：Cypher的模式pattern：</p><ul><li><strong>节点</strong>由圆括号表示 <code>()</code>.</li><li>使用冒号来表示<strong>标签</strong> <code>(:Person)</code>.</li><li>节点之间的<strong>关系</strong>用两个破折号表示， <code>(:Person)--(:Movie)</code>.</li><li>使用大于或小于符号<code>&lt;</code>或<code>&gt;</code>表示<strong>关系的方向</strong>，<code>(:Person)-&gt;(:Movie)</code>.</li><li><strong>关系的类型</strong>是用两个破折号之间的方括号写的<code>[</code> and <code>]</code>，<code>[:ACTED_IN]</code></li><li>在<em>语音气泡</em>中绘制的<strong>属性</strong>类似<code>JSON</code>的语法指定；<ul><li>Neo4j中的属性是<code>key-value对</code>， <code>&#123;name: &#39;Tom Hanks&#39;&#125;</code>.</li></ul></li></ul><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(m:Movie &#123;title: &#x27;Cloud Atlas&#x27;&#125;)&lt;-[:ACTED_IN]-(p:Person)</span><br></pre></td></tr></table></figure><blockquote><p>此模式中的两个节点类型是Movie和Person。</p><p>Person节点与Movie节点有直接的ACTED_IN关系。</p><p>这个模式中的特定Movie节点由值为Cloud Atlas的title属性过滤。</p><p>这个图形代表了所有在电影 Cloud Atlas 中扮演角色的人。</p></blockquote><h3 id="检索-MATCH："><a href="#检索-MATCH：" class="headerlink" title="检索 - MATCH："></a>检索 - MATCH：</h3><p>那么，Cypher是<strong>怎么工作</strong>的呢？</p><p>Cypher的工作原理是匹配数据中的模式；</p><p>我们使用<code>MATCH</code>关键字从图中检索数据；</p><p>可以认为<code>MATCH</code>类似于<code>SQL</code>语句中的<code>FROM</code>。</p><p>例如，如果我们想要在图中找到<strong>一个Person</strong>，我们将<code>MATCH</code>一个带有标签的单个节点的模式<code>:Person</code> -前缀为冒号。</p><p>假设我们想从图中检索<strong>所有Person</strong>节点；</p><p>我们可以通过在冒号前放置一个值来赋值一个变量； </p><p>使用变量<code>p</code>表示从图中检索到的<strong>所有Person</strong>节点，并使用<code>RETURN</code>返回它们。</p><p>就像这样：</p><p>（注意，<code>SHIFT + ENTER</code>可以换行输入，这个界面在右下角可以点开）</p><img src="https://cdnjson.com/images/2023/04/02/image6f9925aec568421d.png" alt="avatar" style="zoom:67%;" /><p>这太酷了，简直符合我对<del>Cyber</del>cCypher的想象！</p><p>我们还可以看到返回的Text，具体来说，该查询返回图中带有Person标签的所有节点。可以使用图视图或表视图查看返回的结果。在选择表视图时，还可以看到返回的节点的属性。</p><img src="https://cdnjson.com/images/2023/04/02/imageb1130e8fe1625bf9.png" alt="avatar" style="zoom: 67%;" /><p>如果我们要具体指定地查询某一个人，应该如何操作呢？</p><p>事实上，就像这样：</p><img src="https://cdnjson.com/images/2023/04/02/imagee952588b70d20236.png" alt="avatar" style="zoom:67%;" /><p>更进一步地，如果我只想知道他的出生年份，如何查询呢？</p><p>很简单，只需要<code>RETURN p.born</code>就可以了，这里就不再演示了。</p><hr><h3 id="检索-WHERE："><a href="#检索-WHERE：" class="headerlink" title="检索 - WHERE："></a>检索 - WHERE：</h3><p>筛选查询的另一种方法是使用<code>WHERE</code>子句，而不是用大括号内联指定属性值。</p><p>Example:</p><img src="https://cdnjson.com/images/2023/04/02/imageaa5fee26e225cfd0.png" alt="avatar" style="zoom:67%;" /><p>一般来说，<code>WHERE</code>在今后会更常用，因为其功能更强大。</p><p>Example:</p><img src="https://cdnjson.com/images/2023/04/02/image18621067e7323998.png" alt="avatar" style="zoom:67%;" /><hr><h3 id="使用与编写习惯："><a href="#使用与编写习惯：" class="headerlink" title="使用与编写习惯："></a>使用与编写习惯：</h3><p>在Cypher中，<strong>标签、属性键和变量</strong>是区分大小写的。Cypher<strong>关键字</strong>不区分大小写。</p><p>一言以蔽之，就是<strong>自定义的东西区分大小写</strong>。</p><p>推荐方式：</p><ul><li>标签：<code>CamelCase</code></li><li>属性键和变量：<code>camelCase</code></li><li>关键字：<code>UPPERCASE</code></li></ul><blockquote><p>随后，你可以在网站中的课后习题中进行练习，那都是一些比较简单和基础但是不可谓不重要的题目。</p></blockquote><hr><h2 id="寻找关系："><a href="#寻找关系：" class="headerlink" title="寻找关系："></a>寻找关系：</h2><p>扩展<code>MATCH</code>子句中的模式，以遍历具有<code>ACTED_IN</code>类型的所有关系到任何节点。</p><p>Domain Model显示<code>ACTED_IN</code>关系从<code>Person</code>节点向外延伸，因此我们可以在模式中添加方向，通常将此称为<strong>遍历traversal</strong>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person &#123;name: &#x27;Tom Hanks&#x27;&#125;)-[:ACTED_IN]-&gt;(m)</span><br><span class="line">RETURN m.title</span><br></pre></td></tr></table></figure><p>当然，我们也可以进行一些指定：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person &#123;name: &#x27;Tom Hanks&#x27;&#125;)-[:ACTED_IN]-&gt;(m:Movie)</span><br><span class="line">RETURN m.title</span><br></pre></td></tr></table></figure><hr><h2 id="过滤查询："><a href="#过滤查询：" class="headerlink" title="过滤查询："></a>过滤查询：</h2><p>Example:</p><p>此查询检索Person节点和Movie节点，该人员在2008或2009年发布的电影中扮演角色。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)</span><br><span class="line">WHERE m.released = 2008 OR m.released = 2009</span><br><span class="line">RETURN p, m</span><br></pre></td></tr></table></figure><img src="https://cdnjson.com/images/2023/04/02/imagefba0d2efeca010cc.png" alt="avatar" style="zoom:67%;" /><h3 id="按节点标签进行过滤："><a href="#按节点标签进行过滤：" class="headerlink" title="按节点标签进行过滤："></a>按节点标签进行过滤：</h3><p>Solution1:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)</span><br><span class="line">WHERE m.title=&#x27;The Matrix&#x27;</span><br><span class="line">RETURN p.name</span><br></pre></td></tr></table></figure><p>Solution2:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p)-[:ACTED_IN]-&gt;(m)</span><br><span class="line">WHERE p:Person AND m:Movie AND m.title=&#x27;The Matrix&#x27;</span><br><span class="line">RETURN p.name</span><br></pre></td></tr></table></figure><hr><h3 id="使用范围进行过滤："><a href="#使用范围进行过滤：" class="headerlink" title="使用范围进行过滤："></a>使用范围进行过滤：</h3><p><code>a &lt;= XX &lt;= b</code></p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)</span><br><span class="line">WHERE 2000 &lt;= m.released &lt;= 2003</span><br><span class="line">RETURN p.name, m.title, m.released</span><br></pre></td></tr></table></figure><hr><h3 id="根据属性的存在进行筛选："><a href="#根据属性的存在进行筛选：" class="headerlink" title="根据属性的存在进行筛选："></a>根据属性的存在进行筛选：</h3><p><code>IS NOT NULL</code></p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)</span><br><span class="line">WHERE p.name=&#x27;Jack Nicholson&#x27; AND m.tagline IS NOT NULL</span><br><span class="line">RETURN m.title, m.tagline</span><br></pre></td></tr></table></figure><hr><h3 id="按部分字符串筛选："><a href="#按部分字符串筛选：" class="headerlink" title="按部分字符串筛选："></a>按部分字符串筛选：</h3><p><code>START WITH</code>  <code>END WITH</code> <code>CONTAINS</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[:ACTED_IN]-&gt;()</span><br><span class="line">WHERE p.name STARTS WITH &#x27;Michael&#x27;</span><br><span class="line">RETURN p.name</span><br></pre></td></tr></table></figure><p>大小写转换：</p><p><code>toLower(p.name)</code> <code>toUpper(p.name)</code></p><hr><h3 id="根据图中的模式进行过滤："><a href="#根据图中的模式进行过滤：" class="headerlink" title="根据图中的模式进行过滤："></a>根据图中的模式进行过滤：</h3><p>假设你想找出所有的，写过一部电影但没有导演这部电影的人。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[:WROTE]-&gt;(m:Movie)</span><br><span class="line">WHERE NOT exists( (p)-[:DIRECTED]-&gt;(m) )</span><br><span class="line">RETURN p.name, m.title</span><br></pre></td></tr></table></figure><hr><h3 id="使用列表过滤："><a href="#使用列表过滤：" class="headerlink" title="使用列表过滤："></a>使用列表过滤：</h3><p>如果你有一组想要测试的值，你可以将它们放在列表中，或者可以使用图中的现有列表进行测试。</p><p>Cypher列表是<strong>方括号内以逗号分隔的值集</strong>。</p><p>可以在<code>WHERE</code>子句中定义列表。在查询期间，图形引擎将每个属性与列表中的值进行比较。您可以在列表中放置数字值或字符串值，但通常情况下，列表的元素是相同类型的数据。如果您正在测试字符串类型的属性，那么列表中的所有元素都将是字符串。</p><p>在这个例子中，我们只想检索出生在1965、1970或1975年的人的<code>Person</code>节点:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)</span><br><span class="line">WHERE p.born IN [1965, 1970, 1975]</span><br><span class="line">RETURN p.name, p.born</span><br></pre></td></tr></table></figure><p>同样的，我们也可以添加一些条件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[r:ACTED_IN]-&gt;(m:Movie)</span><br><span class="line">WHERE  &#x27;Neo&#x27; IN r.roles AND m.title=&#x27;The Matrix&#x27;</span><br><span class="line">RETURN p.name, r.roles</span><br></pre></td></tr></table></figure><hr><h3 id="节点或关系具有哪些属性："><a href="#节点或关系具有哪些属性：" class="headerlink" title="节点或关系具有哪些属性："></a>节点或关系具有哪些属性：</h3><p>使用<code>key()进行查询</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)</span><br><span class="line">RETURN p.name, keys(p)</span><br></pre></td></tr></table></figure><img src="https://cdnjson.com/images/2023/04/02/image0b54197fa6e66fca.png" alt="avatar" style="zoom:67%;" /><hr><h3 id="图中存在什么性质："><a href="#图中存在什么性质：" class="headerlink" title="图中存在什么性质："></a>图中存在什么性质：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CALL db.propertyKeys()</span><br></pre></td></tr></table></figure><img src="https://cdnjson.com/images/2023/04/02/image3bcd488bbbe5a876.png" alt="avatar" style="zoom:67%;" /><blockquote><p>同样地，你可以在网站中的课后习题中进行练习，那都是一些比较简单和基础但是不可谓不重要的题目。</p></blockquote><hr><h1 id="向Neo4j写入数据："><a href="#向Neo4j写入数据：" class="headerlink" title="向Neo4j写入数据："></a>向Neo4j写入数据：</h1><p>因为这主要是语法的部分，并没有什么特别难理解的东西需要进一步的解释，所以，这一部分更推荐动手实践，</p><p>还是挑重点来说，主要内容包括：</p><blockquote><p>使用<code>MERGE</code>在图中创建节点；</p><p>使用<code>MERGE</code>在图中创建关系；</p><p>创建、更新和删除图中节点和关系的属性；</p><p>根据图中的内容执行有条件的<code>MERGE</code>处理；</p><p>从图中删除节点和关系。</p></blockquote><hr><h2 id="创建节点："><a href="#创建节点：" class="headerlink" title="创建节点："></a>创建节点：</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MERGE (p:Person &#123;name: &#x27;Michael Caine&#x27;&#125;)</span><br></pre></td></tr></table></figure><p>另外，可以使用<code>CREATE</code>而不是<code>MERGE</code>创建节点。</p><p>使用<code>CREATE</code>的好处是，它在添加节点之前<strong>不会查找主键</strong>。</p><p>如果您确定数据是干净的，并且希望在导入过程中<strong>提高速度</strong>，则可以使用CREATE。</p><hr><h2 id="创建关系："><a href="#创建关系：" class="headerlink" title="创建关系："></a>创建关系：</h2><p>如果Person和Movie节点都已经存在，我们可以在创建它们之间的关系之前使用<code>MATCH</code>子句找到它们。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person &#123;name: &#x27;Michael Caine&#x27;&#125;)</span><br><span class="line">MATCH (m:Movie &#123;title: &#x27;The Dark Knight&#x27;&#125;)</span><br><span class="line">MERGE (p)-[:ACTED_IN]-&gt;(m)</span><br></pre></td></tr></table></figure><p><code>MERGE</code>的作用是创建图中<strong>不存在的</strong>节点或关系。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MERGE (p:Person &#123;name: &#x27;Emily Blunt&#x27;&#125;)-[:ACTED_IN]-&gt;(m:Movie &#123;title: &#x27;A Quiet Place&#x27;&#125;)</span><br><span class="line">RETURN p, m</span><br></pre></td></tr></table></figure><img src="https://cdnjson.com/images/2023/04/03/image.png" alt="avatar" style="zoom:67%;" /><hr><h2 id="设置属性："><a href="#设置属性：" class="headerlink" title="设置属性："></a>设置属性：</h2><p>有两种办法可以进行属性的设置：</p><ol><li><p>作为<code>MERGE</code>子句的一部分；</p></li><li><p>使用<code>SET</code>关键字来引用节点或关系。</p></li></ol><p>具体来说，我们给出两个样例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MERGE (p:Person &#123;name: &#x27;Michael Caine&#x27;&#125;)</span><br><span class="line">MERGE (m:Movie &#123;title: &#x27;Batman Begins&#x27;&#125;)</span><br><span class="line">MERGE (p)-[:ACTED_IN &#123;roles: [&#x27;Alfred Penny&#x27;]&#125;]-&gt;(m)</span><br><span class="line">RETURN p,m</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[r:ACTED_IN]-&gt;(m:Movie)</span><br><span class="line">WHERE p.name = &#x27;Michael Caine&#x27; AND m.title = &#x27;The Dark Knight&#x27;</span><br><span class="line">SET r.roles = [&#x27;Alfred Penny&#x27;]</span><br><span class="line">RETURN p, r, m</span><br></pre></td></tr></table></figure><p>注意：<code>SET</code>中的<code>[]</code>有时不是必要的。</p><p>如果需要设置多个属性，请使用逗号分隔。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET r.roles = [&#x27;Alfred Penny&#x27;], r.year = 2008</span><br></pre></td></tr></table></figure><hr><h2 id="更新属性："><a href="#更新属性：" class="headerlink" title="更新属性："></a>更新属性：</h2><p>除此之外，<code>SET</code>还可以用于<strong>更新属性</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[r:ACTED_IN]-&gt;(m:Movie)</span><br><span class="line">WHERE p.name = &#x27;Michael Caine&#x27; AND m.title = &#x27;The Dark Knight&#x27;</span><br><span class="line">SET r.roles = [&#x27;Mr. Alfred Penny&#x27;]</span><br><span class="line">RETURN p, r, m</span><br></pre></td></tr></table></figure><blockquote><p>像这样直接覆盖就可以了</p></blockquote><hr><h2 id="删除属性："><a href="#删除属性：" class="headerlink" title="删除属性："></a>删除属性：</h2><p>至于删除属性，我们可以用<code>REMOVE</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[r:ACTED_IN]-&gt;(m:Movie)</span><br><span class="line">WHERE p.name = &#x27;Michael Caine&#x27; AND m.title = &#x27;The Dark Knight&#x27;</span><br><span class="line">REMOVE r.roles</span><br><span class="line">RETURN p, r, m</span><br></pre></td></tr></table></figure><p>还可以用<code>SET xxx = null</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)</span><br><span class="line">WHERE p.name = &#x27;Gene Hackman&#x27;</span><br><span class="line">SET p.born = null</span><br><span class="line">RETURN p</span><br></pre></td></tr></table></figure><hr><h2 id="神奇的自定义MERGE："><a href="#神奇的自定义MERGE：" class="headerlink" title="神奇的自定义MERGE："></a>神奇的自定义MERGE：</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// 查找或创建一个具有此名称的人</span><br><span class="line">MERGE (p:Person &#123;name: &#x27;McKenna Grace&#x27;&#125;)</span><br><span class="line"></span><br><span class="line">// 如果节点是在此查询期间创建的，则只设置&#x27;createdAt&#x27;属性</span><br><span class="line">ON CREATE SET p.createdAt = datetime()</span><br><span class="line"></span><br><span class="line">// 如果之前创建了节点，则只设置&#x27;updatedAt&#x27;属性</span><br><span class="line">ON MATCH SET p.updatedAt = datetime()</span><br><span class="line"></span><br><span class="line">// 设置&#x27;born&#x27;属性</span><br><span class="line">SET p.born = 2006</span><br><span class="line"></span><br><span class="line">RETURN p</span><br></pre></td></tr></table></figure><p>类似地，可以用逗号分割多个属性。</p><hr><h2 id="MERGE处理："><a href="#MERGE处理：" class="headerlink" title="MERGE处理："></a>MERGE处理：</h2><p>使用<code>MERGE</code>来创建节点或关系：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 查找或创建一个具有此名称的人</span><br><span class="line">MERGE (p:Person &#123;name: &#x27;Michael Caine&#x27;&#125;)</span><br><span class="line"></span><br><span class="line">// 用这个标题找到或创建一个电影</span><br><span class="line">MERGE (m:Movie &#123;title: &#x27;The Cider House Rules&#x27;&#125;)</span><br><span class="line"></span><br><span class="line">// 查找或创建两个节点之间的关系</span><br><span class="line">MERGE (p)-[:ACTED_IN]-&gt;(m)</span><br></pre></td></tr></table></figure><p>下面是在查询处理器中发生的事情:</p><blockquote><p>Neo4j将尝试查找名称为Michael Caine的Person节点。</p><p>如果不存在，则创建节点。</p><p>然后，它将尝试在图中展开该节点的ACTED_IN关系。</p><p>如果这个节点中有任何ACTED_IN关系，它将查找标题为“the Cider House Rules”的电影。</p><p>如果Movie没有节点，则创建该节点。</p><p>如果两个节点之间没有关系，则在它们之间创建ACTED_IN关系。</p></blockquote><hr><h2 id="删除数据："><a href="#删除数据：" class="headerlink" title="删除数据："></a>删除数据：</h2><p>前文提到的”删除属性”是删除数据的一个子集。</p><p>还可以包括：节点、关系、属性、标签。</p><p>要删除数据库中的任何数据，必须<strong>先检索</strong>它，<strong>再删除</strong>它。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)</span><br><span class="line">WHERE p.name = &#x27;Jane Doe&#x27;</span><br><span class="line">DETACH DELETE p</span><br></pre></td></tr></table></figure><p>这里仅给出一个例子，其他的类似。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Abstract：本文的主要内容是&lt;strong&gt;图数据库的入门&lt;/strong&gt;以及&lt;strong&gt;Cy</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>初识 - 文献综述丨学习记录</title>
    <link href="https://conqueror712.github.io/post/Literature-Review.html"/>
    <id>https://conqueror712.github.io/post/Literature-Review.html</id>
    <published>2023-03-29T13:25:35.000Z</published>
    <updated>2023-03-30T01:51:38.135Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>鉴于笔者差不多也要开始写文献综述了，而作为一个这方面的萌新，对此可以说是一窍不通。</p><p>下面就记录我自己的经验，关于学习如何写一个文献综述，以及如何写好一个文献综述。</p><p>话说这篇文章事实上就是对于网上的内容进行的一个”小综述”，大概算是吧（笑）。</p><p>再注：本文会随着笔者的不断学习而更新和改进。</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><hr><p>文献综述还有一些别名：研究综述、国内外研究现状</p><p>定义：在全面掌握分析某一学术问题或者研究领域相关文献的基础上，对该学术问题或研究领域在一定时期内已有的研究成果、存在的问题进行<strong>分析、归纳、整理和评述</strong>而形成的一种不同于研究论文的文体。</p><p>文献综述反映当前某一领域中某分支学科或重要专题的历史现状、最新进展、学术见解和建议，它往往能反映出有关问题的新动态、新趋势、新水平、新原理和新技术等等。作者一般不在其中发表个人见解和建议，也不做任何评论，只是客观概括地反映事实。</p><hr><h1 id="文献综述有什么用："><a href="#文献综述有什么用：" class="headerlink" title="文献综述有什么用："></a>文献综述有什么用：</h1><ul><li>是科研新手非常好的入门方式</li><li>可以整理自己的<strong>思路</strong></li><li>避免与已有的研究<strong>重复</strong></li><li>明确现有研究和自己将要做的研究的<strong>必要性与价值</strong></li><li>体现研究者对于相关研究领域的<strong>熟悉程度</strong></li></ul><hr><h1 id="如何入门文献综述："><a href="#如何入门文献综述：" class="headerlink" title="如何入门文献综述："></a>如何入门文献综述：</h1><h2 id="选题："><a href="#选题：" class="headerlink" title="选题："></a>选题：</h2><blockquote><p>良好的开始是成功的一半</p></blockquote><ul><li>新颖性</li><li>可行性</li><li>选题的意义和价值</li></ul><p>具体来说：</p><p><strong>新颖性</strong>主要看是不是能在这个领域贡献新的知识，判断方法与思路：</p><ol><li>是否呈现新事实 or 发现新现象</li><li>是否有新的阐释</li><li>是否提出新问题</li><li>是否拓展新视角</li></ol><p><strong>可行性</strong>顾名思义，既要有一定的相关文献作为基础，又不能选择已经比较完善的领域。</p><p><strong>选题的意义和价值</strong>，这大致可以分为两个方面，理论意义和实践意义。理论意义就是指这个选题是否可以丰富和完善已有的理论研究内容，促进该领域的进一步发展；实践意义就是说是否对现实生活有用，考虑经济效益和社会效益。</p><hr><h2 id="检索："><a href="#检索：" class="headerlink" title="检索："></a>检索：</h2><p>盲目的检索只会徒增自己的时间成本，甚至会影响到之后的研究。</p><p>需要真正明确自己的研究方向，再进行检索。</p><hr><h2 id="看："><a href="#看：" class="headerlink" title="看："></a>看：</h2><p>高效阅读？带着以下的问题：</p><ol><li><p>这篇文章主要解决哪些问题？</p></li><li><p>针对这些问题，作者采用了哪些方法？</p></li><li><p>这些方法各具有哪些优点和不足？</p></li><li><p>目前该领域的最新研究进展如何？</p></li></ol><p>其次，有一些需要注意的点：</p><ul><li>看前言、摘要和绪论，根据关键词，找到适合的书来看。注意不要看的太仔细，不要拘泥于一两个知识点</li><li>找关键词：从师兄师姐或者课题组的其他同学的论文的关键词</li><li>尽可能多的看，看二三十篇开始动手整理</li></ul><hr><h2 id="写："><a href="#写：" class="headerlink" title="写："></a>写：</h2><ul><li>将绪论部分求同存异，然后按自己的逻辑编排，便于了解课题和打基础</li><li>找到一个方向从而缩小关注的范围，便于深入，这一步可以问一问老师</li></ul><hr><h2 id="再看："><a href="#再看：" class="headerlink" title="再看："></a>再看：</h2><ul><li>关注方法，关键性能指标，原理等</li><li>分类，比较</li></ul><hr><h2 id="深入细节："><a href="#深入细节：" class="headerlink" title="深入细节："></a>深入细节：</h2><ul><li>继续看文献，但是要统计和比较</li><li>对别人的图表要有自己的分析，当成未知结论的图，重新分析得出自己的结论，提出新的问题和方法</li><li>熟练应用数学特别是统计</li><li>耐心，文献不是一天看完的，一般这一周期都会持续三个月以上的时间</li></ul><hr><h2 id="关于参考文献："><a href="#关于参考文献：" class="headerlink" title="关于参考文献："></a>关于参考文献：</h2><p>参考文献是很重要的，一定要<strong>认真对待</strong>。</p><p>一个合格的参考文献需要有：</p><ul><li>全面性</li><li>新进性</li><li>权威性、代表性</li><li>相关性</li><li>实事求是性</li></ul><hr><h1 id="文献综述的常见问题："><a href="#文献综述的常见问题：" class="headerlink" title="文献综述的常见问题："></a>文献综述的常见问题：</h1><ul><li>内容不完整</li><li>逻辑混乱</li><li>避重就轻</li><li>只述不评</li><li>个人观点过多</li></ul><hr><h1 id="什么是一个优秀的文献综述："><a href="#什么是一个优秀的文献综述：" class="headerlink" title="什么是一个优秀的文献综述："></a>什么是一个优秀的文献综述：</h1><blockquote><ol><li><p>在将要研究的主题之下，我们<strong>已经知道了什么</strong>？</p></li><li><p>研究的主要概念或主要变量具有怎样的<strong>特征</strong>？</p></li><li><p>这些主要的概念或变量之间具有怎样的<strong>关系</strong>？</p></li><li><p>有关这个研究主题，<strong>已有的理论</strong>有哪些？</p></li><li><p>在已有的研究中，存在怎样的<strong>缺陷或不足</strong>？</p></li><li><p>还有哪些观点<strong>有待检验</strong>？</p></li><li><p>哪些证据是<strong>缺乏的、不全面的、互相矛盾或非常局限的</strong>？</p></li><li><p><strong>为什么</strong>要研究目前所确定的这个问题？</p></li><li><p>你希望当前的研究对这个主题有怎样的<strong>贡献</strong>（你的<strong>研究价值</strong>是什么）？</p></li><li><p>已有的研究设计或研究方法，存在哪些有<strong>待改进之处</strong></p></li></ol></blockquote><hr><h1 id="文献综述结构："><a href="#文献综述结构：" class="headerlink" title="文献综述结构："></a>文献综述结构：</h1><p>研究背景&#x2F;研究目的与意义，研究现状，评述，参考文献；</p><p>（大致如上，但是不仅限于此）</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h1&gt;&lt;p&gt;鉴于笔者差不多也要开始写文献综述了，而作为一个这方面的萌新，对此可以说是一窍不通。&lt;/p&gt;
&lt;p&gt;下面就记录我自己的经验，关于学习如</summary>
      
    
    
    
    <category term="Experience" scheme="https://conqueror712.github.io/categories/Experience/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习 - Ep3 - DL计算丨学习记录</title>
    <link href="https://conqueror712.github.io/post/DL-Calc.html"/>
    <id>https://conqueror712.github.io/post/DL-Calc.html</id>
    <published>2023-03-27T14:52:05.000Z</published>
    <updated>2023-04-05T14:52:29.029Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>本篇内容记录笔者学习深度学习的学习过程，如果你有任何想询问的问题，欢迎在以下任何平台提问！</p><blockquote><p>在本章中，我们将深入探索深度学习计算的<strong>关键组件， 即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘， 以及利用GPU实现显著的加速</strong>。</p><p>这些知识将使读者从深度学习”基础用户”变为”高级用户”。 </p><p>虽然本章不介绍任何新的模型或数据集， 但后面的高级模型章节在很大程度上<strong>依赖于本章的知识</strong>。</p></blockquote><p>参考书：《动手学深度学习》</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><p>注：本文将会随着笔者的学习过程随时补充。</p><hr><h1 id="层和块："><a href="#层和块：" class="headerlink" title="层和块："></a>层和块：</h1><img src="https://cdnjson.com/images/2023/04/05/imagea9c660f81077f9a8.png" alt="avatar" style="zoom: 80%;" /><p>从这里也可以看出，层和块的关系：<strong>块里面包含单个或多个层</strong>。</p><blockquote><p>我们简要说明一下每个块必须提供的基本功能。</p></blockquote><ol><li>将<strong>输入数据</strong>作为其<strong>前向传播函数的参数</strong>。</li><li>通过<strong>前向传播函数</strong>来<strong>生成输出</strong>。请注意，<strong>输出的形状可能与输入的形状不同</strong>。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。</li><li>计算其输出关于输入的<strong>梯度</strong>，可通过其反向传播函数进行访问。</li><li><strong>存储和访问</strong>前向传播计算所需的<strong>参数</strong>。</li><li>根据需要<strong>初始化模型参数</strong>。</li></ol><p><img src="https://cdnjson.com/images/2023/04/05/imagef81de356cba3e122.png" alt="avatar"></p><p>如图：多个层被组合成块，形成更大的模型。</p><blockquote><p>简要总结。</p></blockquote><ul><li>一个块可以由许多层组成；一个块可以由许多块组成。</li><li>块可以包含代码。</li><li>块负责大量的内部处理，包括参数初始化和反向传播。</li><li>层和块的顺序连接由<code>Sequential</code>块处理。</li></ul><hr><h1 id="参数管理："><a href="#参数管理：" class="headerlink" title="参数管理："></a>参数管理：</h1><blockquote><p>在选择了架构并设置了超参数后，我们就进入了<strong>训练阶段</strong>。 </p><p>此时，我们的目标是找到<strong>使损失函数最小化的模型参数值</strong>。 </p><p>经过训练后，我们将需要使用这些参数来做出未来的预测。 </p><p>此外，有时我们希望提取参数，以便在其他环境中复用它们；</p><p>将模型保存下来，以便它可以在其他软件中执行， 或者为了获得科学的理解而进行检查。</p><p><del>这是成为调参工程师的第一步！</del></p></blockquote><p>首先，我们来明确一点，超参数和模型参数有什么区别？如何区分它们？</p><p>（已经了解的读者可以跳过）</p><h2 id="两种不同的参数："><a href="#两种不同的参数：" class="headerlink" title="两种不同的参数："></a>两种不同的参数：</h2><p>（该部分引用自<a href="https://zhuanlan.zhihu.com/p/37476536%EF%BC%8C%E5%B9%B6%E5%A2%9E%E5%8A%A0%E4%BA%86%E5%85%B6%E5%8F%AF%E8%AF%BB%E6%80%A7%EF%BC%89">https://zhuanlan.zhihu.com/p/37476536，并增加了其可读性）</a></p><blockquote><p><strong>模型参数</strong>是模型内部的配置变量，其值可以根据数据进行估计。</p></blockquote><ul><li>模型在进行<strong>预测</strong>时需要它们。</li><li>它们的<strong>值</strong>，定义了可使用的模型。</li><li>他们是<strong>从数据估计</strong>或获悉的。</li><li>它们通常<strong>不</strong>由编程者<strong>手动设置</strong>。</li><li>他们通常<strong>被保存</strong>为<strong>学习模型的一部分</strong>。</li></ul><blockquote><p><strong>模型超参数</strong>是模型外部的配置，其值无法从数据中估计。</p></blockquote><ul><li>它们通常用于<strong>帮助估计模型参数</strong>。</li><li>它们通常由<strong>人工指定</strong>。</li><li>他们通常可以使用<strong>启发式设置</strong>。</li><li>他们经常被调整为给定的<strong>预测建模</strong>问题。</li></ul><p>我们虽然无法知道给定问题的模型超参数的最佳值，但是我们可以使用<strong>经验法则</strong>，在其他问题上使用复制值，或通过反复试验来搜索最佳值。</p><blockquote><p>如何区分？</p></blockquote><p>如果必须手动指定模型参数，那么它<strong>可能</strong>是一个模型超参数。</p><p>接下来，我们进入正题。</p><hr><h2 id="访问参数："><a href="#访问参数：" class="headerlink" title="访问参数："></a>访问参数：</h2><p>我们可以通过一些方式来<strong>访问参数</strong>（也就是查看）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># OrderedDict([(&#x27;weight&#x27;, tensor([[ 0.3016, -0.1901, -0.1991, -0.1220,  0.1121, -0.1424, -0.3060,  0.3400]])), (&#x27;bias&#x27;, tensor([-0.0291]))])</span></span><br></pre></td></tr></table></figure><p>注意，<code>state_dict()</code>并不是一定要这么写，具体要看情况。</p><p>不过我们也可以采用一种更加简便的方式<strong>访问所有的参数</strong>，而不用每一层都问一次：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># (&#x27;weight&#x27;, torch.Size([8, 4])) (&#x27;bias&#x27;, torch.Size([8]))</span></span><br><span class="line"><span class="comment"># (&#x27;0.weight&#x27;, torch.Size([8, 4])) (&#x27;0.bias&#x27;, torch.Size([8])) (&#x27;2.weight&#x27;, torch.Size([1, 8])) (&#x27;2.bias&#x27;, torch.Size([1]))</span></span><br></pre></td></tr></table></figure><p>当然我们也可以从<strong>嵌套块</strong>收集参数，但因为我们的终点不是讲某一个细节，而是作为一个初学者，对于每一部分的知识形成一个整体的印象和框架，所以我们并不深入地去看这到底是怎么实现的。</p><blockquote><p>可能会有较真的严谨一些的读者对此表示嗤之以鼻，的确，如果能第一次学习就将所有的细节都掌握那自然是更好的，不过笔者并不能做到这一点，相信也有一部分读者赞同这个观点，总之大家可以各取所需就是了。</p></blockquote><hr><h2 id="参数初始化："><a href="#参数初始化：" class="headerlink" title="参数初始化："></a>参数初始化：</h2><p>良好的初始化是很有必要的，有两种参数初始化的办法：</p><ul><li>深度学习框架提供的默认随机初始化；</li><li>自定义初始化方法， 满足我们通过其他规则实现初始化权重。</li></ul><p>下面直接看两种办法在Pytorch里面的代码对比：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Init&quot;</span>, *[(name, param.shape)</span><br><span class="line">                        <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[<span class="number">0</span>].weight[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure><hr><h2 id="参数绑定："><a href="#参数绑定：" class="headerlink" title="参数绑定："></a>参数绑定：</h2><blockquote><p>有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。</p></blockquote><p>特别地，当参数绑定时，梯度会发生什么情况？ </p><p>答案是由于模型参数包含梯度，因此<strong>在反向传播期间</strong>，绑定层的梯度会<strong>加在一起</strong>。</p><hr><h1 id="延后初始化与自定义层："><a href="#延后初始化与自定义层：" class="headerlink" title="延后初始化与自定义层："></a>延后初始化与自定义层：</h1><h2 id="延后初始化："><a href="#延后初始化：" class="headerlink" title="延后初始化："></a>延后初始化：</h2><p>深度学习框架无法判断网络的输入维度是什么，不过有<strong>延后初始化</strong>（defers initialization）：</p><ul><li>直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。</li></ul><blockquote><p>延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。</p><p>我们可以通过模型传递数据，使框架最终初始化参数。</p></blockquote><h2 id="自定义层："><a href="#自定义层：" class="headerlink" title="自定义层："></a>自定义层：</h2><p>深度学习成功背后的一个因素是<strong>神经网络的灵活性</strong>： </p><ul><li>我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。</li></ul><hr><blockquote><p>关于文件读写和GPU的介绍就不在本文给出了，读者可以自行查找相关内容。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h1&gt;&lt;p&gt;本篇内容记录笔者学习深度学习的学习过程，如果你有任何想询问的问题，欢迎在以下任何平台提问！&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习 - Ep2 - 多层感知机丨学习记录</title>
    <link href="https://conqueror712.github.io/post/DL-MLP.html"/>
    <id>https://conqueror712.github.io/post/DL-MLP.html</id>
    <published>2023-03-27T14:52:05.000Z</published>
    <updated>2023-04-03T08:10:24.063Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>本篇内容记录笔者学习深度学习的学习过程，如果你有任何想询问的问题，欢迎在以下任何平台提问！</p><p>参考书：《动手学深度学习》</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><p>注：本文将会随着笔者的学习过程随时补充。</p><hr><h1 id="多层感知机："><a href="#多层感知机：" class="headerlink" title="多层感知机："></a>多层感知机：</h1><p>从单层神经网络到多层神经网络。</p><h2 id="隐藏层："><a href="#隐藏层：" class="headerlink" title="隐藏层："></a>隐藏层：</h2><p><img src="https://img.picgo.net/2023/03/29/IJ2BQGEC9_LWUSKH3EZD7ae2acf350291795.png" alt="avatar"></p><hr><h2 id="激活函数："><a href="#激活函数：" class="headerlink" title="激活函数："></a>激活函数：</h2><p><img src="https://img.picgo.net/2023/03/29/image296b3f3f52baec2d.png" alt="avatar"></p><hr><h1 id="权重衰减："><a href="#权重衰减：" class="headerlink" title="权重衰减："></a>权重衰减：</h1><p>权重衰减可以帮助缓解过拟合问题。</p><p>在深度学习中，模型的过拟合问题通常是由于模型过于复杂或训练数据不足等原因造成的。</p><p>权重衰减是一种<strong>正则化技术</strong>，它通过在模型的损失函数中增加一个惩罚项来减少模型中的过度拟合。</p><ul><li><strong>正则化</strong>：是处理过拟合的常用方法：在训练集的损失函数中<strong>加入惩罚项</strong>，以<strong>降低学习到的模型的复杂度</strong>。</li><li>保持模型简单的一个特别的选择是使用$L_2$惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。</li><li>权重衰减功能在深度学习框架的优化器中提供。</li><li>在同一训练代码实现中，不同的参数集可以有不同的更新行为。</li></ul><p><img src="https://cdnjson.com/images/2023/03/29/image.png" alt="avatar"></p><p>权重衰减通过在损失函数中<strong>添加L1或L2范数惩罚项</strong>来实现。</p><ul><li>L1惩罚项将模型中所有参数的绝对值之和作为惩罚项</li><li>L2惩罚项将模型中所有参数的平方和作为惩罚项。</li></ul><p>这些惩罚项的引入使得模型在训练过程中更加倾向于使用较小的参数值，从而减少了过拟合的风险。</p><hr><h1 id="Dropout-暂退法："><a href="#Dropout-暂退法：" class="headerlink" title="Dropout - 暂退法："></a>Dropout - 暂退法：</h1><p>我们希望模型能够<strong>深度挖掘特征</strong>，即<strong>将其权重分散到多个特征中</strong>，而不是<strong>过于依赖少数潜在的虚假关联</strong>。</p><h2 id="重新审视过拟合："><a href="#重新审视过拟合：" class="headerlink" title="重新审视过拟合："></a>重新审视过拟合：</h2><p><img src="https://cdnjson.com/images/2023/03/29/imagee9b5c48d72dac33f.png" alt="avatar"></p><h2 id="扰动的稳健性："><a href="#扰动的稳健性：" class="headerlink" title="扰动的稳健性："></a>扰动的稳健性：</h2><p><img src="https://cdnjson.com/images/2023/03/29/image78c95bd6be4ede4a.png" alt="avatar"></p><p>所以说，Dropout可以改进深层网络的泛化性。</p><p>Dropout是一种在神经网络中进行正则化的方法，</p><p>它通过在训练期间随机屏蔽一部分神经元来减少模型的复杂度，从而避免过拟合的问题。</p><blockquote><p>在每次训练迭代中，</p><p>dropout会随机选择一些神经元，</p><p>将其输出设置为0，</p><p>从而减少这些神经元的贡献，</p><p>同时也促使神经元之间的相互作用变得更加分散和平均。</p></blockquote><hr><h1 id="前向-x2F-反向传播和计算图："><a href="#前向-x2F-反向传播和计算图：" class="headerlink" title="前向 &#x2F; 反向传播和计算图："></a>前向 &#x2F; 反向传播和计算图：</h1><p>前向传播：<strong>按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</strong></p><p>反向传播：计算神经网络参数梯度的方法。 该方法根据微积分中的<strong>链式规则</strong>，按相反的顺序从输出层到输入层遍历网络。</p><p><img src="https://cdnjson.com/images/2023/03/29/imagea4b38c7f912cc1d2.png" alt="avatar"> </p><hr><p>计算图有助于我们可视化计算中操作符和变量的依赖关系。</p><h2 id="前向传播计算图："><a href="#前向传播计算图：" class="headerlink" title="前向传播计算图："></a>前向传播计算图：</h2><p><img src="https://zh-v2.d2l.ai/_images/forward.svg" alt="avatar"></p><ul><li>前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。</li><li>反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。</li><li>在训练深度学习模型时，前向传播和反向传播是相互依赖的。</li><li>训练比预测需要更多的内存。</li></ul><hr><h1 id="数值稳定性和模型初始化："><a href="#数值稳定性和模型初始化：" class="headerlink" title="数值稳定性和模型初始化："></a>数值稳定性和模型初始化：</h1><p>梯度爆炸指的是在深度神经网络训练过程中，梯度值变得异常巨大，导致网络中的某些权重值在更新时发生了非常大的变化。这会导致模型的参数更新过于剧烈，使得模型在训练过程中无法收敛到合适的解决方案，也可能导致数值计算溢出等问题。</p><p>相反，梯度消失是指在深度神经网络训练过程中，梯度值变得非常小，甚至接近于0，导致网络的某些权重值几乎不会被更新，从而使得模型无法学习到足够的特征和模式，导致模型欠拟合的问题。</p><p>梯度爆炸和梯度消失通常是由于神经网络结构太深，导致梯度在反向传播过程中指数级别地增加或减少，使得梯度难以传播到浅层网络，或者使得梯度不稳定而难以优化。为了缓解这些问题，可以采取一些技术，如权重初始化、使用梯度截断和层归一化等方法来稳定训练过程，并提高深度神经网络的性能和泛化能力。</p><h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h2><ul><li>梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。</li><li>需要用启发式的初始化方法来确保初始梯度既不太大也不太小。</li><li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li><li>随机初始化是保证在进行优化前打破对称性的关键。</li><li>Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。</li></ul><hr><p>附注：</p><p>关于环境和分布偏移，这里先以一言以蔽之：</p><p><strong>环境偏移指的是训练和测试数据之间的差异，而分布偏移指的是测试数据和目标分布之间的差异。</strong></p><p>有关内容纷繁复杂，后续单开一篇文章来记录。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h1&gt;&lt;p&gt;本篇内容记录笔者学习深度学习的学习过程，如果你有任何想询问的问题，欢迎在以下任何平台提问！&lt;/p&gt;
&lt;p&gt;参考书：《动手学深度学习》</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习 - Ep1 - 线性神经网络丨学习记录</title>
    <link href="https://conqueror712.github.io/post/DL-LinearNN.html"/>
    <id>https://conqueror712.github.io/post/DL-LinearNN.html</id>
    <published>2023-03-27T02:20:05.000Z</published>
    <updated>2023-03-27T10:33:36.780Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>本篇内容记录笔者学习深度学习的学习过程，如果你有任何想询问的问题，欢迎在以下任何平台提问！</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><p>注：本文将会随着笔者的学习过程随时补充。</p><hr><h1 id="线性回归："><a href="#线性回归：" class="headerlink" title="线性回归："></a>线性回归：</h1><p><img src="https://cdnjson.com/images/2023/03/27/image00739de91ba022ad.png" alt="avatar"></p><h2 id="线性模型："><a href="#线性模型：" class="headerlink" title="线性模型："></a>线性模型：</h2><p>n维输入：$\vec{x}&#x3D;[x_1,x_2,…,x_n]^T$</p><p>有：</p><ul><li>n维权重：$\vec{w}&#x3D;[w_1, w_2, …, w_n]^T$</li><li>标量偏差$b$</li></ul><p>输出是输入的加权和：$y&#x3D;&lt;\vec{w},\vec{x}&gt;+b$</p><p>如此，线性模型（有显式解）可以看作单层的神经网络（带权的层为1）</p><hr><h2 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h2><p><img src="https://cdnjson.com/images/2023/03/27/image9853c9b4dd51251a.png" alt="avatar"></p><p>平方误差损失函数：$l^{(i)}(\vec{w},b)&#x3D;\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2$</p><p>其中，$\hat{y}^{(i)}$是预测值，$y^{(i)}$是真实标签</p><hr><h2 id="随机梯度下降："><a href="#随机梯度下降：" class="headerlink" title="随机梯度下降："></a>随机梯度下降：</h2><p><img src="https://cdnjson.com/images/2023/03/27/imageaf856f6cb117078e.png" alt="avatar"></p><blockquote><p>最小化目标函数 &lt;&#x3D;&gt;  执行极大似然估计</p></blockquote><p>梯度下降中的参数更新公式：$\overrightarrow{w_t} &#x3D; \overrightarrow{w_{t-1}} - η\frac{\partial{l}}{\partial{\overrightarrow{w_{t-1}}}}$</p><p>其中：$η$是学习率（步长的超参数），$\frac{\partial{l}}{\partial{\overrightarrow{w_{t-1}}}}$是梯度</p><hr><h1 id="Softmax回归："><a href="#Softmax回归：" class="headerlink" title="Softmax回归："></a>Softmax回归：</h1><p>Softmax实际上是一个分类问题；</p><p>回归：估计一个连续值</p><ul><li>单连续值输出</li><li>自然区间R</li><li>与真实值的区别作为损失</li></ul><p>分类：预测一个离散类别</p><ul><li>通常多个输出</li><li>输出$i$是预测为第$i$类的置信度（对分类问题，只关心对于正确类的置信度是否足够大）</li></ul><h2 id="一些数据集："><a href="#一些数据集：" class="headerlink" title="一些数据集："></a>一些数据集：</h2><p>MNIST：手写数字识别（10类）</p><p>ImageNet：自然物品分类（1000类）</p><h2 id="回归→分类："><a href="#回归→分类：" class="headerlink" title="回归→分类："></a>回归→分类：</h2><p><img src="https://cdnjson.com/images/2023/03/27/image42108b108f54af64.png" alt="avatar"></p><h2 id="全连接层的开销："><a href="#全连接层的开销：" class="headerlink" title="全连接层的开销："></a>全连接层的开销：</h2><p><img src="https://zh-v2.d2l.ai/_images/softmaxreg.svg" alt="avatar"></p><p>像这样的二分图，对于$d$个输入和$q$个输出:</p><p>原本：参数开销$O(dq)$，这实在是太高了！</p><p>优化：$O(\frac{dq}{n})$，$n$为超参数，可以灵活指定，用以平衡<strong>参数节约</strong>和<strong>模型有效性</strong>。</p><h2 id="交叉熵损失："><a href="#交叉熵损失：" class="headerlink" title="交叉熵损失："></a>交叉熵损失：</h2><p>交叉熵，衡量2个概率的区别，原始公式如下：</p><p>$H(p, q) &#x3D; \Sigma_i-p_ilog(q_i)$</p><p>以此作为损失，有损失函数：</p><p>$l(y, \hat{y})&#x3D;-\Sigma_iy_ilog\hat{y_i}&#x3D;-log\hat{y_y}$</p><p>其梯度是<strong>真实概率和预测概率的区别</strong>：</p><p>$\sigma_{o_i}l(y, \hat{y})&#x3D;softmax(o)_i-y_i$（o是置信度）</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h1&gt;&lt;p&gt;本篇内容记录笔者学习深度学习的学习过程，如果你有任何想询问的问题，欢迎在以下任何平台提问！&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Neo4j - Ep1丨学习记录</title>
    <link href="https://conqueror712.github.io/post/Neo4J.html"/>
    <id>https://conqueror712.github.io/post/Neo4J.html</id>
    <published>2023-03-25T06:47:09.000Z</published>
    <updated>2023-04-02T01:52:01.932Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><blockquote><p>Abstract：本文的主要内容是<strong>图数据库的入门</strong>以及<strong>Neo4j的基础知识</strong>。</p></blockquote><p>笔者近日在学习有关知识图谱Knowledge Graph的相关内容。</p><p>Neo4j是项目所需的一款功能强大的应用，特此来学习之，并记录于本文。</p><p>笔者是AI领域的小白，作为初学者，文章中难免会有出错或者不恰当的部分，烦请读者朋友们指出（在以下任意平台）。</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><p>还有要说明的是，笔者此前的文章存在一个问题是，篇幅过长，</p><p>这不仅导致读者不愿意看，就连笔者自己再次翻阅的时候都只能利用<code>Ctrl+F</code>才能找到我要找的内容。</p><p>故拆分之，便于阅读和查看，每篇文章控制在<code>1.5k~3.5k</code>字之间。</p><p><strong>注意：本文不是教程，只是个人学习的记录和心得，可能会对你有帮助，建议配合官方文档一起学习！</strong></p><p>官方文档链接：<a href="https://graphacademy.neo4j.com/courses/neo4j-fundamentals/">https://graphacademy.neo4j.com/courses/neo4j-fundamentals/</a></p><hr><h1 id="Neo4j-AuraDB入门："><a href="#Neo4j-AuraDB入门：" class="headerlink" title="Neo4j - AuraDB入门："></a>Neo4j - AuraDB入门：</h1><blockquote><p>入门部分会有五个板块：</p><ol><li>Create a database</li><li>View online courses</li><li>Start console tour</li><li>Open Sample Guide</li><li>More resources</li></ol></blockquote><p>首先进入网址：<a href="https://console.neo4j.io/?product=aura-db#create-database/free">https://console.neo4j.io/?product=aura-db#create-database/free</a></p><p>我们就暂时用免费版来学习，点击Start Here，随后会生成一个随机的初始密码，就像这样：</p><img src="https://cdnjson.com/images/2023/03/25/image7ad4ea336942c883.png" alt="avatar" style="zoom:67%;" /><blockquote><p>随后我们会修改这个密码，这实在是太难记了！</p></blockquote><img src="https://cdnjson.com/images/2023/03/25/imagea399953d4c3854ba.png" alt="avatar" style="zoom: 67%;" /><blockquote><p>可以看到，我们已经创建成功了，它在欢迎我们呢！</p><p>到此为止，都非常顺利，不过我们马上就遇到了第一个难题：他居然推荐我们先学四个课程再来玩这个，</p></blockquote><img src="https://cdnjson.com/images/2023/03/25/image9a1551da7ded5ee0.png" alt="avatar" style="zoom: 67%;" /><blockquote><p>那好吧，作为优秀的新时代新青年，我们秉承磨刀不误砍柴工的精神，先去学一下这些课程。</p><p>官方告诉我们，这四门课程总共需要6个小时才能学完，</p><p>不要紧，俗话说的好，慢慢来才比较快，接下来我们就进入课程的学习吧。</p></blockquote><p>（已经学完了或者认为自己不需要学的读者可以直接跳到下一部分）</p><hr><h1 id="Neo4j-基础知识："><a href="#Neo4j-基础知识：" class="headerlink" title="Neo4j - 基础知识："></a>Neo4j - 基础知识：</h1><p>这些课程都有官方的中文版教程，有能力的读者可以自行前往阅读，</p><p>这里笔者给出自己的学习过程，以及一些在学习过程中的见解。</p><p>注意：与官方文档中重复的部分就不予记录了，查阅即可！</p><p>（文档链接还是最上面的链接，大家Start Here之后就有了）</p><h2 id="以图的方式去思考"><a href="#以图的方式去思考" class="headerlink" title="以图的方式去思考"></a>以图的方式去思考</h2><h3 id="关于概念："><a href="#关于概念：" class="headerlink" title="关于概念："></a>关于概念：</h3><p>七桥问题→图论的诞生：节点 + 边</p><p>现在看来，图是一种非常有用的工具，可以帮我们<strong>建模和分析数据</strong>，广泛应用于解决复杂问题上，包括<strong>路线查找、供应链分析和实时推荐</strong>。</p><blockquote><p>学到这里，你会发现这个网站的教程居然还有课堂小测！这真是太良心了，感动哭了</p></blockquote><p>在知识图谱中，我们经常听到”对象”，”实体”这样的名词，事实上，这都是”节点”的别名，它可以代表任何你想代表的<strong>事物</strong>。</p><p>而”关系”，也就是”边”，通常是<strong>动词</strong>。</p><p>关系是图数据库中最重要的元素。</p><hr><h3 id="关于图的遍历："><a href="#关于图的遍历：" class="headerlink" title="关于图的遍历："></a>关于图的遍历：</h3><p><code>Neo4j</code>的<code>Cypher</code>语言针对节点遍历进行了优化，因此<strong>边不会被多次遍历</strong>，这对应用程序来说是一个巨大的<strong>性能提升</strong>。</p><hr><h3 id="Graphs-are-Everywhere："><a href="#Graphs-are-Everywhere：" class="headerlink" title="Graphs are Everywhere："></a>Graphs are Everywhere：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MATCH (c:Category)-[:HAS_CHILD|HAS_PRODUCT*1..3]-&gt;(p:Product)</span><br><span class="line">RETURN p.id, p.title, collect(c.name) AS categories</span><br></pre></td></tr></table></figure><p>解释：</p><ol><li>查找距离类别节点最多三个关系远的产品节点</li><li>查询匹配到的产品节点的id和title属性，同时返回一个集合categories，其中包含了这些产品所属的所有类别的名称。</li></ol><p>为什么推荐算法中常用图数据库？</p><p>因为图数据库的优势在于，需要<strong>遍历更小比例的图就能生成推荐</strong>。 </p><p>你可以简单地从一个产品节点遍历购买该产品的用户，然后再遍历他们购买的后续产品。</p><p>不仅如此，图和图数据库还可以用到方方面面，Neo4j Graphgist站点有许多示例数据模型。</p><hr><h2 id="属性图"><a href="#属性图" class="headerlink" title="属性图"></a>属性图</h2><blockquote><p>哦！我们大概终于是遇到了一个新的概念——属性图。</p></blockquote><p>我们可以使用两个额外的元素来为数据提供一些<strong>附加的上下文</strong>。</p><h3 id="什么是属性图"><a href="#什么是属性图" class="headerlink" title="什么是属性图"></a>什么是属性图</h3><h4 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h4><p>只需要一张图就可以解释清楚，这也是官方文档中给出的图：</p><blockquote><p>看！那些黑框框里面的<strong>零个、一个或多个</strong>词就是标签。</p></blockquote><img src="https://cdnjson.com/images/2023/03/25/imageb82978e7e936b587.png" alt="avatar" style="zoom: 33%;" /><h4 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h4><blockquote><p>同样给出一张图，不过这可能就得多说两句了，</p></blockquote><p>为节点添加属性，可以使其<strong>具体化</strong>。</p><p>属性是<strong>键值对</strong>的形式。</p><p>不需要为具有特定标签的每个节点都指定属性。 如果节点的属性不存在，则将其视为 <code>null</code>。</p><img src="https://cdnjson.com/images/2023/03/25/image3eafeaa374eb730c.png" alt="avatar" style="zoom: 33%;" /><h4 id="边的类型和属性以及方向"><a href="#边的类型和属性以及方向" class="headerlink" title="边的类型和属性以及方向"></a>边的类型和属性以及方向</h4><p>不仅是点，边（关系）也是有标签和属性的，只不过把”标签”换成了”类型”。</p><p>并且和点不同的是，Neo4j的<strong>每个关系必须有一个类型</strong>，</p><p>这不仅<strong>完善了图的含义</strong>，还便于我们对图进行<strong>部分遍历</strong>。</p><blockquote><p>什么是部分遍历呢？就比如说我们只关心谁在哪工作，不关心谁和谁结婚。</p></blockquote><p>当然，如果是这样的话我们可能需要进行多源遍历，这需要看情况。</p><p>对于关系的属性，这些可以指加权图中的<strong>权值</strong>，或者只是为关系<strong>提供额外的上下文</strong>。</p><img src="https://cdnjson.com/images/2023/03/25/image7b7b24b869445a4f.png" alt="avatar" style="zoom: 33%;" /><p>除此之外，在Neo4j中，图的每个关系<strong>必须</strong>有方向。</p><p>不过，尽管方向是必需的，但可以在任一方向查询关系，或在查询时完全忽略。</p><hr><h3 id="原生图的优势"><a href="#原生图的优势" class="headerlink" title="原生图的优势"></a>原生图的优势</h3><blockquote><p>接下来的内容可能有点陌生了，至少对于笔者这样的新手来说是这样的，</p><p>理解和学习可能会花费更多的时间，</p><p>不过没关系，让我们继续吧！</p></blockquote><p>Neo4j是一个原生图数据库，这意味着从数据存储到查询语言的所有内容都是<strong>专门为遍历而设计的</strong>。</p><p>Neo4j符合<strong>ACID</strong>标准——事务中的一组修改将全部提交或失败。</p><h4 id="无索引邻接IFA"><a href="#无索引邻接IFA" class="headerlink" title="无索引邻接IFA"></a>无索引邻接IFA</h4><p>提交数据库事务时，对<strong>关系的引用</strong>与<strong>关系开始和结束处的节点</strong>一起存储。 </p><p>由于每个节点都知道与其连接的每个传入和传出关系，底层图引擎将简单地<strong>查询内存中的指针</strong>——这是计算机非常擅长的。</p><p>查询过程：</p><ol><li>根据指定的锚点计划查询</li><li>使用索引来检索锚节点</li><li>按照指针检索所需的结果节点</li></ol><p>与关系数据库访问相比，IFA 的好处是：</p><ul><li>更少的<strong>索引查找</strong></li><li>没有<strong>表扫描</strong></li><li>减少<strong>数据重复</strong></li></ul><p>还是不好理解？没关系，以下这个例子会帮助到你！</p><blockquote><p>假设你要规划一次旅行，并且需要找到一个好的餐厅去享受美食。</p><p>你可能会在互联网上搜索各种餐厅的信息，并且想了解它们的菜单、位置、评价等信息。</p><p>如果这些信息存储在一个关系型数据库中，那么你可能需要<strong>进行多个查询</strong>来获取所有信息。</p><p>例如，你可能需要先查询餐厅表格，然后再查询菜单表格和评论表格，最后将这些信息整合起来以得出你需要的信息。这种查询方式需要多次查询和连接多个表格，因此查询速度可能会比较慢。</p><p>相比之下，如果这些信息存储在一个图数据库中，那么你只需要进行一个查询，就能够得到所有相关的信息。</p><p>图数据库使用图的方式来存储数据，其中节点表示实体，边表示实体之间的关系。</p><p>例如，在图数据库中，你可以使用<strong>一个节点表示餐厅，另一个节点表示菜单，使用一条边表示餐厅和菜单之间的关系。</strong>这种查询方式只需要一次查询和一次遍历，因此查询速度会比较快！</p></blockquote><h3 id="从其他数据库到图数据库"><a href="#从其他数据库到图数据库" class="headerlink" title="从其他数据库到图数据库"></a>从其他数据库到图数据库</h3><p>随着关系数据库中记录数的增加，查询变得<strong>越来越慢</strong>。 </p><p>图数据库中的查询时间将<strong>与查询期间实际触及的数据大小保持一致</strong>。</p><p>将关系视为<strong>一等公民</strong>也可以在开始时提供优势。 </p><p>在<strong>图中建模关系</strong>比创建<strong>数据透视表</strong>来表示<strong>多对多</strong>关系更<strong>自然</strong>。</p><p>也就是说，总结起来就是一句话：</p><blockquote><p>可以将关系模型实现为图模型，这样对于理解起来会更方便！</p></blockquote><p>除此之外，<strong>键值存储</strong>和<strong>文档存储</strong>也是可以实现用图来提高性能的存储方式。</p><hr><h2 id="你的第一个图"><a href="#你的第一个图" class="headerlink" title="你的第一个图"></a>你的第一个图</h2><h3 id="电影图数据"><a href="#电影图数据" class="headerlink" title="电影图数据"></a>电影图数据</h3><p>这部分内容是一个图数据库的例子，帮助理解上述知识，</p><p>这里放一张图，以示实际应用中的图数据库是什么样子的：</p><img src="https://cdnjson.com/images/2023/03/25/image310d0d810246e431.png" alt="avatar" style="zoom: 67%;" /><p>具体的内容就不展开了，大家可以自行前往查看。</p><blockquote><p>可能有的读者会大失所望，还以为是要开始创建第一个项目了呢！</p><p>先别着急，还记得一开始官方教程中告诉我们的吗？这刚刚是第一个基础，还有三个基础等着我们呢！</p><p>先让我们庆祝一下，我们已经完成了第一部分的学习！</p></blockquote><img src="https://cdnjson.com/images/2023/03/25/imageb3709901b8436875.png" alt="avatar" style="zoom: 67%;" /><blockquote><p>甚至给我颁发了一个证书 XD</p></blockquote><img src="https://cdnjson.com/images/2023/03/25/image48542dc349b9f728.png" alt="avatar" style="zoom:50%;" /><hr><p>Thanks for reading and see you next time!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Abstract：本文的主要内容是&lt;strong&gt;图数据库的入门&lt;/strong&gt;以及&lt;strong&gt;Ne</summary>
      
    
    
    
    <category term="AI" scheme="https://conqueror712.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>计网 - Ep2 - 应用层「万字长文」丨学习记录</title>
    <link href="https://conqueror712.github.io/post/Computer-Network-2.html"/>
    <id>https://conqueror712.github.io/post/Computer-Network-2.html</id>
    <published>2023-03-20T02:58:59.000Z</published>
    <updated>2023-04-23T03:09:54.791Z</updated>
    
    <content type="html"><![CDATA[<p><em>Computer Networking A Top-Down Approach Learning Note Part 2</em></p><p><strong>前言</strong>：</p><p>本文是笔者初学计算机网络的笔记和一些心得，难免会有部分疏漏和错误，还请各位读者积极指出，不吝赐教。</p><p>有一些内容是笔者认为对自己暂时没那么重要的部分，就没有放上去，具体的内容可以查看相关的书籍。</p><p>观前提醒：本文篇幅较长，若您只是想看其中的某一小节的知识，直接点击目录进行跳转即可！</p><p>事不宜迟，我们开始吧！</p><blockquote><p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p><p>知乎：<a href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p><p>Bilibili：<a href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p><p>掘金：<a href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p></blockquote><hr><h1 id="Unit-2-应用层"><a href="#Unit-2-应用层" class="headerlink" title="Unit 2 应用层"></a>Unit 2 应用层</h1><h2 id="应用层协议原理"><a href="#应用层协议原理" class="headerlink" title="应用层协议原理"></a>应用层协议原理</h2><p>网络核心中没有应用层软件</p><p>想法 → 网络应用</p><p>研发网络应用程序的核心：写出能够运行在不同的端系统和通过网络彼此通信的程序。</p><h3 id="网络-x2F-应用程序体系结构"><a href="#网络-x2F-应用程序体系结构" class="headerlink" title="网络&#x2F;应用程序体系结构"></a>网络&#x2F;应用程序体系结构</h3><p>Network architecture and applicaiton architecture are both the “design and structure”.</p><h4 id="常见的网络体系结构："><a href="#常见的网络体系结构：" class="headerlink" title="常见的网络体系结构："></a>常见的网络体系结构：</h4><ul><li>客户 - 服务器模式 <code>C - S</code><ul><li>服务器：<ul><li>一直运行</li><li><strong>固定的IP地址</strong>和周知的端口号，不像DHCP获取的动态IP</li><li>扩展性：服务器农场</li></ul></li><li>客户端：<ul><li>主动与服务器通信</li><li>与互联网有间歇性的连接</li><li>可能是<strong>动态IP地址</strong></li><li>不直接与其他客户端通信</li></ul></li></ul></li><li>对等模式 <code>P2P</code><ul><li>几乎没有一直运行的服务器<ul><li>任意端系统之间可以进行通信</li></ul></li><li>每一个节点既是客户端又是服务器<ul><li><strong>自扩展性</strong>：新的peer节点带来新的服务能力和新的服务请求</li></ul></li><li>参与的主机间歇性连接且可以改变IP地址</li><li>例如：迅雷</li></ul></li><li>混合体<ul><li>Napster<ul><li>文件搜索：集中</li><li>文件传输：P2P</li></ul></li><li>即时通信<ul><li>在线检测：集中</li><li>两个用户之间聊天：P2P</li></ul></li></ul></li></ul><h4 id="常见的应用程序体系结构："><a href="#常见的应用程序体系结构：" class="headerlink" title="常见的应用程序体系结构："></a>常见的应用程序体系结构：</h4><ul><li><p>分层体系结构（Layered Architecture）：</p><p>  应用程序被划分为多个层次，每个层次负责不同的功能。例如，MVC（Model-View-Controller）框架就是一种分层体系结构。</p></li><li><p>事件驱动体系结构（Event-Driven Architecture）：</p><p>  应用程序通过事件的方式进行通信和交互。事件可以是用户的操作、消息的到达、计时器的触发等等。例如，GUI（Graphical User Interface）应用程序就是一种事件驱动体系结构。</p></li><li><p>微服务体系结构（Microservices Architecture）：</p><p>  应用程序被划分为多个小型的、相互独立的服务。每个服务可以独立地开发、部署和扩展，从而提高应用程序的灵活性和可维护性。</p></li><li><p>基于消息的体系结构（Message-Based Architecture）：</p><p>  应用程序通过消息进行通信和交互。消息可以是文本、XML、JSON等格式的数据。例如，企业消息总线（Enterprise Service Bus）就是一种基于消息的体系结构。</p></li><li><p>一体式体系结构（All-in-one architecture），etc.</p></li></ul><h3 id="进程通信"><a href="#进程通信" class="headerlink" title="进程通信"></a>进程通信</h3><p>如何通信？</p><p>进程(process)与进程在相同的端系统上以进程间的通信机制互相通信，这个规则由操作系统决定。</p><p>而我们的重点是：两个<strong>不同的端系统</strong>之间是如何进行通信的——**交换报文(message)**。</p><ul><li>使用OS提供的通信服务</li><li>按照应用协议交换报文</li></ul><p>关于进程：</p><ul><li>客户端进程：发起通信的进程</li><li>服务端进程：等待连接的进程</li></ul><p>（P2P架构的应用也有客户端进程和服务端进程之分）</p><p>除此之外，分布式进程通信需要解决的问题如下：</p><ol><li>对进程进行编址（addressing）</li><li>传输层提供的服务：需要穿过层间的信息 + 层间信息的代表</li></ol><h3 id="TCP-socket"><a href="#TCP-socket" class="headerlink" title="TCP socket"></a>TCP socket</h3><p>为什么要有TCP socket?</p><p>不同的端口号对应了不同的协议 不同的进程，便于进行进程的寻址。</p><p>16bit的端口号 65536个状态</p><p>如果Socket API 每次传输报文，都携带如此多的信息，太繁琐易错，不便于管理</p><p>用个<strong>代号</strong>标示通信的双方或者单方：socket</p><p>就像OS打开文件返回的句柄一样</p><p>TCP socket:</p><ul><li>TCP服务，两个进程之间的通信需要之前要建立连接<ul><li>两个进程通信会持续一段时间，通信关系稳定</li></ul></li><li>可以用一个整数表示两个应用实体之间的通信关系 ，本地标示，类似Hash</li><li>穿过层间接口的信息量最小</li></ul><p>对于使用面向连接服务（TCP）的应用而言，套接字是4元组的一个具有本地意义的标示。</p><ul><li>四元组：源IP，源port，目标IP，目标port</li><li>唯一的指定了一个会话（2个进程之间的会话关系）</li><li>应用使用这个 ，与远程的应用进程通信</li><li>不必在每一个报文的发送都要指定这4元组</li><li>就像使用操作系统打开一个文件，OS返回一个文件句柄一样，以后使用这个文件句柄，而不是使用这个文件的目录名、文件名</li><li>简单，便于管理</li></ul><img src="https://pic1.zhimg.com/v2-8302577e2963b8a605e72234f25d6082_1440w.jpg?source=172ae18b" alt="avatar" style="zoom:80%;" /><h3 id="UDP-socket"><a href="#UDP-socket" class="headerlink" title="UDP socket"></a>UDP socket</h3><ul><li>UDP服务，两个进程之间的通信需要之前无需建立连接<ul><li>每个报文都是独立传输的</li><li>前后报文可能给不同的分布式进程</li></ul></li><li>因此，只能用一个整数表示本应用实体的标示<ul><li>因为这个报文可能传给另外一个分布式进程</li></ul></li><li>穿过层间接口的信息大小最小</li><li>UDP socket：本IP,本端口</li><li>但是传输报文时：必须要提供对方IP，port<ul><li>接收报文时： 传输层需要上传对方的IP，port</li></ul></li></ul><p>对于使用无连接服务（UDP）的应用而言，套接字是2元组的一个具有本地意义的标示</p><ul><li>2元组：IP，port（源端指定）</li><li>UDP套接字指定了应用所在的一个端节点（end point）</li><li>在发送数据报时，采用创建好的本地套接字（标示ID），就不必在发送每个报文中指明自己所采用的ip和port</li><li>但是在发送报文时，必须要指定对方的ip和udpport(另外一个段节点)</li></ul><p>进程向套接字发送报文或从套接字接收报文</p><h3 id="应用层协议"><a href="#应用层协议" class="headerlink" title="应用层协议"></a>应用层协议</h3><p>如何使用传输层提供的服务实现应用?</p><ul><li>定义应用层协议：报文格式，解释，时序等</li><li>编制程序，通过API调用网络基础设施提供通信服务传报文，解析报文，实现应用时序等</li></ul><p>应用层协议是什么？</p><p><strong>定义了</strong>：运行在不同端系统上 的应用<strong>如何相互交换报文</strong></p><p>应用协议仅仅是应用的一个组成部分</p><p>除此之外，还分成公开协议和专有协议</p><p>如何描述传输层的服务？</p><ul><li>数据丢失率</li><li>延时</li><li>吞吐</li><li>安全性</li></ul><h3 id="Internet-传输层提供的服务"><a href="#Internet-传输层提供的服务" class="headerlink" title="Internet 传输层提供的服务"></a>Internet 传输层提供的服务</h3><h4 id="TCP服务"><a href="#TCP服务" class="headerlink" title="TCP服务"></a>TCP服务</h4><ul><li>可靠的传输服务</li><li>流量控制：发送方不会淹没接收方</li><li>拥塞控制：当网络出现拥塞时，能抑制发送方</li><li>不能提供的服务：时间保证、最小吞吐量保证，安全</li><li>面向连接：要求在客户端进程和服务器进程之间建立连接</li></ul><h4 id="UDP服务"><a href="#UDP服务" class="headerlink" title="UDP服务"></a>UDP服务</h4><ul><li>不可靠数据传输</li><li>不提供的服务：可靠，流量控制，拥塞控制，时序，贷款保证，建立连接</li></ul><h4 id="UDP的存在有其必要性"><a href="#UDP的存在有其必要性" class="headerlink" title="UDP的存在有其必要性"></a>UDP的存在有其必要性</h4><ul><li>能够<strong>区分不同的进程</strong>，而IP服务不能<ul><li>在IP提供的主机到主机端到端功能的基础上，区分了主机的应用进程</li></ul></li><li><strong>无需建立连接</strong>，省去了建立连接时间，适合事务性应用</li><li><strong>不做可靠性的工作</strong>，例如检错重发，适合那些对实时性要求比较高而对正确性要求不高的应用</li><li>没有拥塞控制和流量控制，<strong>应用能够按照设定的速度发送数据</strong></li></ul><p><img src="https://cdnjson.com/images/2023/03/20/image.png" alt="avatar"></p><h4 id="安全TCP——SSL"><a href="#安全TCP——SSL" class="headerlink" title="安全TCP——SSL"></a>安全TCP——SSL</h4><p>原本的TCP和UDP，都是没有加密的，甚至于密码都是明文传输；</p><p>引入SSL，<strong>在TCP上面实现</strong>，提供加密的TCP连接；</p><ul><li>私密性</li><li>数据完整性</li><li>端到端的鉴别</li></ul><p><strong>SSL在应用层</strong>，应用采用SSL库，而SSL库使用传输层的TCP进行通信；</p><p>应用通过API将明文交给socket，SSL将其加密并在互联网上传输，详见Unit8。</p><blockquote><p>SSL是<strong>Secure Sockets Layer安全套接字层</strong>的缩写，是一系列<strong>加密技术</strong>，允许Web用户保护他们通过Internet传输的信息的<strong>隐私</strong>。</p><p>当您访问安全网站时，您会在 URL 旁边看到一个锁，表示您与该网站的通信<strong>已加密</strong>。</p><p><img src="https://s2.loli.net/2023/01/29/6mWG8plUqKEt2O1.png" alt="avatar"></p><p>该锁应该表明<strong>第三方将无法读取您发送或接收的任何信息</strong>。在后台，SSL通过将您的数据转换为只有收件人知道如何破译的编码消息来实现这一点。如果恶意方正在监听对话，它只会看到看似随机的字符串，而不会看到您的电子邮件、Facebook 帖子、信用卡号或其他私人信息的内容。</p></blockquote><h4 id="TCP三次握手四次挥手"><a href="#TCP三次握手四次挥手" class="headerlink" title="TCP三次握手四次挥手"></a>TCP三次握手四次挥手</h4><p>为了保证客户端和服务器端的可靠连接，TCP建立连接时<strong>必须</strong>要进行三次会话，也叫TCP三次握手，</p><p>进行三次握手的目的是为了<strong>确认双方的接收能力和发送能力是否正常</strong>。</p><img src="https://cdnjson.com/images/2023/03/21/image1ce75f8803f2d480.png" alt="avatar" style="zoom:80%;" /><blockquote><p>最开始的时候客户端和服务器都是处于CLOSED关闭状态。主动打开连接的为客户端，被动打开连接的是服务器。</p><p>TCP服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了 <strong>LISTEN 监听状态</strong></p><p>第一次握手 TCP客户进程也是先创建传输控制块TCB，然后向服务器发出连接请求报文，这是报文首部中的同部位SYN&#x3D;1，同时选择一个初始序列号 seq&#x3D;x ，此时，TCP客户端进程进入了 <strong>SYN-SENT 同步已发送状态</strong></p><p>第二次握手 TCP服务器收到请求报文后，如果同意连接，则会向客户端发出确认报文。确认报文中应该 ACK&#x3D;1，SYN&#x3D;1，确认号是ack&#x3D;x+1，同时也要为自己初始化一个序列号 seq&#x3D;y，此时，TCP服务器进程进入了 <strong>SYN-RCVD 同步收到状态</strong></p><p>第三次握手 TCP客户端收到确认后，还要向服务器给出确认。确认报文的ACK&#x3D;1，ack&#x3D;y+1，自己的序列号seq&#x3D;x+1，此时，TCP连接建立，客户端进入<strong>ESTABLISHED已建立连接状态</strong> 触发三次握手</p><p>有人可能会很疑惑为什么要进行第三次握手？<br>主要原因：<strong>防止已经失效的连接请求报文突然又传送到了服务器，从而产生错误</strong></p><p><strong>第一次握手： 客户端向服务器端发送报文</strong><br>        证明客户端的发送能力正常<br><strong>第二次握手：服务器端接收到报文并向客户端发送报文</strong><br>        证明服务器端的接收能力、发送能力正常<br><strong>第三次握手：客户端向服务器发送报文</strong><br>        证明客户端的接收能力正常</p></blockquote><img src="https://cdnjson.com/images/2023/03/21/image24fec85472fe6e50.png" alt="avatar" style="zoom:80%;" /><blockquote><p>数据传输完毕后，双方都可释放连接。最开始的时候，客户端和服务器都是处于<strong>ESTABLISHED</strong>状态，然后客户端主动关闭，服务器被动关闭。</p><p>第一次挥手 客户端发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN&#x3D;1，其序列号为seq&#x3D;u（等于前面已经传送过来的数据的最后一个字节的序号加1），此时，客户端进入<strong>FIN-WAIT-1（终止等待1）</strong>状态</p><p>第二次挥手 服务器端接收到连接释放报文后，发出确认报文，ACK&#x3D;1，ack&#x3D;u+1，并且带上自己的序列号seq&#x3D;v，此时，服务端就进入了CLOSE-WAIT 关闭等待状态</p><p>第三次挥手 客户端接收到服务器端的确认请求后，客户端就会进入<strong>FIN-WAIT-2（终止等待2）</strong>状态，等待服务器发送连接释放报文，服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。</p><p>第四次挥手 客户端收到服务器的连接释放报文后，必须发出确认，ACK&#x3D;1，ack&#x3D;w+1，而自己的序列号是seq&#x3D;u+1，此时，客户端就进入了<strong>TIME-WAIT（时间等待）</strong>状态，但此时TCP连接还未终止，必须要经过2MSL后（最长报文寿命），当客户端撤销相应的TCB后，客户端才会进入CLOSED关闭状态，服务器端接收到确认报文后，会立即进入CLOSED关闭状态，到这里TCP连接就断开了，四次挥手完成</p><p>为什么客户端要等待2MSL？<br>主要原因是为了保证客户端发送那个的第一个ACK报文能到服务器，因为这个ACK报文可能丢失，并且2MSL是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃，这样<strong>新的连接中不会出现旧连接的请求报文</strong>。</p></blockquote><hr><h2 id="Web-amp-HTTP"><a href="#Web-amp-HTTP" class="headerlink" title="Web &amp; HTTP"></a>Web &amp; HTTP</h2><h3 id="Web概述"><a href="#Web概述" class="headerlink" title="Web概述"></a>Web概述</h3><p>由一些<strong>对象</strong>组成，有哪些对象？</p><ul><li>HTML文件、JPEG图像、Java小程序、音视频文件etc.</li></ul><p>Web页含有一个<strong>基本的HTML文件</strong>，该基本HTML文件又包含若干对象的引用（链接）；</p><p>对象如何引用？</p><ul><li>通过URL对每个对象进行引用，URL包括访问协议，用户名，口令字，端口，目录文件等；</li></ul><p><img src="https://cdnjson.com/images/2023/03/20/image3119aaf482192bea.png" alt="avatar"></p><h3 id="HTTP概述"><a href="#HTTP概述" class="headerlink" title="HTTP概述"></a>HTTP概述</h3><p><strong>一句话解释</strong>：HTTP 是用于获取 HTML 文档等资源的<strong>协议</strong>——我们称之为<strong>超文本传输协议</strong>。</p><p>它是 Web 上任何数据交换的基础，在<strong>应用层</strong>上，是一种<strong>客户端-服务器协议</strong>，</p><p>这意味着请求由接收者（通常是 Web 浏览器）发起。</p><p>从获取的不同子文档（例如文本、布局描述、图像、视频、脚本等）重建完整的文档。</p><p>它使用TCP，默认端口号为80；</p><p>（HTTPS使用了SSL，默认端口号为443）</p><ul><li>具体过程为：客户发起一个与服务器的TCP连接（建立socket），服务器接受连接，随后在浏览器和Web服务器之间交换HTTP报文，最后TCP连接关闭。</li></ul><p><img src="https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview/fetching_a_page.png" alt="avatar"></p><p>客户端和服务器通过交换单个消息（而不是数据流）进行通信。 </p><ul><li>客户端（通常是 Web 浏览器）发送的消息称为<strong>请求</strong>；</li><li>服务器作为应答发送的消息称为<strong>响应</strong>。</li></ul><p><strong>协议</strong>是定义如何在计算机内部或计算机之间交换数据的规则系统。设备之间的通信要求设备就正在交换的数据的格式达成一致。</p><p>定义格式的规则集称为协议。</p><p><img src="https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview/http-layers.png" alt="avatar"></p><hr><h3 id="基于HTTP的系统组件"><a href="#基于HTTP的系统组件" class="headerlink" title="基于HTTP的系统组件"></a>基于HTTP的系统组件</h3><p>HTTP是一种<strong>客户端-服务器协议</strong>：请求由一个实体发送，即用户代理（或代表它的代理）。 大多数情况下，用户代理是一个Web浏览器，但它可以是任何东西。</p><p>每个单独的请求都发送到服务器，服务器处理它并提供称为<strong>响应</strong>的答案。</p><p>在客户端和服务器之间有许多实体，统称为<strong>代理Proxy</strong>，例如，它们执行不同的操作并充当网关or<strong>缓存Cache</strong>。</p><p>实际上，浏览器和处理请求的服务器之间有更多的计算机：有路由器、调制解调器等。 由于Web的分层设计，这些隐藏在网络和传输层中。 HTTP位于应用程序层的<strong>顶部</strong>。 虽然对于诊断网络问题很重要，但底层大多与 HTTP 的描述无关。</p><p><img src="https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview/client-server-chain.png" alt="avatar"></p><h4 id="Proxy："><a href="#Proxy：" class="headerlink" title="Proxy："></a>Proxy：</h4><p><strong>代理服务器</strong>是在互联网的不同网络中导航时使用的<strong>中间程序或计算机</strong>。它们<strong>有助于访问万维网上的内容</strong>。</p><p>代理拦截请求并提供回响应；它可以转发或不转发请求（例如在缓存的情况下），并且可以修改请求（例如，在两个网络之间的边界处更改其标头）。</p><p>代理可以位于用户的本地计算机上，也可以位于用户计算机和 Internet 上的目标服务器之间的任何位置。通常有两种主要类型的代理服务器：由于 Web 堆栈的分层结构，其中大多数在传输、网络或物理级别运行，在 HTTP 层变得透明，并可能对性能产生重大影响。</p><ul><li>处理来自和发送到互联网上任何位置的请求的<strong>转发代理</strong>。</li><li>一种<strong>反向代理</strong>，从 Internet 接收请求并将其转发到内部网络中的服务器。</li></ul><p>代理的<strong>功能</strong>：</p><ul><li>缓存（缓存可以是公共的或私有的，就像浏览器缓存一样）</li><li>过滤（如防病毒扫描或家长控制）</li><li>负载平衡（允许多个服务器为不同的请求提供服务）</li><li>身份验证（控制对不同资源的访问）</li><li>日志记录（允许存储历史信息）</li></ul><h4 id="Cache："><a href="#Cache：" class="headerlink" title="Cache："></a>Cache：</h4><p><strong>缓存</strong>（Web 缓存或 HTTP 缓存）是临时存储 HTTP 响应的组件，以便只要满足某些条件，就可以将其用于后续 HTTP 请求。</p><p><strong>Web缓存</strong>（代理服务器）：</p><img src="https://cdnjson.com/images/2023/03/20/image28c57533d28138e0.png" alt="avatar" style="zoom:67%;" /><ul><li>缓存既是客户端又是服务器</li><li>通常缓存是由ISP安装 (大学、公司、居民区ISP)</li></ul><img src="https://cdnjson.com/images/2023/03/20/image526f313d27cbed65.png" alt="avatar" style="zoom: 67%;" /><img src="https://cdnjson.com/images/2023/03/20/image59b4b89bf717356d.png" alt="avatar" style="zoom:67%;" /><blockquote><p>这部分会有一些计算题，题型示例如下：</p><ul><li>更快的接入链路</li><li>安装本地缓存</li></ul></blockquote><h4 id="客户端：用户代理"><a href="#客户端：用户代理" class="headerlink" title="客户端：用户代理"></a>客户端：用户代理</h4><p>用户代理是代表用户执行操作的任何工具。 </p><p>浏览器<strong>始终</strong>是发起请求的实体，它<strong>从来</strong>都不是服务器。</p><h4 id="网络服务器"><a href="#网络服务器" class="headerlink" title="网络服务器"></a>网络服务器</h4><p>通信通道的另一端是服务器，它根据客户端的请求<strong>提供文档</strong>。 服务器实际上仅显示为一台计算机；</p><p>但它实际上可能是共享负载（负载平衡）的服务器的<strong>集合</strong>，或者是询问其他计算机（如缓存、数据库服务器或电子商务服务器）的<strong>复杂软件</strong>。</p><hr><h3 id="HTTP的基本方面"><a href="#HTTP的基本方面" class="headerlink" title="HTTP的基本方面"></a>HTTP的基本方面</h3><h4 id="HTTP是可扩展的"><a href="#HTTP是可扩展的" class="headerlink" title="HTTP是可扩展的"></a>HTTP是可扩展的</h4><p>HTTP &#x2F; 1.0中引入的<code>HTTP标头</code>使该协议易于扩展和试验。 </p><p>新功能甚至可以通过客户端和服务器之间关于新标头语义的简单协议来引入。</p><h4 id="HTTP是无状态的，但不是无会话的"><a href="#HTTP是无状态的，但不是无会话的" class="headerlink" title="HTTP是无状态的，但不是无会话的"></a>HTTP是无状态的，但不是无会话的</h4><p>服务器并不维护关于客户的任何信息</p><img src="https://cdnjson.com/images/2023/03/20/image0e6c0ee3d22f034b.png" alt="avatar" style="zoom: 80%;" /><ul><li><p>无状态的：在同一连接上连续执行的两个请求之间没有链接。</p><blockquote><p>具体来说，HTTP是无状态的意味着从客户端到服务器的每个请求都被视为一个独立的事务，不知道之前的任何请求。这意味着服务器不存储客户端的会话信息，也不保留任何来自该客户端的先前请求的内存。客户端负责维护状态，如果有必要，可以在每个请求中向服务器发送信息，例如身份验证凭据或会话ID。这种设计在可扩展性和可靠性方面非常有用，因为它允许服务器独立处理每个请求，并确保在服务器出现故障时不会丢失会话数据。</p></blockquote></li><li><p>有状态会话：HTTP cookie允许使用有状态会话。 使用标头可扩展性，HTTP Cookie 将添加到工作流中，从而允许在每个 HTTP 请求上创建会话以共享相同的上下文或相同的状态。</p></li></ul><h4 id="HTTP和连接"><a href="#HTTP和连接" class="headerlink" title="HTTP和连接"></a>HTTP和连接</h4><p>连接在传输层进行控制，因此从根本上超出了 HTTP 的范围。</p><p>HTTP 不要求基础传输协议基于连接;它只要求它是可靠的，或者不丢失消息。</p><p>在互联网上最常见的两种传输协议中，TCP是可靠的，UDP不是。 因此，<strong>HTTP依赖于基于连接的TCP标准</strong>。</p><p>更进一步地，HTTP分为<strong>非持久HTTP和可持久HTTP</strong></p><img src="https://cdnjson.com/images/2023/03/20/imagefdaa0ed04498039f.png" alt="avatar" style="zoom:80%;" /><p><strong>响应时间模型</strong>：</p><p><img src="https://cdnjson.com/images/2023/03/20/image0650084f136d9f1c.png" alt="avatar"></p><p>则可以得出：</p><img src="https://cdnjson.com/images/2023/03/20/image1710c48906cf7aff.png" alt="avatar" style="zoom:80%;" /><hr><h3 id="HTTP可以控制什么"><a href="#HTTP可以控制什么" class="headerlink" title="HTTP可以控制什么?"></a>HTTP可以控制什么?</h3><blockquote><ul><li>缓存： 文档的缓存方式可以通过 HTTP 控制。 服务器可以指示代理和客户端缓存什么以及缓存多长时间。 客户端可以指示中间缓存代理忽略存储的文档。</li><li>放宽原点约束： 为了防止窥探和其他隐私侵犯，Web 浏览器强制在网站之间严格隔离。 只有来自同一来源的页面才能访问网页的所有信息。 虽然这样的约束对服务器来说是一种负担，但 HTTP 标头可以放松服务器端的这种严格分离，允许文档成为来自不同域的信息的拼凑;这样做甚至可能有与安全相关的原因。</li><li>身份验证： 某些页面可能受到保护，以便只有特定用户才能访问它们。 基本身份验证可以通过 HTTP 提供，可以使用 WWW-Authenticate 和类似的标头，也可以通过使用 HTTP cookie 设置特定会话来提供。</li><li>代理和隧道： 服务器或客户端通常位于 Intranet 上，对其他计算机隐藏其真实 IP 地址。 然后，HTTP 请求通过代理来跨越此网络屏障。 并非所有代理都是 HTTP 代理。 例如，SOCKS协议在较低级别运行。 其他协议（如 ftp）可以由这些代理处理。</li><li>会议： 使用 HTTP Cookie 允许您将请求与服务器的状态相关联。 这会创建会话，尽管基本的HTTP是无状态协议。 这不仅对电子商务购物篮有用，而且对允许用户配置输出的任何站点也很有用。</li></ul></blockquote><hr><h3 id="HTTP流"><a href="#HTTP流" class="headerlink" title="HTTP流"></a>HTTP流</h3><p>当客户端想要与服务器，也可能是中间代理通信时，它会执行以下步骤：</p><ol><li><p>打开 TCP 连接：</p><p> TCP 连接用于发送一个或多个请求并接收应答。 客户端可以打开新连接、重用现有连接或打开与服务器的多个 TCP 连接。</p></li><li><p>发送HTTP消息：</p><p> HTTP消息（在HTTP &#x2F; 2之前）是人类可读的ASCII。 使用HTTP &#x2F; 2，这些简单的消息被封装在框架中，使它们无法直接读取，但原理保持不变。</p><p> 一个例子：</p> <img src="https://cdnjson.com/images/2023/03/20/image6d271a2fc58c258e.png" alt="avatar" style="zoom:80%;" />  </li><li><p>读取服务器发送的响应</p><p> 一个例子：</p> <img src="https://cdnjson.com/images/2023/03/20/image22be64134434b9e0.png" alt="avatar" style="zoom: 80%;" />  </li><li><p>关闭或重新使用连接以处理进一步的请求</p></li></ol><hr><h3 id="HTTP消息与报文"><a href="#HTTP消息与报文" class="headerlink" title="HTTP消息与报文"></a>HTTP消息与报文</h3><p>有两种类型的 HTTP 消息：请求和响应，每种都有自己的格式。</p><h4 id="请求："><a href="#请求：" class="headerlink" title="请求："></a>请求：</h4><img src="https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview/http_request.png" alt="avatar" style="zoom:80%;" /><p>更形式化地：</p><img src="https://cdnjson.com/images/2023/03/20/imageac69b0bb3a3c845a.png" alt="avatar" style="zoom:80%;" /><p>关于提交表单输入：</p><ul><li><p>URL(Uniform Resource Locator统一资源定位符)：</p><ul><li><p>一个标准的URL语法组成是下面这样的：</p><p>  <code>scheme://login:password@address:port/path_to_resource?query_string#fragment</code></p><p>  抽象一下：传输协议 + 域名或IP地址 + [端口(端口为80时可省略)] + 资源路径 + 查询字符串</p></li><li><p><strong>GET</strong>：</p><p>  GET - 通常是从指定的服务器中获取数据，查询字符串（键值对）被附加在URL地址后面一起发送到服务器，如下面这样的：<code>http://localhost:8090/api/query?id=3</code>。</p></li><li><p><strong>POST</strong>：</p><p>  POST - 通常是提交数据给指定的服务器处理，当然也可以从服务器获取数据。使用POST方法时，查询字符串或发送的数据在POST信息中单独存在，和请求URL一起发送到服务器，而不是像GET方法一样直接放在URL中。</p></li></ul></li><li><p>GET和POST的区别：</p><ul><li>从上面的例子我们可以看到，GET请求消息体（body）为空，POST请求带有消息体（请区分请求body和响应body）。</li><li>GET提交的数据会放在URL之后，以<code>?</code>分割URL和传输数据，参数之间以&amp;相连，如<code>query?name=test1&amp;id=123456</code></li><li>POST方法是把提交的数据放在HTTP包的请求body中。</li><li>GET提交的数据大小有限制（因为浏览器对URL的长度有限制），而POST方法提交的数据没有限制。</li><li>GET方式提交数据，会带来安全问题，比如一个登录页面，通过GET方式提交数据时，用户名和密码将出现在URL上，如果页面可以被缓存或者其他人可以访问这台机器，就可以从历史记录获得该用户的账号和密码。</li></ul></li></ul><h4 id="响应："><a href="#响应：" class="headerlink" title="响应："></a>响应：</h4><img src="https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview/http_response.png" alt="avatar" style="zoom:80%;" /><p><strong>HTTP响应状态码</strong>：</p><p>位于服务器-&gt;客户端的响应报文中的首行，一些例子如下：</p><img src="https://cdnjson.com/images/2023/03/20/image24df29d653810384.png" alt="avatar" style="zoom:80%;" /><hr><h3 id="HTTP-Cookies："><a href="#HTTP-Cookies：" class="headerlink" title="HTTP - Cookies："></a>HTTP - Cookies：</h3><p>HTTP cookies就是 <strong>服务器端发送给浏览器端的一小部分数据</strong></p><p>用户-服务器状态，有四个组成部分，可以用于用户验证，购物车，推荐和用户状态等。</p><img src="https://cdnjson.com/images/2023/03/20/image25e57c65ed133283.png" alt="avatar" style="zoom:67%;" /><p>Cookies可以<strong>维护状态</strong>，How？：</p><ul><li>协议端节点：在多个事务上 ，发送端和接收端维持状态</li><li>cookies: http报文携带状态信息</li></ul><p>Cookies与隐私：</p><img src="https://cdnjson.com/images/2023/03/20/image4af0dd2b5b6bab42.png" alt="avatar" style="zoom: 67%;" /><hr><h2 id="FTP文件传输协议"><a href="#FTP文件传输协议" class="headerlink" title="FTP文件传输协议"></a>FTP文件传输协议</h2><p>第七版的自顶向下计算机网络删除了这一部分内容，有可能是因为这个技术有点老，现在用的不是很多了。</p><p>了解即可。</p><p>默认端口号：21</p><p>FTP的RFC：959</p><img src="https://cdnjson.com/images/2023/03/21/imageafdfc0d77807b783.png" alt="avatar" style="zoom: 80%;" /><h3 id="控制连接和数据连接事分开的，两个不同的端口"><a href="#控制连接和数据连接事分开的，两个不同的端口" class="headerlink" title="控制连接和数据连接事分开的，两个不同的端口"></a>控制连接和数据连接事分开的，两个不同的端口</h3><p>HTTP只用一个端口</p><img src="https://cdnjson.com/images/2023/03/21/image1759ddbc37f71cbc.png" alt="avatar" style="zoom:67%;" /><p>服务主动向客户端20号端口发起一个连接</p><ul><li>带内的传数据</li><li>带外的传控制信息和指令</li></ul><h3 id="FTP协议有状态"><a href="#FTP协议有状态" class="headerlink" title="FTP协议有状态"></a>FTP协议有状态</h3><p>服务器要维护这个状态，与HTTP不一样。</p><hr><h2 id="Email"><a href="#Email" class="headerlink" title="Email"></a>Email</h2><p>电子邮件的三个主要组成部分：</p><ul><li>用户代理（邮件阅读器）</li><li>邮件服务器</li><li>简单邮件传输协议：SMTP</li></ul><img src="https://cdnjson.com/images/2023/03/21/imagee0d2ad6dbbeb693d.png" alt="avatar" style="zoom: 80%;" /><h3 id="邮件服务器"><a href="#邮件服务器" class="headerlink" title="邮件服务器"></a>邮件服务器</h3><p><strong>管理和维护</strong>发送给用户的邮件；</p><p>输出<strong>报文队列</strong>保持待发送邮件报文；</p><h3 id="SMTP"><a href="#SMTP" class="headerlink" title="SMTP"></a>SMTP</h3><p>RFC 2821；</p><p>SMTP使用持久连接；</p><p>SMTP使用CRLF.CRLF决定报文的尾部；</p><p>使用TCP在客户端和服务器之间发送报文；</p><p>默认端口号为25；</p><p>使用直接传输，即从发送方服务器到接收方服务器</p><p>传输的三个阶段：</p><ul><li>握手</li><li>传输报文</li><li>关闭</li></ul><p>命令&#x2F;响应交互：（报文必须是7位ASCII码）</p><ul><li>命令：ASCII文本</li><li>响应：状态码和状态信息</li></ul><p>与HTTP的对比：</p><img src="https://cdnjson.com/images/2023/03/21/image94f864dc927fc86b.png" alt="avatar" style="zoom:67%;" /><h3 id="邮件报文格式与多媒体扩展"><a href="#邮件报文格式与多媒体扩展" class="headerlink" title="邮件报文格式与多媒体扩展"></a>邮件报文格式与多媒体扩展</h3><img src="https://cdnjson.com/images/2023/03/21/image5fdf03bceeccec09.png" alt="avatar" style="zoom: 80%;" /><p>多媒体扩展：</p><p>MIME - multimedia mail extension</p><p>RFC 2045, 2056</p><p>在报文首部用额外的行申明MIME内容类型</p><h3 id="邮件访问协议"><a href="#邮件访问协议" class="headerlink" title="邮件访问协议"></a>邮件访问协议</h3><img src="https://cdnjson.com/images/2023/03/21/image64369186f029e146.png" alt="avatar" style="zoom:80%;" /><p>与SMTP的区别：</p><ul><li><p>SMTP是<strong>传送到接收方</strong>的邮件服务器</p></li><li><p>邮件访问协议是<strong>从服务器访问邮件</strong></p><p>  具体来说：</p><ul><li>POP：邮局访问协议——用户身份确认 (代理&lt;–&gt;服务器) 并下载</li><li>IMAP：Internet邮件访问协议——更多特性但也更复杂，在服务器上处理存储的报文</li><li>HTTP：Hotmail , Yahoo! Mail等——方便</li></ul></li></ul><h3 id="关于POP3协议的更多："><a href="#关于POP3协议的更多：" class="headerlink" title="关于POP3协议的更多："></a>关于POP3协议的更多：</h3><img src="https://cdnjson.com/images/2023/03/21/image5a65dcef36501dd0.png" alt="avatar" style="zoom: 80%;" /><img src="https://cdnjson.com/images/2023/03/21/imagebbaa174f5b19e4a5.png" alt="avatar" style="zoom:80%;" /><hr><h2 id="DNS-互联网的电话簿"><a href="#DNS-互联网的电话簿" class="headerlink" title="DNS - 互联网的电话簿"></a>DNS - 互联网的电话簿</h2><h3 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h3><p>DNS（域名系统 Domain Name System ）是互联网连接资源的分层和分散命名系统；</p><p>DNS维护域名列表以及与其关联的资源（如 IP 地址）；</p><p>DNS最突出的功能是将人性化域名转换为数字IP地址；</p><p>将域名映射到相应 IP 地址的过程称为 DNS 查找，相比之下，反向 DNS 查找 （rDNS） 用于确定与 IP 地址关联的域名。</p><h3 id="额外的安全层？"><a href="#额外的安全层？" class="headerlink" title="额外的安全层？"></a>额外的安全层？</h3><p>默认情况下，DNS查询Request和响应Response以<strong>明文形式</strong>，通过UDP发送，也就是说网络、ISP 或任何能够监控传输的人都可以读取它们。即使网站使用 HTTPS，也会公开导航到该网站所需的 DNS 查询。</p><p>如何解决呢？事实上有两种办法：TLS上的DNS和HTTPS上的DNS，他们都是为加密明文DNS流量而开发的标准。</p><p><img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/7qcyOJwWyOt4EVJykiIRTn/30e34453409eb42fa1ec36680609ad8d/dns-traffic-over-tls-https.svg" alt="avatar"></p><h3 id="DNS-x2F-HTTPS-over-DNS"><a href="#DNS-x2F-HTTPS-over-DNS" class="headerlink" title="DNS &#x2F; HTTPS-over-DNS?"></a>DNS &#x2F; HTTPS-over-DNS?</h3><p>TLS 上的 DNS 或 DoT 是加密 DNS 查询以保持其安全和私密性的标准。</p><p>DoT 使用与 HTTPS 网站相同的安全协议 TLS 来加密和验证通信（TLS也称为”SSL”）。</p><p>DoT 在用户数据报协议 UDP 之上添加了 TLS 加密，该协议用于 DNS 查询。</p><p>此外，它还确保 DNS 请求和响应不会因中间人攻击而被篡改或伪造。</p><p>DNS over HTTPS，或DoH，是DoT的替代方案。</p><p>使用 DoH，DNS 查询和响应是加密的，但它们是通过 HTTP 或 HTTP&#x2F;2 协议发送的，而不是直接通过 UDP 发送的。</p><h4 id="有何不同？"><a href="#有何不同？" class="headerlink" title="有何不同？"></a><strong>有何不同</strong>？</h4><p>每个标准都是单独开发的，都有自己的 RFC（征求意见） 文档，</p><p>但 DoT 和 DoH 之间最重要的<strong>区别在于它们使用的端口</strong>。</p><p>DoT 仅使用端口 853，而 DoH 使用端口 443，这是<strong>所有其他 HTTPS 流量使用的端口</strong>。</p><p>由于 DoT 具有专用端口，因此任何具有网络可见性的人都可以看到 DoT 流量的来来去去，即使请求和响应本身是加密的。相比之下，在DoH中，DNS查询和响应伪装在其他HTTPS流量中，因为它们都来自同一端口。</p><h4 id="哪个更好？"><a href="#哪个更好？" class="headerlink" title="哪个更好？"></a>哪个更好？</h4><p>从网络安全的角度来看，DoT可以说更好。它使网络管理员能够监视和阻止 DNS 查询，这对于识别和阻止恶意流量非常重要。同时，DoH查询隐藏在常规HTTPS流量中，这意味着如果不阻止所有其他HTTPS流量，就无法轻松阻止它们。</p><p>但是，从隐私的角度来看，DoH可以说是可取的。使用 DoH，DNS 查询隐藏在较大的 HTTPS 流量中。这降低了网络管理员的可见性，但为用户提供了更多的隐私。</p><hr><h3 id="涉及-DNS-的常见攻击有哪些？"><a href="#涉及-DNS-的常见攻击有哪些？" class="headerlink" title="涉及 DNS 的常见攻击有哪些？"></a>涉及 DNS 的常见攻击有哪些？</h3><ul><li><strong>DNS 欺骗&#x2F;缓存中毒</strong>：这是一种将伪造的 DNS 数据引入 DNS 解析程序缓存的攻击，导致解析器返回域的错误 IP 地址。流量可以转移到恶意机器或攻击者想要的任何地方，而不是转到正确的网站;通常，这将是用于恶意目的（例如分发恶意软件或收集登录信息）的原始站点的副本。</li><li><strong>DNS 隧道</strong>：此攻击使用其他协议通过 DNS 查询和响应进行隧道传输。攻击者可以使用 SSH、TCP 或 HTTP 将恶意软件或被盗信息传递到 DNS 查询中，而大多数防火墙都不会检测到。</li><li><strong>DNS 劫持</strong>：在 DNS 劫持中，攻击者将查询重定向到其他域名服务器。这可以通过恶意软件或未经授权修改 DNS 服务器来完成。尽管结果与DNS欺骗的结果相似，但这是一种根本不同的攻击，因为它针对的是名称服务器上网站的DNS记录，而不是解析器的缓存。</li><li><strong>NXDOMAIN 攻击</strong>：这是一种 DNS 洪水攻击，攻击者用请求淹没 DNS 服务器，请求不存在的记录，试图导致合法流量的拒绝服务。这可以使用复杂的攻击工具来实现，这些工具可以为每个请求自动生成唯一的子域。NXDOMAIN 攻击还可以针对递归解析器，目的是用垃圾请求填充解析器的缓存。</li><li><strong>幻域攻击</strong>：幻像域攻击的结果与 DNS 解析器上的 NXDOMAIN 攻击类似。攻击者设置了一堆“幻像”域服务器，这些服务器要么响应请求非常慢，要么根本不响应。然后，解析器受到对这些域的大量请求的打击，解析器被占用等待响应，导致性能降低和拒绝服务。</li><li><strong>随机子域攻击</strong>：在这种情况下，攻击者会针对一个合法站点的多个随机、不存在的子域发送 DNS 查询。目标是为域的权威名称服务器创建拒绝服务，从而无法从名称服务器查找网站。作为副作用，为攻击者提供服务的ISP也可能受到影响，因为他们的递归解析器的缓存将加载错误请求。</li><li><strong>域锁定攻击</strong>：攻击者通过设置特殊域和解析器来与其他合法解析器创建 TCP 连接，从而策划这种形式的攻击。当目标解析器发送请求时，这些域会发回缓慢的随机数据包流，从而占用解析器的资源。</li><li><strong>基于僵尸网络的 CPE 攻击</strong>：这些攻击是使用 CPE 设备（客户端设备;这是服务提供商提供的供其客户使用的硬件，例如调制解调器、路由器、电缆盒等）进行的。攻击者破坏CPE，设备成为僵尸网络的一部分，用于对一个站点或域执行随机子域攻击。</li></ul><p><del>（怎么名字都这么帅的）</del></p><p><img src="https://www.cloudflare.com/img/learning/dns/dns-security/dns-hijacking.png" alt="avatar"></p><hr><h3 id="域名安全"><a href="#域名安全" class="headerlink" title="域名安全"></a>域名安全</h3><p>DNS 安全扩展 （DNSSEC） 是为缓解此问题而创建的安全协议。</p><p>DNSSEC 通过对数据进行数字签名来帮助确保其有效性，从而防止攻击。</p><p>为了确保安全查找，必须在 DNS 查找过程中的<strong>每个级别进行签名</strong>。</p><p>有关 DNSSEC 的更多内容请参考：<a href="https://blog.cloudflare.com/dnssec-an-introduction/?_gl=1*1k44464*_ga*MzE2OTQ0NzY1LjE2NzUxNDM3NzI.*_gid*NjAzODc2NjAyLjE2NzUxNDM3NzI.">DNSSEC: An Introduction (cloudflare.com)</a></p><hr><h3 id="其他内容："><a href="#其他内容：" class="headerlink" title="其他内容："></a>其他内容：</h3><h4 id="DNS防火墙："><a href="#DNS防火墙：" class="headerlink" title="DNS防火墙："></a>DNS防火墙：</h4><p>DNS firewall 是一种可为 DNS 服务器提供众多安全和性能服务的工具。</p><p>DNS firewall 位于用户的递归解析器与他们正尝试访问的网站或服务的权威性域名服务器之间。</p><p>防火墙可提供速率限制服务，以关闭试图淹没服务器的攻击者。如果服务器确实由于攻击或其他任何原因而停机，则 DNS firewall 可通过提供来自高速缓存的 DNS 响应来使运营商的站点或服务保持正常运行。</p><h4 id="用户隐私："><a href="#用户隐私：" class="headerlink" title="用户隐私："></a>用户隐私：</h4><p>DNS 查询未加密。即使用户使用像 1.1.1.1 这样不跟踪他们活动的 DNS 解析器，DNS 查询也会以明文形式在互联网上传输。这意味着拦截查询的任何人都可以看到用户正在访问哪些网站。</p><p><em>还有诸如DNS高速缓存中毒等内容我们暂且略过。</em></p><hr><h2 id="DomainName-域名"><a href="#DomainName-域名" class="headerlink" title="DomainName - 域名"></a>DomainName - 域名</h2><h3 id="0-简介："><a href="#0-简介：" class="headerlink" title="0. 简介："></a>0. 简介：</h3><p>域名是一个文本字符串，映射到一个数字 IP 地址，可用于从客户端软件访问网站。</p><p>简单来说，域名是用户在浏览器窗口中键入以访问特定网站的文本。</p><p>网站的实际地址是一个复杂的数字 IP 地址（例如 103.21.244.0），但由于 DNS 的存在，用户可以输入人类友好的域名并将其路由到他们要查找的网站。此过程称为 DNS 查找。</p><p>域名全部由域名注册管理机构管理。</p><h3 id="1-与URL的区别？"><a href="#1-与URL的区别？" class="headerlink" title="1. 与URL的区别？"></a>1. 与URL的区别？</h3><p>统一资源定位符（URL）有时也称为网址，包含站点的域名以及其他信息，如传输协议和路径等。</p><p>例如，在 URL<code>https://www.bilibili.com/anime/</code>中，<code>www.bilibili.com</code>是域名，而<code>https</code>是协议，<code>/anime/</code>是指向网站上特定页面的路径。</p><h3 id="2-域名的组成部分？"><a href="#2-域名的组成部分？" class="headerlink" title="2. 域名的组成部分？"></a>2. 域名的组成部分？</h3><p>域名通常分为两个或三个部分，各个部分用一个点分隔。</p><p>从右到左阅读时，域名中的<strong>标识符从最广泛到最具体</strong>。</p><p>域名中最后一个点右边的部分是顶级域 (TLD)。</p><p>其中包括<code>.com、.net和.org </code>等通用TLD，以及<code>.uk</code>和<code>.cn</code>等特定国家&#x2F;地区的 TLD。</p><p>TLD 的左侧是第二级域（2LD），如果 2LD 的左侧有任何内容，则称为第三级域（3LD）。</p><h3 id="3-查找可用域名："><a href="#3-查找可用域名：" class="headerlink" title="3. 查找可用域名："></a>3. 查找可用域名：</h3><p>法一：转到域名注册商的网站。他们中的大多数都提供<code>whois</code>服务，告诉您域名是否可用。</p><p>法二：如果使用具有内置 shell 的系统，请在其中键入命令，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ whois mozilla.org</span><br><span class="line">Domain Name:MOZILLA.ORG</span><br><span class="line">Domain ID: D1409563-LROR</span><br><span class="line">Creation Date: 1998-01-24T05:00:00Z</span><br><span class="line">Updated Date: 2013-12-08T01:16:57Z</span><br><span class="line">Registry Expiry Date: 2015-01-23T05:00:00Z</span><br><span class="line">Sponsoring Registrar:MarkMonitor Inc. (R37-LROR)</span><br><span class="line">Sponsoring Registrar IANA ID: 292</span><br><span class="line">WHOIS Server:</span><br><span class="line">Referral URL:</span><br><span class="line">Domain Status: clientDeleteProhibited</span><br><span class="line">Domain Status: clientTransferProhibited</span><br><span class="line">Domain Status: clientUpdateProhibited</span><br><span class="line">Registrant ID:mmr-33684</span><br><span class="line">Registrant Name:DNS Admin</span><br><span class="line">Registrant Organization:Mozilla Foundation</span><br><span class="line">Registrant Street: 650 Castro St Ste 300</span><br><span class="line">Registrant City:Mountain View</span><br><span class="line">Registrant State/Province:CA</span><br><span class="line">Registrant Postal Code:94041</span><br><span class="line">Registrant Country:US</span><br><span class="line">Registrant Phone:+1.6509030800</span><br></pre></td></tr></table></figure><h3 id="4-其他内容"><a href="#4-其他内容" class="headerlink" title="4. 其他内容"></a>4. 其他内容</h3><p>域名的最长保留期是十年。用户可以持有域名超过十年，因为注册商通常让他们无限期地续订域名。但是，用户从来没有真正拥有过这个域名，他们只是租用了它。</p><p><img src="https://www.cloudflare-cn.com/img/learning/dns/glossary/expired-domains/expired-domain-timeline.svg" alt="avatar"></p><p>如果没有人购买过期的域名，它可能会在一定时间后退回到原来的注册机构。它将不再可用，直到注册表决定释放它。</p><hr><h2 id="P2P应用"><a href="#P2P应用" class="headerlink" title="P2P应用"></a>P2P应用</h2><p>暂略</p><hr><h2 id="TCP套接字编程"><a href="#TCP套接字编程" class="headerlink" title="TCP套接字编程"></a>TCP套接字编程</h2><p>暂略</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;Computer Networking A Top-Down Approach Learning Note Part 2&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;本文是笔者初学计算机网络的笔记和一些心得，难免会有部分疏漏和错误</summary>
      
    
    
    
    <category term="408" scheme="https://conqueror712.github.io/categories/408/"/>
    
    
  </entry>
  
</feed>
