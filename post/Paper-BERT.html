<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>2023了，再来看看NLP经典之作 - BERT丨论文解读 | 落雨乄天珀夜</title><meta name="author" content="落雨乄天珀夜"><meta name="copyright" content="落雨乄天珀夜"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="零、前言：📕欢迎访问：  个人博客：https:&#x2F;&#x2F;conqueror712.github.io&#x2F; 知乎：https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;soeur712&#x2F;posts Bilibili：https:&#x2F;&#x2F;space.bilibili.com&#x2F;57089326 掘金：https:&#x2F;&#x2F;juejin.cn&#x2F;user&#x2F;1297878069809725&#x2F;posts   🍬观前小剧">
<meta property="og:type" content="article">
<meta property="og:title" content="2023了，再来看看NLP经典之作 - BERT丨论文解读">
<meta property="og:url" content="https://conqueror712.github.io/post/Paper-BERT.html">
<meta property="og:site_name" content="落雨乄天珀夜">
<meta property="og:description" content="零、前言：📕欢迎访问：  个人博客：https:&#x2F;&#x2F;conqueror712.github.io&#x2F; 知乎：https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;soeur712&#x2F;posts Bilibili：https:&#x2F;&#x2F;space.bilibili.com&#x2F;57089326 掘金：https:&#x2F;&#x2F;juejin.cn&#x2F;user&#x2F;1297878069809725&#x2F;posts   🍬观前小剧">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a82ec344c5e746a789f5a5f10577397a~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?">
<meta property="article:published_time" content="2023-05-20T13:48:01.000Z">
<meta property="article:modified_time" content="2023-05-20T13:49:03.900Z">
<meta property="article:author" content="落雨乄天珀夜">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a82ec344c5e746a789f5a5f10577397a~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?"><link rel="shortcut icon" href="https://s2.loli.net/2023/02/15/LprEbGXq9xgfdDn.jpg"><link rel="canonical" href="https://conqueror712.github.io/post/Paper-BERT"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '2023了，再来看看NLP经典之作 - BERT丨论文解读',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-20 21:49:03'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="落雨乄天珀夜" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2023/02/15/LprEbGXq9xgfdDn.jpg" onerror="onerror=null;src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a82ec344c5e746a789f5a5f10577397a~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">落雨乄天珀夜</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">2023了，再来看看NLP经典之作 - BERT丨论文解读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-20T13:48:01.000Z" title="发表于 2023-05-20 21:48:01">2023-05-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-20T13:49:03.900Z" title="更新于 2023-05-20 21:49:03">2023-05-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="2023了，再来看看NLP经典之作 - BERT丨论文解读"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="零、前言："><a href="#零、前言：" class="headerlink" title="零、前言："></a>零、前言：</h1><p><strong>📕欢迎访问</strong>：</p>
<blockquote>
<p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p>
<p>知乎：<a target="_blank" rel="noopener" href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p>
<p>Bilibili：<a target="_blank" rel="noopener" href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p>
<p>掘金：<a target="_blank" rel="noopener" href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p>
</blockquote>
<hr>
<p><strong>🍬观前小剧场</strong>：</p>
<blockquote>
<p>Q：2023年了，如果还没看过BERT，或者是刚入门的小白，该怎么办？</p>
<p>A：没关系，那就现在开始看吧，<strong>只有先开始才能走向新的开始</strong>！</p>
<p>Q：我觉得你说的有道理，不过话说回来，正确的阅读姿势是什么？</p>
<p>A：我想，你需要准备一杯咖啡或者茶，安静地坐下来，以及<strong>30~45分钟的时间</strong>，慢下来阅读，这可能会比急急忙忙地看一遍的收获更大，印象更深。</p>
<p>Q：如果我只是想了解个大概，并不想知道具体的细节怎么办，我现在就在通勤的地铁上，我也没有位置坐，很急！</p>
<p>A：没关系，笔者考虑到了这种情况，只需要阅读<strong>前两节或者前三节</strong>的内容即可。</p>
<p>A：事不宜迟，如果没有其他问题的话我们就开始吧！</p>
</blockquote>
<hr>
<h1 id="一、BERT简介："><a href="#一、BERT简介：" class="headerlink" title="一、BERT简介："></a>一、BERT简介：</h1><blockquote>
<p>BERT开源仓库：<a target="_blank" rel="noopener" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></p>
</blockquote>
<h2 id="一句话解释BERT："><a href="#一句话解释BERT：" class="headerlink" title="一句话解释BERT："></a>一句话解释BERT：</h2><p>是一个预训练的语言表征模型，是一个<strong>深度的、双向的Transformer</strong>，用来做<strong>预训练</strong>，针对一般的语言理解任务。</p>
<blockquote>
<p>Q：预训练是什么？</p>
<p>A：预训练就是，事先在一个较大的数据集上面训练好一个模型的这次训练过程（在这之后，再把这个模型拿到别的任务上面继续训练）。</p>
</blockquote>
<hr>
<h2 id="BERT出现的目的和意义："><a href="#BERT出现的目的和意义：" class="headerlink" title="BERT出现的目的和意义："></a>BERT出现的目的和意义：</h2><p>BERT的目的是通过<strong>在所有层中对左右上下文</strong>进行联合条件反射，从<strong>未标记的文本</strong>中预训练深度双向表示。</p>
<p>普适性：预训练的BERT模型可以通过一个<strong>额外的输出层</strong>进行<strong>微调</strong>，从而应用于更多样的任务，如问答和语言推理，并且无需对特定的任务架构进行大量的修改。</p>
<p>BERT出现的意义：让我们能够从一个大数据集上训练好一个比较深的神经网络，用于很多不同的NLP任务上面，既简化了训练，又提升了性能。</p>
<blockquote>
<p>Q：BERT名字的由来？</p>
<p>A：B&#x3D;Bidirectional, E&#x3D;Encoder, R&#x3D;Representations, T&#x3D;Transformers</p>
</blockquote>
<p>乍一看，怎么BERT这个名字和他的含义不是特别沾边啊？</p>
<p>事实上，这是因为BERT基于了ELMo的一些工作，而这两个名字都是美国的一个老少皆宜的少儿英语学习节目——<strong>芝麻街</strong>（笑）。</p>
<blockquote>
<p>芝麻街中文官网：<a target="_blank" rel="noopener" href="https://www.sesamestreetchina.com.cn/">https://www.sesamestreetchina.com.cn/</a></p>
</blockquote>
<p>于是乎，一发不可收拾，NLP领域的文章就把芝麻街中人物的名字用了个遍。</p>
<p><img src="https://cdnjson.com/images/2023/05/19/image79f52a4bd5c81654.png" alt="avatar"></p>
<hr>
<h1 id="二、BERT与GPT、ELMo的关系"><a href="#二、BERT与GPT、ELMo的关系" class="headerlink" title="二、BERT与GPT、ELMo的关系"></a>二、BERT与GPT、ELMo的关系</h1><blockquote>
<p>由于BERT是基于<strong>GPT</strong>和<strong>ELMo</strong>这两个工作的，我们就先来看一下他们之间有什么区别。</p>
</blockquote>
<p><img src="https://cdnjson.com/images/2023/05/20/image2decf2d7bd0b6324.png" alt="avatar"></p>
<h2 id="BERT与GPT的区别："><a href="#BERT与GPT的区别：" class="headerlink" title="BERT与GPT的区别："></a>BERT与GPT的区别：</h2><p>在2018年，GPT是<strong>单向的</strong>，基于左边的上下文信息，来生成新的文段。（2023年有待考究）；</p>
<p>而BERT不仅用了左侧的信息，还用了右侧的信息，所以说是一个<strong>双向的</strong>操作，即Bidirectional。</p>
<h2 id="BERT与ELMo的区别："><a href="#BERT与ELMo的区别：" class="headerlink" title="BERT与ELMo的区别："></a>BERT与ELMo的区别：</h2><p>ELMo用的是基于RNN的架构，在用于一些特定的下游任务时，需要<strong>对架构做一定的调整</strong>；</p>
<p>BERT用的是Transformer，只需要<strong>改最上层</strong>的就行了，这里指的是BERT的那个额外的输出层。</p>
<blockquote>
<p>之后文章列举了自己在11个NLP任务上面的更好的表现，并且科学地给出了绝对值和相对值，这是非常优秀的一种写文细节，我们在写论文的时候也可以借鉴这种写法。</p>
</blockquote>
<hr>
<h1 id="三、BERT预训练"><a href="#三、BERT预训练" class="headerlink" title="三、BERT预训练"></a>三、BERT预训练</h1><h2 id="预训练在NLP的应用："><a href="#预训练在NLP的应用：" class="headerlink" title="预训练在NLP的应用："></a>预训练在NLP的应用：</h2><p>采用预训练的两大类自然语言任务：</p>
<ul>
<li><strong>句子层面</strong>的任务：通过整体分析来预测句子之间的关系，如对句子情绪的识别，或者两个句子之间的关系 ；</li>
<li><strong>词语层面</strong>的任务：标记级任务，如命名实体识别和问答（比如”张三”这个词是不是人名？还是一种食物？）。</li>
</ul>
<p>事实上，预训练在计算机视觉领域已经用了很多年了，BERT也不是自然语言处理领域第一个用预训练的，只是说，因为BERT的效果太好了，以至于**”抢了风头”**。</p>
<p>预训练的表示，减少了对许多高度工程化的任务的<strong>特定架构的需求</strong>。</p>
<p>BERT是第一个<strong>基于调优</strong>的表示模型，在大量句子级和记号级的任务上实现了最先进的性能，甚至优于很多特定于某种任务的架构。</p>
<hr>
<h2 id="NLP中两种预训练的策略："><a href="#NLP中两种预训练的策略：" class="headerlink" title="NLP中两种预训练的策略："></a>NLP中两种预训练的策略：</h2><p>语言模型预训练应用于下游任务，或者说特征表示，有两种现成的策略：基于特征、基于微调。</p>
<ul>
<li>基于特征：如ELMo——使用<strong>特定于任务</strong>的RNN架构，把学到的特征和输入一起作为特征的表达。</li>
<li>基于微调：如GPT——引入了最小的任务特定参数，并通过简单的<strong>一点点微调</strong>所有预训练参数，来对下游任务进行训练。</li>
</ul>
<p>在当时，这些的技术限制了预训练表征的力量，主要是因为标准语言模型是单向的。</p>
<p>传统的单向语言模型，或者是浅层拼接两个单向语言模型，只能获取单方向的上下文信息。</p>
<p>如果要获取句子层面，或者说句子之间的信息的话，就显得有些吃力了。</p>
<blockquote>
<p>那么，BERT是如何来解决这一问题的呢？</p>
</blockquote>
<hr>
<h2 id="MLM——来自远古的秘宝："><a href="#MLM——来自远古的秘宝：" class="headerlink" title="MLM——来自远古的秘宝："></a>MLM——来自远古的秘宝：</h2><p>BERT的使用Transformer的双向编码器表示来改进基于微调的方法；</p>
<p>使用带<strong>掩码的语言模型MLM</strong>, Masked Language Model对双向的Transformer进行预训练，缓解了单向性约束，生成深度的双向语言表征。</p>
<p>MLM使得能<strong>融合左右上下文</strong>，所以才能预训练深度双向Transformer，收到了Cloze的启发（这个竟然是1953年的论文）。</p>
<p>那么MLM在做什么呢？其实就是每一次<strong>随机选择一些字</strong>，然后**”盖住”<strong>，目标函数显而易见了，就是</strong>预测那些被盖住的字**。</p>
<p>有没有**”完形填空”**的感觉？事实上就是这样。</p>
<p>有了这个概念，我们就方便理解了，做完形填空的时候，就是不仅要看左边的文段，还要看右边的文段才行。</p>
<p>MLM除了完形填空之外，对于**”句子匹配”**也是有帮助的（怎么都是英语考试的题型）。</p>
<p>句子匹配的意思是说，<strong>随机的选两个句子</strong>，目标函数是要判断这两个句子在原文中<strong>是不是相邻的</strong>。</p>
<p>（有点像句子匹配的题目吧？虽然我也不知道那个题目具体叫什么名字）</p>
<hr>
<h2 id="有标注的数据集一定更好吗："><a href="#有标注的数据集一定更好吗：" class="headerlink" title="有标注的数据集一定更好吗："></a>有标注的数据集一定更好吗：</h2><p>事实上，BERT和它之后的一些工作证明了，有些时候，在NLP任务上，通过没有标号的、大量的数据集训练的模型，效果比有标注的、规模小一些的数据集上，效果更好。</p>
<p>这样的想法也在被计算机视觉等其他领域所采用。</p>
<hr>
<h2 id="BERT预训练与微调："><a href="#BERT预训练与微调：" class="headerlink" title="BERT预训练与微调："></a>BERT预训练与微调：</h2><h3 id="预训练Pre-Training："><a href="#预训练Pre-Training：" class="headerlink" title="预训练Pre-Training："></a>预训练Pre-Training：</h3><p>让我们再次回顾一下预训练在干什么，事实上，就是<strong>在一个没有标注的数据集上训练BERT模型</strong>。</p>
<p>有两个关键点：目标函数、预训练数据。</p>
<h3 id="微调Fine-Tuning："><a href="#微调Fine-Tuning：" class="headerlink" title="微调Fine-Tuning："></a>微调Fine-Tuning：</h3><p>同样是用的BERT模型，但是<strong>初始化权重为预训练过程中得到的权重</strong>。</p>
<p><strong>所有的权重</strong>，在微调的过程中都会参与训练，并且用的是<strong>有标号的数据</strong>。</p>
<p>基于预训练得到的模型，在每个后续的下游任务中都会训练出<strong>适用于当前任务的模型</strong>。</p>
<blockquote>
<p>以下是预训练和微调的图示：（后续还会介绍其细节，先大致浏览一下即可）</p>
</blockquote>
<p><img src="https://cdnjson.com/images/2023/05/19/image4ba18c7a6d70362c.png" alt="avatar"></p>
<blockquote>
<p>论文的这一部分对这些前置的技术做了简要的介绍，这是非常好的习惯。倘若是对于大家可能熟知的任务一笔带过，会显得整个文章不那么得体，不那么自洽。</p>
</blockquote>
<hr>
<h1 id="四、深入BERT模型"><a href="#四、深入BERT模型" class="headerlink" title="四、深入BERT模型"></a>四、深入BERT模型</h1><h2 id="BERT模型架构："><a href="#BERT模型架构：" class="headerlink" title="BERT模型架构："></a>BERT模型架构：</h2><p>简而言之，就是一个多层的、双向的Transformer编码器。</p>
<p>BERT调整了Transformer的三个参数：L（Transformer块的个数）、H（隐藏层维数）、A（自注意力机制的多头的头的数目）。</p>
<p>$BERT_{BASE}$：$L &#x3D; 12, H &#x3D; 768, A &#x3D; 12, Total\ Parameters&#x3D;110M$</p>
<p>$BERT_{LARGE}$：$L &#x3D; 24, H &#x3D; 1024, A &#x3D; 16, Total\ Parameters&#x3D;340M$</p>
<blockquote>
<p>如何把超参数换算成可学习参数的大小呢？</p>
</blockquote>
<h2 id="BERT模型可学习参数计算："><a href="#BERT模型可学习参数计算：" class="headerlink" title="BERT模型可学习参数计算："></a>BERT模型可学习参数计算：</h2><p>可学习参数主要分为两块：嵌入层、Transformer块，过程有图简述之：</p>
<p><img src="https://cdnjson.com/images/2023/05/19/imagef1add2ac16e1587b.png" alt="avatar"><br>自注意力机制本身并没有可学习参数，但是对于<strong>多头注意力</strong>，会把进入的所有K(Key)、V(Value)、Q(Query)都做一次投影，每次投影的维度为64，而且有$A × 64 &#x3D; H$；</p>
<p>如上，前面的自注意力块有$4H^2$个参数，MLP有$H×4H×2&#x3D;8H^2$个参数，则每个Transformer块中有$12H^2$个参数；</p>
<p>最后再乘$L$和字典大小，得到$Sum &#x3D; 30000H+12LH^2$，代入$BERT_{BASE}$可以获得确实是大约$110M$个参数。</p>
<hr>
<h2 id="BERT输入输出："><a href="#BERT输入输出：" class="headerlink" title="BERT输入输出："></a>BERT输入输出：</h2><p>输入：既可以是”句子”，也可以是”句子对”，不过事实上，这里的句子和句子对都不是狭义上的概念，事实上是一个**”序列”**。</p>
<p>因为BERT不同于一般的Transformer，<strong>BERT只有编码器</strong>，而Transformer有编码器和解码器，所以为了能够处理两个句子的情况，需要把两个句子变成一个序列。</p>
<h3 id="切词方法：WordPiece"><a href="#切词方法：WordPiece" class="headerlink" title="切词方法：WordPiece"></a>切词方法：WordPiece</h3><p>如果说按照空格来切词的话，也就是<strong>一个词对应一个Token</strong>，那样会导致<strong>字典大小特别大</strong>，根据上文的可学习参数计算方法，这会导致参数太多而且集中在嵌入层上面。</p>
<p>而WordPiece的想法是：如果一个词在整个文段里出现的<strong>概率不大</strong>的话，应该切开，看其<strong>子序列</strong>（有可能是一个词根）。</p>
<p>如果一个词根出现的概率比较大的话，就<strong>只保留词根</strong>就好了，这样的话会让<strong>字典大小缩小不少</strong>。</p>
<h3 id="合并句子的方法："><a href="#合并句子的方法：" class="headerlink" title="合并句子的方法："></a>合并句子的方法：</h3><p>每一个序列的<strong>第一个词</strong>都是<code>[cls]</code>，代表classification，因为最后的输出要代表整个序列的一个信息，这是因为BERT使用了Transformer的编码器，所以自注意力层里的每一个词都会看输入的所有词的关系，所以可以放在第一个位置。</p>
<p>需要对句子进行一定的区分，以从句子层面进行分类，这里有两个办法：</p>
<ul>
<li>为每个<strong>句子的末尾</strong>添加了一个<code>[SEP]</code>表示separate；</li>
<li>学习一个嵌入层，来知道某个句子是第一个还是第二个。</li>
</ul>
<p>这个时候我们再回去看一下上面给出过的图，就很好理解了：</p>
<p><img src="https://cdnjson.com/images/2023/05/19/image4ba18c7a6d70362c.png" alt="avatar"></p>
<p>如上图，每一个Token进入BERT，得到这个Token的Embedding表示。</p>
<p>整体上的效果就是：输入一个序列，得到一个序列，最后再添加额外的输出层来得到想要的结果。</p>
<p>对于每一个词元，进入BERT时的向量表示的是：</p>
<p>这个词元本身的Embedding + 它在哪个位置的Embedding + 位置的Embedding，如下图所示：</p>
<p><img src="https://cdnjson.com/images/2023/05/19/image856058d1358a11da.png" alt="avatar"></p>
<ul>
<li><strong>Token Embedding</strong>：词元的Embedding层，对于每一个词元，输出一个对应的向量；</li>
<li><strong>Segment Embedding</strong>：A表示第一句话，B表示第二句话；</li>
<li><strong>Position Embedding</strong>：位置的嵌入层，输入的大小是序列最长有多长，输入的是每一个词元的位置信息（0base）</li>
</ul>
<p>这些都是通过学习得来的，而不同于Transformer的手动构造。</p>
<hr>
<h2 id="MLM预训练的细节："><a href="#MLM预训练的细节：" class="headerlink" title="MLM预训练的细节："></a>MLM预训练的细节：</h2><p>知道了切词的方法，我们再来看所谓的”英语题”。</p>
<h3 id="完形填空："><a href="#完形填空：" class="headerlink" title="完形填空："></a>完形填空：</h3><p>对一个输入的词元序列，如果一个词元是<strong>由WordPiece生成</strong>的话（也就是切开来的），那就会有15%的概率<strong>随机替换成一个掩码</strong><code>[MASK]</code>。</p>
<p>（当然，对于<code>[CLS]</code>和<code>[SEP]</code>就不做替换了，这属于功能性的）</p>
<p>整体来看，如果输入序列是1000的话，那就要预测大约150个词。</p>
<p>当然，这一过程是MLM预训练独有的，在微调的过程中是看不到这一个过程的，更看不到<code>[MASK]</code>。</p>
<p>但是，这个<strong>替换</strong>的过程，也不是100%替换成<code>[MASK]</code>的，具体来说是这样的方案：</p>
<ul>
<li>80%：替换为<code>[MASK]</code></li>
<li>10%：替换为一个随机的词元</li>
<li>10%：什么都不做，但是用来做预测</li>
</ul>
<blockquote>
<p>附录的一个例子：</p>
</blockquote>
<img src="https://cdnjson.com/images/2023/05/20/imagec7866559f6d03693.png" alt="avatar" style="zoom: 80%;" />

<h3 id="句子匹配："><a href="#句子匹配：" class="headerlink" title="句子匹配："></a>句子匹配：</h3><p>无论是在QA还是语言推理层面，都是一个句子对，所以最好让其学习一些句子层面的信息。</p>
<p>对于句子A和B，有可能是相邻，也有可能不是，那么相邻的就是正例，不相邻的就是负例，各占50%。</p>
<blockquote>
<p>附录的一个例子，简单读一下Input和Label就明白了：</p>
</blockquote>
<img src="https://cdnjson.com/images/2023/05/20/imagedb613b6793064687.png" alt="avatar" style="zoom: 80%;" />

<p>值得一提的是，可以看到有一个<code>##</code>，这个意思是说，flightless这个词在原文中不常见，所以砍成了两个词，<code>##</code>的作用是表明这原本是一个词。</p>
<blockquote>
<p>两种不同策略的预训练精度结果（MLM和LR）：</p>
</blockquote>
<img src="https://cdnjson.com/images/2023/05/20/image4c8380af809bea06.png" alt="avatar" style="zoom:67%;" />

<hr>
<h2 id="再看微调："><a href="#再看微调：" class="headerlink" title="再看微调："></a>再看微调：</h2><p>由于选择了单一个编码器的架构，而不是像传统的Transformer一样有编码器和解码器，所以会有一些缺点：</p>
<p>与GPT比，BERT用的是编码器，GPT用的是解码器。BERT做机器翻译、文本的摘要，也就是生成类的任务并不好做。</p>
<p>不过事实上，分类问题在NLP中更常见。</p>
<blockquote>
<p>如下图是BERT在分类任务上的表现情况：</p>
</blockquote>
<p><img src="https://cdnjson.com/images/2023/05/20/image20c21b2fae29114d.png" alt="avatar"></p>
<hr>
<h1 id="五、结论与尾声"><a href="#五、结论与尾声" class="headerlink" title="五、结论与尾声:"></a>五、结论与尾声:</h1><p>在本文发布的当时，语言模型迁移学习的经验改进表明，丰富的<strong>无监督预训练</strong>是许多<strong>语言理解系统</strong>的组成部分，这个效果是非常好的。</p>
<p>使得训练集比较少的任务也能够使用深度神经网络，获得比较好的效果。</p>
<p>特别是，这些结果使<strong>低资源任务</strong>也能从深度单向架构中受益。BERT的主要贡献是进一步将这些发现<strong>推广到深度双向架构</strong>，允许相同的预训练模型成功地处理<strong>广泛的</strong>NLP任务。</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://conqueror712.github.io">落雨乄天珀夜</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://conqueror712.github.io/post/Paper-BERT.html">https://conqueror712.github.io/post/Paper-BERT.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://conqueror712.github.io" target="_blank">落雨乄天珀夜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a82ec344c5e746a789f5a5f10577397a~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/post/Computer-Network-4-2.html"><img class="next-cover" src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e26cc82ed8b74dfbbad55d94b885444e~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" onerror="onerror=null;src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计网 - 路由算法丨学习记录</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2023/02/15/LprEbGXq9xgfdDn.jpg" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="avatar"/></div><div class="author-info__name">落雨乄天珀夜</div><div class="author-info__description">「落花人独立丨微雨燕双飞」</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Conqueror712"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog ~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9B%B6%E3%80%81%E5%89%8D%E8%A8%80%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">零、前言：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81BERT%E7%AE%80%E4%BB%8B%EF%BC%9A"><span class="toc-number">2.</span> <span class="toc-text">一、BERT简介：</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E5%8F%A5%E8%AF%9D%E8%A7%A3%E9%87%8ABERT%EF%BC%9A"><span class="toc-number">2.1.</span> <span class="toc-text">一句话解释BERT：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT%E5%87%BA%E7%8E%B0%E7%9A%84%E7%9B%AE%E7%9A%84%E5%92%8C%E6%84%8F%E4%B9%89%EF%BC%9A"><span class="toc-number">2.2.</span> <span class="toc-text">BERT出现的目的和意义：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81BERT%E4%B8%8EGPT%E3%80%81ELMo%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">3.</span> <span class="toc-text">二、BERT与GPT、ELMo的关系</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT%E4%B8%8EGPT%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9A"><span class="toc-number">3.1.</span> <span class="toc-text">BERT与GPT的区别：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT%E4%B8%8EELMo%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9A"><span class="toc-number">3.2.</span> <span class="toc-text">BERT与ELMo的区别：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81BERT%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">4.</span> <span class="toc-text">三、BERT预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E5%9C%A8NLP%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%9A"><span class="toc-number">4.1.</span> <span class="toc-text">预训练在NLP的应用：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NLP%E4%B8%AD%E4%B8%A4%E7%A7%8D%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E7%AD%96%E7%95%A5%EF%BC%9A"><span class="toc-number">4.2.</span> <span class="toc-text">NLP中两种预训练的策略：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MLM%E2%80%94%E2%80%94%E6%9D%A5%E8%87%AA%E8%BF%9C%E5%8F%A4%E7%9A%84%E7%A7%98%E5%AE%9D%EF%BC%9A"><span class="toc-number">4.3.</span> <span class="toc-text">MLM——来自远古的秘宝：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%89%E6%A0%87%E6%B3%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%80%E5%AE%9A%E6%9B%B4%E5%A5%BD%E5%90%97%EF%BC%9A"><span class="toc-number">4.4.</span> <span class="toc-text">有标注的数据集一定更好吗：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83%EF%BC%9A"><span class="toc-number">4.5.</span> <span class="toc-text">BERT预训练与微调：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83Pre-Training%EF%BC%9A"><span class="toc-number">4.5.1.</span> <span class="toc-text">预训练Pre-Training：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83Fine-Tuning%EF%BC%9A"><span class="toc-number">4.5.2.</span> <span class="toc-text">微调Fine-Tuning：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%B7%B1%E5%85%A5BERT%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">四、深入BERT模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%EF%BC%9A"><span class="toc-number">5.1.</span> <span class="toc-text">BERT模型架构：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT%E6%A8%A1%E5%9E%8B%E5%8F%AF%E5%AD%A6%E4%B9%A0%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97%EF%BC%9A"><span class="toc-number">5.2.</span> <span class="toc-text">BERT模型可学习参数计算：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%EF%BC%9A"><span class="toc-number">5.3.</span> <span class="toc-text">BERT输入输出：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%87%E8%AF%8D%E6%96%B9%E6%B3%95%EF%BC%9AWordPiece"><span class="toc-number">5.3.1.</span> <span class="toc-text">切词方法：WordPiece</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E5%8F%A5%E5%AD%90%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="toc-number">5.3.2.</span> <span class="toc-text">合并句子的方法：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MLM%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E7%BB%86%E8%8A%82%EF%BC%9A"><span class="toc-number">5.4.</span> <span class="toc-text">MLM预训练的细节：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E5%BD%A2%E5%A1%AB%E7%A9%BA%EF%BC%9A"><span class="toc-number">5.4.1.</span> <span class="toc-text">完形填空：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%A5%E5%AD%90%E5%8C%B9%E9%85%8D%EF%BC%9A"><span class="toc-number">5.4.2.</span> <span class="toc-text">句子匹配：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%8D%E7%9C%8B%E5%BE%AE%E8%B0%83%EF%BC%9A"><span class="toc-number">5.5.</span> <span class="toc-text">再看微调：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E7%BB%93%E8%AE%BA%E4%B8%8E%E5%B0%BE%E5%A3%B0"><span class="toc-number">6.</span> <span class="toc-text">五、结论与尾声:</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/Paper-BERT.html" title="2023了，再来看看NLP经典之作 - BERT丨论文解读"><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a82ec344c5e746a789f5a5f10577397a~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="2023了，再来看看NLP经典之作 - BERT丨论文解读"/></a><div class="content"><a class="title" href="/post/Paper-BERT.html" title="2023了，再来看看NLP经典之作 - BERT丨论文解读">2023了，再来看看NLP经典之作 - BERT丨论文解读</a><time datetime="2023-05-20T13:48:01.000Z" title="发表于 2023-05-20 21:48:01">2023-05-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/Computer-Network-4-2.html" title="计网 - 路由算法丨学习记录"><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e26cc82ed8b74dfbbad55d94b885444e~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="计网 - 路由算法丨学习记录"/></a><div class="content"><a class="title" href="/post/Computer-Network-4-2.html" title="计网 - 路由算法丨学习记录">计网 - 路由算法丨学习记录</a><time datetime="2023-05-15T11:19:06.000Z" title="发表于 2023-05-15 19:19:06">2023-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/Computer-Network-4-1.html" title="计网 - 网络层的功能丨学习记录"><img src="https://cdnjson.com/images/2023/05/14/wallhaven-7pw383.jpg" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="计网 - 网络层的功能丨学习记录"/></a><div class="content"><a class="title" href="/post/Computer-Network-4-1.html" title="计网 - 网络层的功能丨学习记录">计网 - 网络层的功能丨学习记录</a><time datetime="2023-05-14T02:57:03.000Z" title="发表于 2023-05-14 10:57:03">2023-05-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/MLC-LLM.html" title="MLC-LLM教程丨无需科技，浏览器就能用的大模型"><img src="https://cdnjson.com/images/2023/05/01/wallhaven-kx1ez7.png" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="MLC-LLM教程丨无需科技，浏览器就能用的大模型"/></a><div class="content"><a class="title" href="/post/MLC-LLM.html" title="MLC-LLM教程丨无需科技，浏览器就能用的大模型">MLC-LLM教程丨无需科技，浏览器就能用的大模型</a><time datetime="2023-05-01T12:14:34.000Z" title="发表于 2023-05-01 20:14:34">2023-05-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/Interview-BUPT.html" title="面经丨BUPT第三届模拟面试大赛"><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9906557577504b17a327378fc9834891~tplv-k3u1fbpfcp-watermark.image?" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="面经丨BUPT第三届模拟面试大赛"/></a><div class="content"><a class="title" href="/post/Interview-BUPT.html" title="面经丨BUPT第三届模拟面试大赛">面经丨BUPT第三届模拟面试大赛</a><time datetime="2023-04-26T15:32:52.000Z" title="发表于 2023-04-26 23:32:52">2023-04-26</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a82ec344c5e746a789f5a5f10577397a~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 落雨乄天珀夜</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>