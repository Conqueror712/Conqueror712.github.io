<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>DeepLearning - Part1 - 深度学习基础丨学习记录 | 落雨乄天珀夜</title><meta name="author" content="落雨乄天珀夜"><meta name="copyright" content="落雨乄天珀夜"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="00-Abstract本文介绍了笔者在学习《动手学深度学习》课程中的笔记。 这一Part的内容是深度学习基础。 后续将会补充更多的公式推导和原理部分。 敬请期待！  01-Guide TopicThe most import thing of deep learning is Neural Network. The Neural Network is a language, it is very">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepLearning - Part1 - 深度学习基础丨学习记录">
<meta property="og:url" content="https://conqueror712.github.io/post/DeepLearningNote.html">
<meta property="og:site_name" content="落雨乄天珀夜">
<meta property="og:description" content="00-Abstract本文介绍了笔者在学习《动手学深度学习》课程中的笔记。 这一Part的内容是深度学习基础。 后续将会补充更多的公式推导和原理部分。 敬请期待！  01-Guide TopicThe most import thing of deep learning is Neural Network. The Neural Network is a language, it is very">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://conqueror712.github.io/img/NEW/5AXIB$]U4@VZM5%60207WRQP2.jpg">
<meta property="article:published_time" content="2023-01-01T13:32:11.000Z">
<meta property="article:modified_time" content="2023-02-11T02:55:37.285Z">
<meta property="article:author" content="落雨乄天珀夜">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://conqueror712.github.io/img/NEW/5AXIB$]U4@VZM5%60207WRQP2.jpg"><link rel="shortcut icon" href="/img/head.jpg"><link rel="canonical" href="https://conqueror712.github.io/post/DeepLearningNote"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'DeepLearning - Part1 - 深度学习基础丨学习记录',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-11 10:55:37'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="落雨乄天珀夜" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/NEW/5AXIB$%5DU4@VZM5%60207WRQP2.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">落雨乄天珀夜</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">DeepLearning - Part1 - 深度学习基础丨学习记录</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-01T13:32:11.000Z" title="发表于 2023-01-01 21:32:11">2023-01-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-11T02:55:37.285Z" title="更新于 2023-02-11 10:55:37">2023-02-11</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="DeepLearning - Part1 - 深度学习基础丨学习记录"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="00-Abstract"><a href="#00-Abstract" class="headerlink" title="00-Abstract"></a>00-Abstract</h1><p>本文介绍了笔者在学习《动手学深度学习》课程中的笔记。</p>
<p>这一Part的内容是深度学习基础。</p>
<p>后续将会补充更多的公式推导和原理部分。</p>
<p>敬请期待！</p>
<hr>
<h1 id="01-Guide-Topic"><a href="#01-Guide-Topic" class="headerlink" title="01-Guide Topic"></a>01-Guide Topic</h1><p>The most import thing of deep learning is <strong>Neural Network</strong>.</p>
<p>The Neural Network is a <strong>language</strong>, it is very <strong>flexible</strong>.</p>
<p>AutoGluon⭐</p>
<p>Book Version 1: <a target="_blank" rel="noopener" href="https://zh/d2.ai/">https://zh/d2.ai/</a></p>
<p>Book Version 2: <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/">https://zh-v2.d2l.ai/</a></p>
<p>Source Code: <a target="_blank" rel="noopener" href="https://github.com./d2l-ai/d2l-zh">https://github.com./d2l-ai/d2l-zh</a></p>
<p>Course Web: <a target="_blank" rel="noopener" href="https://courses.d2l.ai/zh-v2">https://courses.d2l.ai/zh-v2</a></p>
<p>Discuss: <a target="_blank" rel="noopener" href="https://discuss.pytorch.org/">https://discuss.pytorch.org/</a> <a target="_blank" rel="noopener" href="https://discuss.d2l.ai/c/16">https://discuss.d2l.ai/c/16</a></p>
<h2 id="Introduce-DeepLearning"><a href="#Introduce-DeepLearning" class="headerlink" title="Introduce DeepLearning"></a>Introduce DeepLearning</h2><p>The target of this course: </p>
<ul>
<li>introduce the classic and new model like LeNet, ResNet, LSTM, BERT,…</li>
<li>Basic Machine Learning</li>
<li>Practice</li>
</ul>
<h2 id="The-Application-of-DL"><a href="#The-Application-of-DL" class="headerlink" title="The  Application of DL"></a>The  Application of DL</h2><p>图片分类：2012年深度学习开始之后错误率迅速降低，目前大约5%</p>
<p><a target="_blank" rel="noopener" href="https://qz.com/1034972/the-data-that-changed-thedirection-of-ai-research-and-possibly-the-world/">https://qz.com/1034972/the-data-that-changed-thedirection-of-ai-research-and-possibly-the-world/</a></p>
<p>物体检测和分割</p>
<p>样式迁移：可以初步理解为，加滤镜</p>
<p>人脸合成</p>
<p>文字生成图片<a target="_blank" rel="noopener" href="https://openai.com/blog/dall-e/">DALL·E: Creating Images from Text (openai.com)</a></p>
<p>文字生成</p>
<p>无人驾驶</p>
<p><em>广告推荐的预测与训练步骤：</em>特征提取→模型→点击率预测→训练数据→特征和用户点击→模型</p>
<p>模型的可解释性：XAI，DL上做的还不太好，ML上还行</p>
<h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><p>流程如下：</p>
<ol>
<li><p><code>ssh ubuntu@&lt;IP&gt;</code>连接云服务器，如果直接能在服务器的terminal上操作也可以</p>
</li>
<li><p><code>sudo apt update</code>更新一下机器，因为一开始就是一个裸的ubuntu啥也没有</p>
</li>
<li><p><code>sudo apt install build-essential</code>装一些开发环境，例如gcc</p>
</li>
<li><p><code>sudo apt install python3.8</code>安装python</p>
</li>
<li><p>miniconda安装，去官网复制对应版本和需求的链接，下载：<code>wget &lt;link&gt;</code> → 安装：<code>bash Miniconda3-latest-Linux-x64_64.sh（根据自己的文件进行替换）</code>默认装在根目录下面就可以</p>
</li>
<li><p>进入conda环境：<code>bash</code>（直接在terminal里打bash）之后可以直接在base环境里来用，也可以新创建一个环境。 </p>
</li>
<li><p>内部环境的安装：<code>pip install jupyter d2l torch torchvision</code>（本地记得换源）</p>
</li>
<li><p>课程记事本的下载：<code>http://zh-v2.d2l.ai</code>→<code>在Jupyter记事本文件这里复制链接</code>→<code>wget &lt;link&gt;</code></p>
</li>
<li><p>如果服务器里没有zip，则需要安装一个zip<code>sudo spt install zip</code>，之后<code>ls</code>可以显示一下名称，然后再进行解压即可<code>unzip d2l-zh.zip</code></p>
<p>之后再<code>ls</code>可以看到解压出来了三个文件夹<code>mxnet pytorch tensorflow</code></p>
<p>进入pytorch的文件夹查看有什么东西<code>cd pytorch/``ls</code></p>
</li>
<li><p>PPT的下载可以直接<code>git clone &lt;link&gt;</code>（链接在github里，是仓库）</p>
</li>
<li><p>运行jupyter<code>jupyter notebook</code>（会出现一个链接，需要将远端的<code>link</code>map到本地<code>ssh -L8888:localhost:8888 ubuntu@&lt;IP&gt;</code>）（这里的8888是远程的端口）</p>
<p>值得一提的是，打开jupyter之后笔记有幻灯片格式，还有非幻灯片格式，若想打开幻灯片格式则需要安装一个插件<code>pip install rise</code></p>
</li>
</ol>
<hr>
<h1 id="02-Pre-Knowledge"><a href="#02-Pre-Knowledge" class="headerlink" title="02-Pre Knowledge"></a>02-Pre Knowledge</h1><h2 id="Data-Operation"><a href="#Data-Operation" class="headerlink" title="Data Operation"></a>Data Operation</h2><p>机器学习中用到的最多的数据结构：N维数组</p>
<p>访问元素的几种常用方法：</p>
<ul>
<li>一个元素<code>[1, 2]</code></li>
<li>一行<code>[1, :]</code></li>
<li>子区域<code>[1:3, 1:]</code></li>
<li>子区域<code>[::3, ::2]</code></li>
</ul>
<p>除此之外，还可以通过<code>-1</code>来访问最后一个元素</p>
<h3 id="Tensor张量"><a href="#Tensor张量" class="headerlink" title="Tensor张量"></a>Tensor张量</h3><p>张量表示一个数值组成的数组，这个数组可能有多个维度。</p>
<p><code>x = torch.arange(12)</code></p>
<p>我们可以通过张量的<code>shape</code>属性来访问张量的形状和<strong>张量</strong>中<strong>元素的总数</strong><br><code>x.shape</code></p>
<p><code>x.numel</code></p>
<p>要改变一个张量的形状而不改变元素的数量和数值，我们可以调用<code>reshape</code>函数</p>
<p><code>X = x.reshape(3, 4)</code></p>
<p>使用全0，全1或其他常量或者从特定分布中随机采样的数字</p>
<p><code>torch.zeros((2, 3, 4))</code></p>
<p><code>torch.ones((2, 3, 4))</code></p>
<p>也可以直接在定义的时候就赋好初始值，这里不过多赘述；</p>
<p>常见的四则运算等都可以被升级为按元素运算；</p>
<p>把多个张量连接在一起</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>), torch.cat((X, Y), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>通过逻辑运算符构建二元张量；</p>
<p>对张量中所有元素求和会获得一个只有一个元素的张量<code>X.sum()</code>；</p>
<p>即使形状不同，我们仍然可以通过调用<strong>广播机制</strong>来执行按元素操作； </p>
<p>还有一些赋值的方法：<code>X[0:2, :]  = 12</code>；</p>
<p>运行一些操作可能会导致为新结果分配内存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">before = <span class="built_in">id</span>(Y) <span class="comment"># 在Python中，id()类似于指针的意思</span></span><br><span class="line">Y = Y + X</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(Y) == before)</span><br><span class="line"><span class="comment"># output = False</span></span><br></pre></td></tr></table></figure>

<p>执行原地内存操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">new_Y = torch.zeros_like(Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;id(new_Y):&quot;</span>, <span class="built_in">id</span>(new_Y))</span><br><span class="line">new_Y[:] = X + Y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;id(new_Y):&quot;</span>, <span class="built_in">id</span>(new_Y))</span><br></pre></td></tr></table></figure>

<p>转换为NumPy张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(A), <span class="built_in">type</span>(B)) <span class="comment"># 查看数据类型</span></span><br><span class="line"><span class="comment"># 将大小为1的张量转换为Python标量：</span></span><br><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br><span class="line"><span class="comment"># output = (tensor([3.5000]), 3.5, 3.5, 3)</span></span><br></pre></td></tr></table></figure>



<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>作用：读取csv文件→做一定的特征预处理→变成pytorch能用的一个tenser(张量)</p>
<p>创建一个人工数据集，并存储在csv（逗号分隔值）文件；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)  <span class="comment"># 列名</span></span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)  <span class="comment"># 每行表示一个数据样本</span></span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>从创建的csv文件中加载原始数据集；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="comment"># 事实上这里也可直接写成data，不使用print，会保留HTML格式</span></span><br></pre></td></tr></table></figure>

<p>处理缺失的数据，典型的方法是<strong>插值</strong>和<strong>删除</strong>，代码就不放了，可以直接看<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_preliminaries/pandas.html">2.2. 数据预处理 — 动手学深度学习 2.0.0-beta1 documentation (d2l.ai)</a></p>
<hr>
<h2 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a>Linear Algebra</h2><h3 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h3><p>c &#x3D; A * b  hence  ||c|| &lt;&#x3D; ||A|| * ||b||</p>
<p>取决于如何衡量b和c的长度</p>
<p>常见范数：</p>
<ul>
<li>矩阵范数：最小的满足上面公式的值</li>
<li>Frobenius范数（具体公式不给出了）</li>
</ul>
<p>正定矩阵：如果一个矩阵是正定的，那么它乘上任何一个行向量或者列向量之后值都是大于等于0的；</p>
<h3 id="哈达玛积"><a href="#哈达玛积" class="headerlink" title="哈达玛积"></a>哈达玛积</h3><p>两个矩阵按元素乘法</p>
<h3 id="亚导数"><a href="#亚导数" class="headerlink" title="亚导数"></a>亚导数</h3><p>将导数拓展到不可微的函数</p>
<h3 id="向量链式法则"><a href="#向量链式法则" class="headerlink" title="向量链式法则"></a>向量链式法则</h3><p>略</p>
<h3 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h3><p>自动求导计算一个函数在指定值上的导数</p>
<p>有别于符号求导和数值求导</p>
<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>将代码分解成操作子</p>
<p>将计算表示成一个无环图</p>
<p>（显式构造、隐式构造）</p>
<hr>
<h2 id="Matrix-Calculation"><a href="#Matrix-Calculation" class="headerlink" title="Matrix Calculation"></a>Matrix Calculation</h2><h3 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h3><ul>
<li><p>标量导数（回忆一下即可）</p>
</li>
<li><p>亚导数（将导数拓展到不可微的函数）</p>
</li>
<li><p>向量的导数（注意方向是如何变化的）<strong>（梯度与等高线正交，指向值变化最大的方向）</strong></p>
<img src="https://s2.loli.net/2023/01/01/tp45niawvJU9qlo.png" alt="avatar" style="zoom:50%;" />
</li>
<li><p>拓展到矩阵以及更高维度的张量</p>
<p><img src="https://s2.loli.net/2023/01/01/ZqCuXydNbRKawoQ.png" alt="avatar"></p>
</li>
</ul>
<p>机器学习不关心P问题，只关心NP问题，所以一般不会处理凸函数问题，因为凸函数问题显然可以很容易得到最优解。（下面补充一下P与NP的内容好了）</p>
<h3 id="P与NP问题"><a href="#P与NP问题" class="headerlink" title="P与NP问题"></a>P与NP问题</h3><blockquote>
<p>**<em>P类问题*<strong>：所有可以在<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%97%B6%E9%97%B4?fromModule=lemma_inlink">多项式时间</a>内求解的判定问题构成P类问题。</strong></em>判定问题***<em>：</em>判断是否有一种能够解决某一类问题的能行算法的研究课题。</p>
<p>**<em>NP类问题*<strong>：所有的非确定性多项式时间可解的判定问题构成NP类问题。</strong></em>非确定性算法***：非确定性算法将问题分解成猜测和验证两个阶段。算法的猜测阶段是非确定性的，算法的验证阶段是确定性的，它验证猜测阶段给出解的正确性。设算法A是解一个判定问题Q的非确定性算法，如果A的验证阶段能在多项式时间内完成，则称A是一个多项式时间非确定性算法。有些计算问题是确定性的，例如加减乘除，只要按照公式推导，按部就班一步步来，就可以得到结果。但是，有些问题是无法按部就班直接地计算出来。比如，找大<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%B4%A8%E6%95%B0?fromModule=lemma_inlink">质数</a>的问题。有没有一个公式能推出下一个<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%B4%A8%E6%95%B0?fromModule=lemma_inlink">质数</a>是多少呢？这种问题的答案，是无法直接计算得到的，只能通过间接的“猜算”来得到结果。这也就是非确定性问题。而这些问题的通常有个算法，它不能直接告诉你答案是什么，但可以告诉你，某个可能的结果是正确的答案还是错误的。这个可以告诉你“猜算”的答案正确与否的算法，假如可以在多项式（polynomial）时间内算出来，就叫做多项式非确定性问题。</p>
<p>***NPC问题***<strong>：</strong>NP中的某些问题的复杂性与整个类的复杂性相关联.这些问题中任何一个如果存在多项式时间的算法,那么所有NP问题都是多项式时间可解的.这些问题被称为NP-完全问题(NPC问题)。</p>
</blockquote>
<hr>
<h2 id="自动求导-1"><a href="#自动求导-1" class="headerlink" title="自动求导"></a>自动求导</h2><h3 id="向量链式法则-1"><a href="#向量链式法则-1" class="headerlink" title="向量链式法则"></a>向量链式法则</h3><p><img src="https://s2.loli.net/2023/01/01/g5DQzkZC49jmqlF.png" alt="avatar"></p>
<p>接下来我们看一个例子：</p>
<p><img src="https://s2.loli.net/2023/01/01/4aYqp7nE9HCgDW6.png" alt="avatar"></p>
<h3 id="不同的求导："><a href="#不同的求导：" class="headerlink" title="不同的求导："></a>不同的求导：</h3><ul>
<li>自动求导：计算一个函数在指定值上的导数</li>
<li>符号求导：显式的一个求导</li>
<li>数值求导：用数值来拟合，不需要知道具体的表达式</li>
</ul>
<h3 id="计算图："><a href="#计算图：" class="headerlink" title="计算图："></a>计算图：</h3><p>计算图是Pytorch内部自动计算导数的一个方式；</p>
<p>其实等同于用链式法则求导的一个过程，有点像同路相乘异路相加；</p>
<ul>
<li>将代码分解成操作子</li>
<li>将计算表示成一个有向无环图DAG</li>
</ul>
<p>构造计算图有显式构造和隐式构造两种方式，数学上一般是显示，Pytorch是隐式；</p>
<h3 id="自动求导的模式："><a href="#自动求导的模式：" class="headerlink" title="自动求导的模式："></a>自动求导的模式：</h3><p><img src="https://s2.loli.net/2023/01/01/QfPDEl9yCiHLMVt.png" alt="avatar"></p>
<p><img src="https://s2.loli.net/2023/01/01/FS3pXUNqrdWjEBO.png" alt="avatar"></p>
<p><strong>其实，反向传播就是人们用手来算复合函数求导的过程，是一样的。</strong></p>
<p>demo代码如下<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_preliminaries/autograd.html#id2">2.5. 自动微分 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p>
<hr>
<h1 id="03-Linear-NeuralNetwork"><a href="#03-Linear-NeuralNetwork" class="headerlink" title="03-Linear NeuralNetwork"></a>03-Linear NeuralNetwork</h1><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>经典的买房例子，成交价是关键因素的加权和：</p>
<p><code>y = w1x1 + w2x2 + w3x3 + b</code></p>
<p>权重和偏差的实际值在后面决定；</p>
<p><img src="https://s2.loli.net/2023/01/02/5hOwMlWQ28zT7fX.png" alt="avatar"></p>
<p>正因如此，线性模型可以看作是单层的神经网络</p>
<p>之所以叫单层的神经网络，是因为其带权的层就是1；</p>
<h3 id="衡量预估质量（平方损失）"><a href="#衡量预估质量（平方损失）" class="headerlink" title="衡量预估质量（平方损失）"></a>衡量预估质量（平方损失）</h3><p><img src="https://s2.loli.net/2023/01/02/y3MvRgoASFd4PeT.png" alt="avatar"></p>
<h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><p>收集一些数据点来决定参数值（权重和偏差），被称之为<strong>训练数据</strong>，越多越好。</p>
<h3 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h3><p><img src="https://s2.loli.net/2023/01/02/2N8chwA7xUyifPt.png" alt="avatar"></p>
<h3 id="显式解"><a href="#显式解" class="headerlink" title="显式解"></a>显式解</h3><p><img src="https://s2.loli.net/2023/01/02/1fpJQ6UCFSZu2VT.png" alt="avatar"></p>
<p>注意 这里导数的最后应该有一个负号；</p>
<p>总的来说：</p>
<ul>
<li>线性回归是对n维输入的加权和，外加偏差</li>
<li>使用平方损失来衡量预测值和真实值的差异</li>
<li>线性回归有显式解</li>
<li>线性回归可以看作是单层的神经网络</li>
</ul>
<h2 id="基础优化方法"><a href="#基础优化方法" class="headerlink" title="基础优化方法"></a>基础优化方法</h2><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><img src="https://s2.loli.net/2023/01/02/2rP5oYbRD43aZgd.png" alt="avatar"></p>
<p>（后面那个导数就是梯度，η就是学习率）</p>
<p>梯度的负方向就是函数值下降最快的方向；</p>
<p><strong>超参数</strong>不需要数据来驱动，而是在训练前或者训练中认为的进行调整的参数；</p>
<p>学习率不能太小（计算梯度太贵了），也不能太大； </p>
<h3 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h3><p><img src="https://s2.loli.net/2023/01/02/ZeTlzFEkm7pKnrN.png" alt="avatar"></p>
<p><code>batch_size</code></p>
<p>总结</p>
<ul>
<li>梯度下降通过不断延着反梯度方向更新参数求解</li>
<li>小批量随机梯度下降是深度学习默认的求解算法</li>
<li>两个重要的超参数是批量大小和学习率</li>
</ul>
<p>代码参考：</p>
<p><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression-scratch.html">3.2. 线性回归的从零开始实现 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p>
<hr>
<h2 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h2><h3 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h3><p>虽然叫做回归，但其实是一个分类问题；</p>
<ul>
<li>回归估计一个连续值</li>
<li>分类预测一个离散类别</li>
</ul>
<p><img src="https://s2.loli.net/2023/01/03/ZQR7a84eKEUn9TY.png" alt="avatar"></p>
<p><img src="https://s2.loli.net/2023/01/03/GANJgOwtWCTkeUq.png" alt="avatar"></p>
<p>衡量两个概率的区别——交叉熵</p>
<p><img src="https://s2.loli.net/2023/01/03/BNYKFAjx7Vu4Rsb.png" alt="avatar"></p>
<p><strong>划重点： 对于分类问题， 我们不关心非正确类的预测值，我们只关心对于正确类的置信度是否够大。</strong></p>
<p>顺带一提 log的默认底数好像是2</p>
<p>总结：</p>
<ul>
<li>Softmax回归是一个多类分类模型</li>
<li>使用Softmax操作子得到每个类的预测置信度</li>
<li>使用交叉熵来衡量预测和标号的区别</li>
</ul>
<hr>
<h3 id="损失函数Loss-function"><a href="#损失函数Loss-function" class="headerlink" title="损失函数Loss-function"></a>损失函数Loss-function</h3><p>用来衡量预测值和真实值之间的区别；</p>
<p><img src="https://s2.loli.net/2023/01/03/ShcnVsvTCi1QwWY.png" alt="avatar"></p>
<p>下面介绍几种常见常用的损失函数：</p>
<h3 id="均方损失L2-Loss"><a href="#均方损失L2-Loss" class="headerlink" title="均方损失L2 Loss"></a>均方损失L2 Loss</h3><p><img src="https://s2.loli.net/2023/01/03/2cBkAS7meW6CvsO.png" alt="avatar"></p>
<ul>
<li>蓝色的线代表y &#x3D; 0的时候y’的函数，显然是一个二次函数</li>
<li>绿色线是一个标准正态分布</li>
</ul>
<hr>
<h1 id="04-Multilayer-Perceptron"><a href="#04-Multilayer-Perceptron" class="headerlink" title="04-Multilayer Perceptron"></a>04-Multilayer Perceptron</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>感知机由两层神经元组成</p>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><hr>
<h2 id="Model-Select"><a href="#Model-Select" class="headerlink" title="Model Select"></a>Model Select</h2><h3 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h3><hr>
<h2 id="Weight-Decay"><a href="#Weight-Decay" class="headerlink" title="Weight Decay"></a>Weight Decay</h2><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><hr>
<h2 id="Numerical-Stability"><a href="#Numerical-Stability" class="headerlink" title="Numerical Stability"></a>Numerical Stability</h2><h3 id="模型初始化与激活函数"><a href="#模型初始化与激活函数" class="headerlink" title="模型初始化与激活函数"></a>模型初始化与激活函数</h3><hr>
<p><em>预告：下一Part是卷积神经网络</em></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://conqueror712.github.io">落雨乄天珀夜</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://conqueror712.github.io/post/DeepLearningNote.html">https://conqueror712.github.io/post/DeepLearningNote.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://conqueror712.github.io" target="_blank">落雨乄天珀夜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/NEW/5AXIB$%5DU4@VZM5%60207WRQP2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/ServerLearningNote.html"><img class="prev-cover" src="/img/NEW/0%5DV4D7564YZ_CCEKZFHYDWN.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">CloudServer丨学习记录</div></div></a></div><div class="next-post pull-right"><a href="/post/MySQL.html"><img class="next-cover" src="/img/705445.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">MySQL入门丨学习记录</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">落雨乄天珀夜</div><div class="author-info__description">「落花人独立丨微雨燕双飞」</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Conqueror712"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog ~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#00-Abstract"><span class="toc-number">1.</span> <span class="toc-text">00-Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#01-Guide-Topic"><span class="toc-number">2.</span> <span class="toc-text">01-Guide Topic</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduce-DeepLearning"><span class="toc-number">2.1.</span> <span class="toc-text">Introduce DeepLearning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Application-of-DL"><span class="toc-number">2.2.</span> <span class="toc-text">The  Application of DL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Install"><span class="toc-number">2.3.</span> <span class="toc-text">Install</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#02-Pre-Knowledge"><span class="toc-number">3.</span> <span class="toc-text">02-Pre Knowledge</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-Operation"><span class="toc-number">3.1.</span> <span class="toc-text">Data Operation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor%E5%BC%A0%E9%87%8F"><span class="toc-number">3.1.1.</span> <span class="toc-text">Tensor张量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">3.2.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Algebra"><span class="toc-number">3.3.</span> <span class="toc-text">Linear Algebra</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8C%83%E6%95%B0"><span class="toc-number">3.3.1.</span> <span class="toc-text">范数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%93%88%E8%BE%BE%E7%8E%9B%E7%A7%AF"><span class="toc-number">3.3.2.</span> <span class="toc-text">哈达玛积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%9A%E5%AF%BC%E6%95%B0"><span class="toc-number">3.3.3.</span> <span class="toc-text">亚导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-number">3.3.4.</span> <span class="toc-text">向量链式法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="toc-number">3.3.5.</span> <span class="toc-text">自动求导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">3.3.6.</span> <span class="toc-text">计算图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Matrix-Calculation"><span class="toc-number">3.4.</span> <span class="toc-text">Matrix Calculation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0"><span class="toc-number">3.4.1.</span> <span class="toc-text">导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#P%E4%B8%8ENP%E9%97%AE%E9%A2%98"><span class="toc-number">3.4.2.</span> <span class="toc-text">P与NP问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC-1"><span class="toc-number">3.5.</span> <span class="toc-text">自动求导</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99-1"><span class="toc-number">3.5.1.</span> <span class="toc-text">向量链式法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%9A%84%E6%B1%82%E5%AF%BC%EF%BC%9A"><span class="toc-number">3.5.2.</span> <span class="toc-text">不同的求导：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%9A"><span class="toc-number">3.5.3.</span> <span class="toc-text">计算图：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E7%9A%84%E6%A8%A1%E5%BC%8F%EF%BC%9A"><span class="toc-number">3.5.4.</span> <span class="toc-text">自动求导的模式：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#03-Linear-NeuralNetwork"><span class="toc-number">4.</span> <span class="toc-text">03-Linear NeuralNetwork</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression"><span class="toc-number">4.1.</span> <span class="toc-text">Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">4.1.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A1%E9%87%8F%E9%A2%84%E4%BC%B0%E8%B4%A8%E9%87%8F%EF%BC%88%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1%EF%BC%89"><span class="toc-number">4.1.2.</span> <span class="toc-text">衡量预估质量（平方损失）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">4.1.3.</span> <span class="toc-text">训练数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.1.4.</span> <span class="toc-text">参数学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%98%BE%E5%BC%8F%E8%A7%A3"><span class="toc-number">4.1.5.</span> <span class="toc-text">显式解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">基础优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.2.1.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.2.2.</span> <span class="toc-text">小批量随机梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax-Regression"><span class="toc-number">4.3.</span> <span class="toc-text">Softmax Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax%E5%9B%9E%E5%BD%92"><span class="toc-number">4.3.1.</span> <span class="toc-text">Softmax回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0Loss-function"><span class="toc-number">4.3.2.</span> <span class="toc-text">损失函数Loss-function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E6%8D%9F%E5%A4%B1L2-Loss"><span class="toc-number">4.3.3.</span> <span class="toc-text">均方损失L2 Loss</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#04-Multilayer-Perceptron"><span class="toc-number">5.</span> <span class="toc-text">04-Multilayer Perceptron</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">5.1.</span> <span class="toc-text">感知机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">5.2.</span> <span class="toc-text">多层感知机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Select"><span class="toc-number">5.3.</span> <span class="toc-text">Model Select</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">5.3.1.</span> <span class="toc-text">欠拟合与过拟合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Weight-Decay"><span class="toc-number">5.4.</span> <span class="toc-text">Weight Decay</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout"><span class="toc-number">5.4.1.</span> <span class="toc-text">Dropout</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Numerical-Stability"><span class="toc-number">5.5.</span> <span class="toc-text">Numerical Stability</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">5.5.1.</span> <span class="toc-text">模型初始化与激活函数</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/Computer-Network.html" title="计算机网络 - 上丨学习记录"><img src="https://s2.loli.net/2023/02/15/7lxtXfsgvhpMJ3b.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机网络 - 上丨学习记录"/></a><div class="content"><a class="title" href="/post/Computer-Network.html" title="计算机网络 - 上丨学习记录">计算机网络 - 上丨学习记录</a><time datetime="2023-02-09T12:03:22.000Z" title="发表于 2023-02-09 20:03:22">2023-02-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/RabbitMQ.html" title="RabbitMQ丨学习记录"><img src="/img/NEW2/1300134.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="RabbitMQ丨学习记录"/></a><div class="content"><a class="title" href="/post/RabbitMQ.html" title="RabbitMQ丨学习记录">RabbitMQ丨学习记录</a><time datetime="2023-02-01T02:42:42.000Z" title="发表于 2023-02-01 10:42:42">2023-02-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/Internet_All.html" title="Internet基础丨学习记录"><img src="/img/NEW1/1298218.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Internet基础丨学习记录"/></a><div class="content"><a class="title" href="/post/Internet_All.html" title="Internet基础丨学习记录">Internet基础丨学习记录</a><time datetime="2023-01-30T01:24:31.000Z" title="发表于 2023-01-30 09:24:31">2023-01-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/FFmpeg.html" title="Go + FFmpeg交互丨学习记录"><img src="/img/NEW1/1296409.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Go + FFmpeg交互丨学习记录"/></a><div class="content"><a class="title" href="/post/FFmpeg.html" title="Go + FFmpeg交互丨学习记录">Go + FFmpeg交互丨学习记录</a><time datetime="2023-01-29T06:07:05.000Z" title="发表于 2023-01-29 14:07:05">2023-01-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/Docker.html" title="Docker + VSCode组合丨学习记录"><img src="/img/820043.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Docker + VSCode组合丨学习记录"/></a><div class="content"><a class="title" href="/post/Docker.html" title="Docker + VSCode组合丨学习记录">Docker + VSCode组合丨学习记录</a><time datetime="2023-01-29T06:06:45.000Z" title="发表于 2023-01-29 14:06:45">2023-01-29</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/NEW/5AXIB$%5DU4@VZM5%60207WRQP2.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 落雨乄天珀夜</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>