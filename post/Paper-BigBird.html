<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读 | 落雨乄天珀夜</title><meta name="author" content="落雨乄天珀夜"><meta name="copyright" content="落雨乄天珀夜"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="零、前言：📕欢迎访问：  个人博客：https:&#x2F;&#x2F;conqueror712.github.io&#x2F; 知乎：https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;soeur712&#x2F;posts Bilibili：https:&#x2F;&#x2F;space.bilibili.com&#x2F;57089326 掘金：https:&#x2F;&#x2F;juejin.cn&#x2F;user&#x2F;1297878069809725&#x2F;posts   🍬观前小剧">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读">
<meta property="og:url" content="https://conqueror712.github.io/post/Paper-BigBird.html">
<meta property="og:site_name" content="落雨乄天珀夜">
<meta property="og:description" content="零、前言：📕欢迎访问：  个人博客：https:&#x2F;&#x2F;conqueror712.github.io&#x2F; 知乎：https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;soeur712&#x2F;posts Bilibili：https:&#x2F;&#x2F;space.bilibili.com&#x2F;57089326 掘金：https:&#x2F;&#x2F;juejin.cn&#x2F;user&#x2F;1297878069809725&#x2F;posts   🍬观前小剧">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b44fb44b68d84a9c8bbb56134ade2e57~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?">
<meta property="article:published_time" content="2023-05-29T04:32:00.000Z">
<meta property="article:modified_time" content="2023-05-29T04:33:38.266Z">
<meta property="article:author" content="落雨乄天珀夜">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b44fb44b68d84a9c8bbb56134ade2e57~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?"><link rel="shortcut icon" href="https://s2.loli.net/2023/02/15/LprEbGXq9xgfdDn.jpg"><link rel="canonical" href="https://conqueror712.github.io/post/Paper-BigBird"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-29 12:33:38'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="落雨乄天珀夜" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2023/02/15/LprEbGXq9xgfdDn.jpg" onerror="onerror=null;src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">32</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b44fb44b68d84a9c8bbb56134ade2e57~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">落雨乄天珀夜</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-29T04:32:00.000Z" title="发表于 2023-05-29 12:32:00">2023-05-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-29T04:33:38.266Z" title="更新于 2023-05-29 12:33:38">2023-05-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="零、前言："><a href="#零、前言：" class="headerlink" title="零、前言："></a>零、前言：</h1><p><strong>📕欢迎访问</strong>：</p>
<blockquote>
<p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p>
<p>知乎：<a target="_blank" rel="noopener" href="https://www.zhihu.com/people/soeur712/posts">https://www.zhihu.com/people/soeur712/posts</a></p>
<p>Bilibili：<a target="_blank" rel="noopener" href="https://space.bilibili.com/57089326">https://space.bilibili.com/57089326</a></p>
<p>掘金：<a target="_blank" rel="noopener" href="https://juejin.cn/user/1297878069809725/posts">https://juejin.cn/user/1297878069809725/posts</a></p>
</blockquote>
<hr>
<p><strong>🍬观前小剧场</strong>：</p>
<blockquote>
<p>Q：既然上次我们知道了ELMo、BERT都是芝麻街的东西，那还有什么吗？我觉得这样的命名好有趣！</p>
<p>A：当然！不妨来看看2020年的一篇大鸟，呃不对，是论文——BigBird吧！</p>
</blockquote>
<hr>
<h1 id="一、这是什么鸟？看一下："><a href="#一、这是什么鸟？看一下：" class="headerlink" title="一、这是什么鸟？看一下："></a>一、这是什么鸟？看一下：</h1><h2 id="一句话解释「大鸟」BigBird："><a href="#一句话解释「大鸟」BigBird：" class="headerlink" title="一句话解释「大鸟」BigBird："></a>一句话解释「大鸟」BigBird：</h2><p>通过<strong>稀疏的注意力机制</strong>，将对序列长度的二次依赖<strong>减少到线性</strong>。</p>
<p>基于此，相较于BERT，可以<strong>处理更长的上下文信息</strong>，大大提高了各种NLP任务的性能。</p>
<hr>
<h2 id="BigBird出现的目的和意义："><a href="#BigBird出现的目的和意义：" class="headerlink" title="BigBird出现的目的和意义："></a>BigBird出现的目的和意义：</h2><p>虽然BERT是近年来NLP中最成功的深度学习模型之一，但是由于其全注意力机制，会存在<strong>对序列长度的二次依赖</strong>，从而<strong>不能很好地处理较长的上下文信息</strong>。</p>
<p>而BigBird采用<strong>稀疏的注意力机制</strong>，突破了全注意力机制的<strong>局限</strong>，使得可以<strong>处理更长的上下文信息</strong>。（怎么感觉和上面说的一样）</p>
<hr>
<h2 id="BigBird背景铺垫与介绍："><a href="#BigBird背景铺垫与介绍：" class="headerlink" title="BigBird背景铺垫与介绍："></a>BigBird背景铺垫与介绍：</h2><p>Transformer的广泛应用使得现代NLP模型性能大大优于以前的序列模型，如LSTM，主要是因为<strong>自注意力机制</strong>的引入，这使得我们可以对输入序列的每个令牌去<strong>并行</strong>的评估，而不是像LSTM等循环神经网络的<strong>顺序</strong>依赖性。这样做的好处是可以充分利用GPU和TPU这种现代SIMD硬件加速器的全部功能，从而可以让我们在空前规模的大数据集上训练NLP模型，而这一过程往往是通过预训练以及对下游任务的微调进行的。</p>
<p>但是，正如矛盾是对立而统一的一样，这种改进并不是没有代价的，完全自注意力机制的计算和内存开销是<strong>序列长度的平方</strong>，这降低了它们对更大上下文的任务的直接适用性，最典型的大上下文例子就是QA和文档分类与摘要。如果非要做的话，就会将大字符串拆分更小的段，这种内容碎片化也会导致上下文的严重丢失，从而使其应用程序受到限制。</p>
<p>那么，我们有没有一种可能，可以通过<strong>更少的内积</strong>实现一个完全二次型的自注意力机制呢？这样<strong>稀疏的自注意力机制</strong>能否保留原始网络的表现力和灵活性呢？</p>
<p>答案就是”大鸟”——BigBird！（灵感来源于图的稀疏化方法）</p>
<p>“大鸟”的稀疏注意力机制可以提高需要较长上下文的任务的性能。其复杂性是线性的而非二次的。</p>
<p>“大鸟”主要由三个部分组成：</p>
<ul>
<li>在序列的所有部分上出现的一组（$g$个）全局<code>token</code>的集合</li>
<li>所有与$w$个本地相邻<code>token</code>集合相关的<code>token</code></li>
<li>所有标记都属于$r$个随机标记的集合</li>
</ul>
<p>“大鸟”的三个主要贡献：</p>
<ul>
<li>BigBird满足所有已知的全变换的理论性质，特别是添加额外的记号，允许将所有连续序列表示为线性个内积的序列函数，并且在标准精度下是图灵完备的。这也就意味着，一切可以计算的问题，BigBird都能计算，理论上，它能够用来解决任何算法。</li>
<li>BigBird的扩展上下文有利于各种NLP任务。</li>
<li>一种新的基于注意力机制的模型应用：提取基因组序列（如DNA）的上下文表示。</li>
</ul>
<blockquote>
<p>我们不妨来看一看，各种不同的Transformer变种的效率：</p>
<p>圈越大，时间复杂度越高；</p>
<p>纵坐标越小，空间复杂度越高；</p>
<p>横坐标越大，性能越好；</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2668aa7424b34a929c4b30e2d2200b45~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
</blockquote>
<hr>
<h2 id="BigBird相关工作："><a href="#BigBird相关工作：" class="headerlink" title="BigBird相关工作："></a>BigBird相关工作：</h2><p>相关工作这一部分主要概括了两条限制Transformer的依赖关系，的研究方向（加逗号是为了断句，方便理解）。</p>
<ol>
<li><p>对于长度限制，最简单的方法是使用滑动窗口；更复杂的方法通常重复调用Transformer块，即每次提供不同的上下文。</p>
</li>
<li><p>讨论完全的注意力机制是否是必要的，不完全的注意力机制可以减少内存和计算需求。</p>
</li>
</ol>
<p>除此之外，Longformer引入了局部滑动窗口和少量全局遮罩来减少计算，并将BERT扩展到更长的序列任务。BigBird与Extended Transformers Construction工作密切相关，后者旨在为Transformer编码文本结构。其广泛使用全局令牌来实现目标。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cf44ce15f914426b8ffb41182ab4ec74~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<blockquote>
<p>注意力机制的示意图如上所示，白色表示注意力不集中；</p>
<p>从左到右分别是r &#x3D; 2的随机注意力、w &#x3D; 3的滑动窗口注意力、g &#x3D; 2的全局注意力以及将之合并的”大鸟”。</p>
</blockquote>
<p>这三种注意力机制结合起来为什么就能接近BERT的全注意力机制呢？我们以图释之：</p>
<blockquote>
<p>BigBird：</p>
</blockquote>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ae25633798ac480687061916991123b3~tplv-k3u1fbpfcp-watermark.image" alt="graph.gif"></p>
<blockquote>
<p>BERT：</p>
</blockquote>
<p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ece3c3e207df40538c46632ef7326e87~tplv-k3u1fbpfcp-watermark.image" alt="full.png"></p>
<hr>
<h1 id="二、林子大了，什么鸟都有："><a href="#二、林子大了，什么鸟都有：" class="headerlink" title="二、林子大了，什么鸟都有："></a>二、林子大了，什么鸟都有：</h1><h2 id="稀疏注意力到底在做什么？"><a href="#稀疏注意力到底在做什么？" class="headerlink" title="稀疏注意力到底在做什么？"></a>稀疏注意力到底在做什么？</h2><p>BigBird使用稀疏注意力机制，这说明注意力机制是逐个<code>token</code>应用的，而不是像BERT那样，只对整个输入进行一次应用。</p>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f16dff0b54f246c2b99e0850431b81a6~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<p>如图所示，有额外的向左平移和向右平移的句子，就是顶部的<code>them Vasudev is proting BigBird...</code>这些；</p>
<p>这是计算<strong>滑动注意力</strong>时所需要的，而滑动注意力又是BigBird稀疏注意力的一个重要组成部分，也就是图中黄色的那些。</p>
<h2 id="蛋白质是牛肉的八倍？"><a href="#蛋白质是牛肉的八倍？" class="headerlink" title="蛋白质是牛肉的八倍？"></a>蛋白质是牛肉的八倍？</h2><p>BigBird能够处理比以前长8倍的序列。利用 BigBird 及其稀疏注意力机制，研究小组将 BERT 的复杂度$O(n^2)$降到$O(n)$。</p>
<p>这说明，原来的BERT只能处理512个<code>token</code>的输入序列，现在BigBird可以处理4096个<code>token</code>，整整八倍！</p>
<p>然而事实上，BigBird带给我们的惊喜远不止如此，在BigBird的论文中，使用的是4096这一数值，然而实际上的效果可以达到更大的 16K以上！</p>
<h2 id="一较高下？"><a href="#一较高下？" class="headerlink" title="一较高下？"></a>一较高下？</h2><p>用GPT-3举例（2020年还没有GPT-3.5，更别说GPT-4了），BigBird的预训练集远不如GPT-3大，因为GPT-3的训练参数为1750亿，但如下表所示，BigBird比很多模型的性能更好。</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9a286fffb71c4e82a4346cb6ca61adc6~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<h2 id="21世纪是生物的世纪？"><a href="#21世纪是生物的世纪？" class="headerlink" title="21世纪是生物的世纪？"></a>21世纪是生物的世纪？</h2><p>深度学习在基因组学数据处理中的应用越来越多。编码器将 DNA 序列的片段作为输入，用于诸如甲基化分析、预测非编码变体的功能效应等任务。这些任务以DNA序列片段作为输入，既然是序列，那么BigBird想必可以派上用场，果不其然，甚至因为DNA中的许多功能效应是高度非局部的，也就是偏向于更长的范围，所以长序列的处理显得尤为重要。</p>
<h2 id="大显身手！Google搜索引擎："><a href="#大显身手！Google搜索引擎：" class="headerlink" title="大显身手！Google搜索引擎："></a>大显身手！Google搜索引擎：</h2><p>2019年，BERT出现的时候，Google第一时间就把BERT集成到其搜索引擎中，来理解用户的输入，从而为用户呈现更多、更相关的内容了；2020年，”大鸟”飞来，很快啊！Google马上就又放了进去。</p>
<hr>
<h1 id="三、鸟师傅，你是做什么工作的："><a href="#三、鸟师傅，你是做什么工作的：" class="headerlink" title="三、鸟师傅，你是做什么工作的："></a>三、鸟师傅，你是做什么工作的：</h1><h2 id="再看稀疏注意力："><a href="#再看稀疏注意力：" class="headerlink" title="再看稀疏注意力："></a>再看稀疏注意力：</h2><p>在一般的完全注意力，例如BERT中，序列为$X&#x3D;x_1, x_2,…,x_n$，由稠密向量$Q,K,V$计算的注意力为$Z&#x3D;Softmax(QK^T)$；</p>
<p>在BigBird注意力计算中，这样的过程只在个别的query向量和key向量之间计算。</p>
<p>那么，更多的是什么呢？</p>
<p>设：<code>b, r, s, g</code>分别为<code>block_size, num_random_blocks, num_sliding_blocks, num_global_blocks</code>；</p>
<p>当<code>b = 4, r = 1, s = 3, g = 2</code>时，<code>Q</code>和<code>V</code>的块如下所示：</p>
<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d8d363c4d8e4329b1a5e22eaba7b995~tplv-k3u1fbpfcp-zoom-1.image" alt="avatar" style="zoom:50%;">

<p>注意力得分$q_1,q_2,q_3,q_{n-2},q_{n-1},q_n$计算过程：</p>
<p>$q_1$由$a_1$来表示：$a_1&#x3D;Softmax(q_1 × K^T)$，$q_1$代表第一个块，$g_i$代表第$i$个块</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9f2fd4486f8b46f8ac8ee4611ed77a74~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<p>当计算$a_2$时，事情就变得没那么简单了，需要加上另外两种块，即$a_2&#x3D;Softmax(q2×concat(k1, k2, k3, k5, k7))$</p>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/401e904ae01547ee84f7a3bf2c1421ef~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<p>类似地，来到$q_3$：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a590cbe45d224c2da609cace940a4a11~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<p>倒数第二个，$q_{n-1}$：</p>
<p><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/35ea6b7a23cd45a68ea936c1a96295d0~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<p>最后，也如初见，$a_n&#x3D;Softmax(q_n × K^T)$：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b5ecc98a9f384a6a9fad15041580fde7~tplv-k3u1fbpfcp-zoom-1.image" alt="avatar"></p>
<blockquote>
<p>整体效果图：</p>
</blockquote>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fe6ad1ee41764801a7911c3f5051c7ff~tplv-k3u1fbpfcp-watermark.image" alt="block-sparse-attn.gif"></p>
<p>这就是稀疏注意力中最难的部分了。</p>
<blockquote>
<p>实事求是：</p>
</blockquote>
<p>纸上谈兵显然是行不通的，我们需要数据支撑，稀疏注意力到底为什么好：</p>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/11a9a72862534034bc76be7ca2099324~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<p>嗯！显然好！</p>
<hr>
<h2 id="大鸟大鸟，实战见分晓！"><a href="#大鸟大鸟，实战见分晓！" class="headerlink" title="大鸟大鸟，实战见分晓！"></a>大鸟大鸟，实战见分晓！</h2><blockquote>
<p>像这样，就可以使用BigBird模型了！</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BigBirdModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从预先训练的检查点装载BigBird</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>)</span><br><span class="line"><span class="comment"># 这将初始化模型的默认配置，即attention_type = &quot;block_sparse&quot;, num_random_blocks = 3, block_size = 64</span></span><br><span class="line"><span class="comment"># 但是您可以使用任何检查点自由地更改这些参数</span></span><br><span class="line"><span class="comment"># 这3个参数只会更改每个查询token将要参加的token数量</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>, num_random_blocks=<span class="number">2</span>, block_size=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过将attention_type设置为&quot;original_full&quot;</span></span><br><span class="line"><span class="comment"># BigBird将依赖于n^2复杂度的全部注意力</span></span><br><span class="line"><span class="comment"># 这样，BigBird与BERT的相似度达到99.9%</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>, attention_type=<span class="string">&quot;original_full&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>一个问答任务的例子：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BigBirdForQuestionAnswering, BigBirdTokenizer</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从预训练的权重中初始化BigBird模型，并在其顶部随机初始化头部</span></span><br><span class="line">model = BigBirdForQuestionAnswering.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>, block_size=<span class="number">64</span>, num_random_blocks=<span class="number">3</span>)</span><br><span class="line">tokenizer = BigBirdTokenizer.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">dataset = <span class="string">&quot;torch.utils.data.DataLoader object&quot;</span></span><br><span class="line">optimizer = <span class="string">&quot;torch.optim object&quot;</span></span><br><span class="line">epochs = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 极小的训练循环</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> dataset:</span><br><span class="line">        model.train()</span><br><span class="line">        batch = &#123;k: batch[k].to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = model(**batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        output[<span class="string">&quot;loss&quot;</span>].backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将最终权重保存在本地目录中</span></span><br><span class="line">model.save_pretrained(<span class="string">&quot;&lt;YOUR-WEIGHTS-DIR&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推至Hub</span></span><br><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> ModelHubMixin</span><br><span class="line">ModelHubMixin.push_to_hub(<span class="string">&quot;&lt;YOUR-WEIGHTS-DIR&gt;&quot;</span>, model_id=<span class="string">&quot;&lt;YOUR-FINETUNED-ID&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用微调模型进行推理</span></span><br><span class="line">question = [<span class="string">&quot;How are you doing?&quot;</span>, <span class="string">&quot;How is life going?&quot;</span>]</span><br><span class="line">context = [<span class="string">&quot;&lt;some big context having ans-1&gt;&quot;</span>, <span class="string">&quot;&lt;some big context having ans-2&gt;&quot;</span>]</span><br><span class="line">batch = tokenizer(question, context, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">batch = &#123;k: batch[k].to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch&#125;</span><br><span class="line"></span><br><span class="line">model = BigBirdForQuestionAnswering.from_pretrained(<span class="string">&quot;&lt;YOUR-FINETUNED-ID&gt;&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    start_logits, end_logits = model(**batch).to_tuple()</span><br><span class="line">    <span class="comment"># 用你想要的策略解码start_logits, end_logits</span></span><br></pre></td></tr></table></figure>

<h2 id="BigBird的架构的细节部分："><a href="#BigBird的架构的细节部分：" class="headerlink" title="BigBird的架构的细节部分："></a>BigBird的架构的细节部分：</h2><p>注：这一部分推荐阅读原论文，笔者的翻译水平有限（笑）。</p>
<p>原文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.14062">https://arxiv.org/abs/2007.14062</a></p>
<h3 id="1-使用广义注意力机制来描述BigBird模型："><a href="#1-使用广义注意力机制来描述BigBird模型：" class="headerlink" title="1. 使用广义注意力机制来描述BigBird模型："></a>1. 使用<strong>广义注意力机制</strong>来描述BigBird模型：</h3><p>BigBird使用广义注意力机制和稀疏随机图概念描述模型，以减少自注意机制的复杂度。</p>
<ol>
<li><p>用广义注意力机制描述BigBird模型，该机制用于Transformer的每个层。广义注意力机制由有向图<code>D</code>表示：</p>
<ul>
<li><code>D</code>的节点集是输入序列中的记号；</li>
<li><code>D</code>的弧表示注意力机制将考虑的内积。</li>
</ul>
</li>
<li><p>如果<code>D</code>是完全有向图，可以恢复完全二次注意力机制。为简化表达，BigBird操作<code>D</code>的邻接矩阵<code>A</code>，即使<code>D</code>可能是稀疏的。<code>A(i，j) = 1</code>表示查询<code>i attend</code>到键<code>j</code>，否则为0。当<code>A</code>是全1矩阵时，导致二次复杂度，因为所有记号都<code>attend</code>每个其他记号。</p>
</li>
<li><p>将自注意视为完全连接图允许利用现有图论来减少其复杂度。减少自注意的二次复杂度问题可以视为图稀疏问题。</p>
</li>
<li><p>稀疏随机图注意机制应具有两个理想条件：节点间平均路径长度小和局部性概念。</p>
</li>
<li><p>随机图是展开器，可以在许多不同的上下文中近似完全图，包括在其谱属性方面。</p>
</li>
</ol>
<h3 id="2-Erdos-Renyi随机图模型："><a href="#2-Erdos-Renyi随机图模型：" class="headerlink" title="2. Erdos-Rényi随机图模型："></a>2. Erdos-Rényi随机图模型：</h3><p>在这个模型中，每条边都是独立选定的，出现的概率是固定的。在只有<code>Θ(n)</code>条边的随机图中，任意两个节点之间的最短路径是对数级的。因此，这样的随机图在谱上近似完全图，其第二特征值远离第一个特征值。这一属性导致随机游走在图中的混合时间很快，这暗示信息可以在任意一对节点之间快速流动。因此，BigBird提出了稀疏注意力，其中每个查询仅关注<code>r</code>个随机键。</p>
<h3 id="3-相邻内积与聚类系数："><a href="#3-相邻内积与聚类系数：" class="headerlink" title="3. 相邻内积与聚类系数："></a>3. 相邻内积与聚类系数：</h3><p>大多数NLP和计算生物学中的上下文都具有较高的局部性参考。这意味着一个记号可以从其相邻记号中获取大量信息。BigBird研究自注意模型时得出结论，相邻内积对于NLP任务极其重要。在图论术语中，聚类系数是局部连接性的度量，当图包含许多完全图或近完全图时，聚类系数较高。简单的Erdos-Rényi随机图没有高的聚类系数，但小世界图具有高聚类系数。因此，BigBird定义了滑动窗口注意力，使查询在宽度为<code>w</code>的自注意中attend到<code>i - w/2</code>到<code>i + w/2</code>之间的键。</p>
<hr>
<h1 id="四、结论与尾声："><a href="#四、结论与尾声：" class="headerlink" title="四、结论与尾声："></a>四、结论与尾声：</h1><p>BigBird是一个线性相关的稀疏注意机制，有理论保证和实践检验，是序列到序列函数的通用近似器，并且是图灵完备的。</p>
<p><strong>在理论上：</strong></p>
<ul>
<li>BigBird使用额外的全局令牌来保留模型的表达能力。</li>
<li>BigBird通过移动到稀疏注意机制来减少计算，但同时确实也会产生一些其他的成本。</li>
</ul>
<p><strong>在实际上：</strong></p>
<p>BigBird在许多NLP任务上取得了最先进的性能，如问答和长文档分类。</p>
<p>BigBird进一步引入了基于注意机制的DNA上下文语言模型，并对下游任务进行调优，如启动子区域预测和预测非编码变异的效果。</p>
<p>最后，BigBird虽好，但是正如其名，太Big了，这里是指代码量，实现起来实在是比较复杂。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://conqueror712.github.io">落雨乄天珀夜</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://conqueror712.github.io/post/Paper-BigBird.html">https://conqueror712.github.io/post/Paper-BigBird.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://conqueror712.github.io" target="_blank">落雨乄天珀夜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b44fb44b68d84a9c8bbb56134ade2e57~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/Paper-Transformer.html"><img class="prev-cover" src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/48c24f16f14c4efcadec5ad2dfd34de1~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" onerror="onerror=null;src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">再回首划时代的技术 - Transformer丨论文解读</div></div></a></div><div class="next-post pull-right"><a href="/post/Paper-BERT.html"><img class="next-cover" src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a82ec344c5e746a789f5a5f10577397a~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" onerror="onerror=null;src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">2023了，再来看看NLP经典之作 - BERT丨论文解读</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2023/02/15/LprEbGXq9xgfdDn.jpg" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="avatar"/></div><div class="author-info__name">落雨乄天珀夜</div><div class="author-info__description">「落花人独立丨微雨燕双飞」</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">32</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Conqueror712"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog ~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9B%B6%E3%80%81%E5%89%8D%E8%A8%80%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">零、前言：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%BF%99%E6%98%AF%E4%BB%80%E4%B9%88%E9%B8%9F%EF%BC%9F%E7%9C%8B%E4%B8%80%E4%B8%8B%EF%BC%9A"><span class="toc-number">2.</span> <span class="toc-text">一、这是什么鸟？看一下：</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E5%8F%A5%E8%AF%9D%E8%A7%A3%E9%87%8A%E3%80%8C%E5%A4%A7%E9%B8%9F%E3%80%8DBigBird%EF%BC%9A"><span class="toc-number">2.1.</span> <span class="toc-text">一句话解释「大鸟」BigBird：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BigBird%E5%87%BA%E7%8E%B0%E7%9A%84%E7%9B%AE%E7%9A%84%E5%92%8C%E6%84%8F%E4%B9%89%EF%BC%9A"><span class="toc-number">2.2.</span> <span class="toc-text">BigBird出现的目的和意义：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BigBird%E8%83%8C%E6%99%AF%E9%93%BA%E5%9E%AB%E4%B8%8E%E4%BB%8B%E7%BB%8D%EF%BC%9A"><span class="toc-number">2.3.</span> <span class="toc-text">BigBird背景铺垫与介绍：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BigBird%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%EF%BC%9A"><span class="toc-number">2.4.</span> <span class="toc-text">BigBird相关工作：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%9E%97%E5%AD%90%E5%A4%A7%E4%BA%86%EF%BC%8C%E4%BB%80%E4%B9%88%E9%B8%9F%E9%83%BD%E6%9C%89%EF%BC%9A"><span class="toc-number">3.</span> <span class="toc-text">二、林子大了，什么鸟都有：</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%B0%E5%BA%95%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">3.1.</span> <span class="toc-text">稀疏注意力到底在做什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%9B%8B%E7%99%BD%E8%B4%A8%E6%98%AF%E7%89%9B%E8%82%89%E7%9A%84%E5%85%AB%E5%80%8D%EF%BC%9F"><span class="toc-number">3.2.</span> <span class="toc-text">蛋白质是牛肉的八倍？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E8%BE%83%E9%AB%98%E4%B8%8B%EF%BC%9F"><span class="toc-number">3.3.</span> <span class="toc-text">一较高下？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21%E4%B8%96%E7%BA%AA%E6%98%AF%E7%94%9F%E7%89%A9%E7%9A%84%E4%B8%96%E7%BA%AA%EF%BC%9F"><span class="toc-number">3.4.</span> <span class="toc-text">21世纪是生物的世纪？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%98%BE%E8%BA%AB%E6%89%8B%EF%BC%81Google%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%9A"><span class="toc-number">3.5.</span> <span class="toc-text">大显身手！Google搜索引擎：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E9%B8%9F%E5%B8%88%E5%82%85%EF%BC%8C%E4%BD%A0%E6%98%AF%E5%81%9A%E4%BB%80%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84%EF%BC%9A"><span class="toc-number">4.</span> <span class="toc-text">三、鸟师傅，你是做什么工作的：</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%8D%E7%9C%8B%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%9A"><span class="toc-number">4.1.</span> <span class="toc-text">再看稀疏注意力：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E9%B8%9F%E5%A4%A7%E9%B8%9F%EF%BC%8C%E5%AE%9E%E6%88%98%E8%A7%81%E5%88%86%E6%99%93%EF%BC%81"><span class="toc-number">4.2.</span> <span class="toc-text">大鸟大鸟，实战见分晓！</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BigBird%E7%9A%84%E6%9E%B6%E6%9E%84%E7%9A%84%E7%BB%86%E8%8A%82%E9%83%A8%E5%88%86%EF%BC%9A"><span class="toc-number">4.3.</span> <span class="toc-text">BigBird的架构的细节部分：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BD%BF%E7%94%A8%E5%B9%BF%E4%B9%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%9D%A5%E6%8F%8F%E8%BF%B0BigBird%E6%A8%A1%E5%9E%8B%EF%BC%9A"><span class="toc-number">4.3.1.</span> <span class="toc-text">1. 使用广义注意力机制来描述BigBird模型：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Erdos-Renyi%E9%9A%8F%E6%9C%BA%E5%9B%BE%E6%A8%A1%E5%9E%8B%EF%BC%9A"><span class="toc-number">4.3.2.</span> <span class="toc-text">2. Erdos-Rényi随机图模型：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%9B%B8%E9%82%BB%E5%86%85%E7%A7%AF%E4%B8%8E%E8%81%9A%E7%B1%BB%E7%B3%BB%E6%95%B0%EF%BC%9A"><span class="toc-number">4.3.3.</span> <span class="toc-text">3. 相邻内积与聚类系数：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%BB%93%E8%AE%BA%E4%B8%8E%E5%B0%BE%E5%A3%B0%EF%BC%9A"><span class="toc-number">5.</span> <span class="toc-text">四、结论与尾声：</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/ZLUDA.html" title="ZLUDA丨如何在Intel的GPU上运行CUDA代码？"><img src="https://cdnjson.com/images/2023/07/02/wallhaven-welrv7.jpg" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="ZLUDA丨如何在Intel的GPU上运行CUDA代码？"/></a><div class="content"><a class="title" href="/post/ZLUDA.html" title="ZLUDA丨如何在Intel的GPU上运行CUDA代码？">ZLUDA丨如何在Intel的GPU上运行CUDA代码？</a><time datetime="2023-07-02T13:07:09.000Z" title="发表于 2023-07-02 21:07:09">2023-07-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/Paper-Transformer.html" title="再回首划时代的技术 - Transformer丨论文解读"><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/48c24f16f14c4efcadec5ad2dfd34de1~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="再回首划时代的技术 - Transformer丨论文解读"/></a><div class="content"><a class="title" href="/post/Paper-Transformer.html" title="再回首划时代的技术 - Transformer丨论文解读">再回首划时代的技术 - Transformer丨论文解读</a><time datetime="2023-06-06T09:25:30.000Z" title="发表于 2023-06-06 17:25:30">2023-06-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/Paper-BigBird.html" title="NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读"><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b44fb44b68d84a9c8bbb56134ade2e57~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读"/></a><div class="content"><a class="title" href="/post/Paper-BigBird.html" title="NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读">NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读</a><time datetime="2023-05-29T04:32:00.000Z" title="发表于 2023-05-29 12:32:00">2023-05-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/Paper-BERT.html" title="2023了，再来看看NLP经典之作 - BERT丨论文解读"><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a82ec344c5e746a789f5a5f10577397a~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="2023了，再来看看NLP经典之作 - BERT丨论文解读"/></a><div class="content"><a class="title" href="/post/Paper-BERT.html" title="2023了，再来看看NLP经典之作 - BERT丨论文解读">2023了，再来看看NLP经典之作 - BERT丨论文解读</a><time datetime="2023-05-20T13:48:01.000Z" title="发表于 2023-05-20 21:48:01">2023-05-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/Computer-Network-4-2.html" title="计网 -  路由算法丨学习记录"><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e26cc82ed8b74dfbbad55d94b885444e~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" onerror="this.onerror=null;this.src='https://s2.loli.net/2023/02/15/gKRvwPCJqcN8Wiu.jpg'" alt="计网 -  路由算法丨学习记录"/></a><div class="content"><a class="title" href="/post/Computer-Network-4-2.html" title="计网 -  路由算法丨学习记录">计网 -  路由算法丨学习记录</a><time datetime="2023-05-15T11:19:06.000Z" title="发表于 2023-05-15 19:19:06">2023-05-15</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b44fb44b68d84a9c8bbb56134ade2e57~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 落雨乄天珀夜</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>