<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Conqueror712">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://conqueror712.github.io/post/paper-bigbird.html"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="零、前言：📕欢迎访问：  个人博客：https:&#x2F;&#x2F;conqueror712.github.io&#x2F; 知乎：https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;soeur712&#x2F;posts  Bilibili：https:&#x2F;&#x2F;space.bilibili.com&#x2F;57089326  掘金：https:&#x2F;&#x2F;juejin.cn&#x2F;user&#x2F;1297878069809725&#x2F;posts    🍬观">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读">
<meta property="og:url" content="https://conqueror712.github.io/post/Paper-BigBird.html">
<meta property="og:site_name" content="落雨乄天珀夜">
<meta property="og:description" content="零、前言：📕欢迎访问：  个人博客：https:&#x2F;&#x2F;conqueror712.github.io&#x2F; 知乎：https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;soeur712&#x2F;posts  Bilibili：https:&#x2F;&#x2F;space.bilibili.com&#x2F;57089326  掘金：https:&#x2F;&#x2F;juejin.cn&#x2F;user&#x2F;1297878069809725&#x2F;posts    🍬观">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2668aa7424b34a929c4b30e2d2200b45~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cf44ce15f914426b8ffb41182ab4ec74~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ae25633798ac480687061916991123b3~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ece3c3e207df40538c46632ef7326e87~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f16dff0b54f246c2b99e0850431b81a6~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9a286fffb71c4e82a4346cb6ca61adc6~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d8d363c4d8e4329b1a5e22eaba7b995~tplv-k3u1fbpfcp-zoom-1.image">
<meta property="og:image" content="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9f2fd4486f8b46f8ac8ee4611ed77a74~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/401e904ae01547ee84f7a3bf2c1421ef~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a590cbe45d224c2da609cace940a4a11~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/35ea6b7a23cd45a68ea936c1a96295d0~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b5ecc98a9f384a6a9fad15041580fde7~tplv-k3u1fbpfcp-zoom-1.image">
<meta property="og:image" content="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fe6ad1ee41764801a7911c3f5051c7ff~tplv-k3u1fbpfcp-watermark.image">
<meta property="og:image" content="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/11a9a72862534034bc76be7ca2099324~tplv-k3u1fbpfcp-watermark.image">
<meta property="article:published_time" content="2023-05-29T04:32:00.000Z">
<meta property="article:modified_time" content="2023-05-29T04:33:38.266Z">
<meta property="article:author" content="落雨乄天珀夜">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2668aa7424b34a929c4b30e2d2200b45~tplv-k3u1fbpfcp-watermark.image">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/redefine-favicon.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/redefine-favicon.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/redefine-favicon.svg">
    <!--- Page Info-->
    
    <title>
        
            NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读 -
        
        Conqueror712
    </title>
    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/css/build/styles.css">

    

    
<link rel="stylesheet" href="/fonts/fonts.css">

    
<link rel="stylesheet" href="/fonts/Satoshi/satoshi.css">

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">

    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"conqueror712.github.io","root":"/","language":"zh-CN"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"This blog will be updated in November, so stay tuned...","subtitle":{"text":[],"hitokoto":{"enable":true,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"links":{"github":"https://github.com/Conqueror712","email":"Conqueror712@bupt.edu.cn"},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.4.4","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2022/8/17 11:45:14"};
    Global.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="落雨乄天珀夜" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="swup-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container" id="swup">

    

    <div class="main-content-container">


        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="https://conqueror712.github.io/">
                
                Conqueror712
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        首页
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer w-full absolute top-0 left-0 bg-background-color">
        <ul class="drawer-navbar-list flex flex-col justify-start items-center">
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                首页
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="post-page-container">
    <div class="article-content-container">

        <div class="article-title">
            
                
                
                <img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b44fb44b68d84a9c8bbb56134ade2e57~tplv-k3u1fbpfcp-zoom-crop-mark:1512:1512:1512:851.awebp?" alt="NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读" class="max-w-none"/>
                
                <h1 class="article-title-cover">NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读</h1>
            
            </div>
            
                    
        
        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/redefine-avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Conqueror712</span>
                        
                            <span class="author-label">Lv4</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2023-05-29 12:32</span>
        <span class="mobile">2023-05-29 12:32</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2023-05-29 12:33:38</span>
            <span class="mobile">2023-05-29 12:33:38</span>
            <span class="hover-info">更新</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                
                    
                        
                        <li>
                            <a href="/categories/AI/">AI</a>&nbsp;
                        </li>
                    
                    
                
            </ul>
        </span>
    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        


        <div class="article-content markdown-body">
            <h1 id="零、前言："><a href="#零、前言：" class="headerlink" title="零、前言："></a>零、前言：</h1><p><strong>📕欢迎访问</strong>：</p>
<blockquote>
<p>个人博客：<a href="https://conqueror712.github.io/">https://conqueror712.github.io/</a></p>
<p>知乎：<a class="link"   target="_blank" rel="noopener" href="https://www.zhihu.com/people/soeur712/posts" >https://www.zhihu.com/people/soeur712/posts <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>Bilibili：<a class="link"   target="_blank" rel="noopener" href="https://space.bilibili.com/57089326" >https://space.bilibili.com/57089326 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>掘金：<a class="link"   target="_blank" rel="noopener" href="https://juejin.cn/user/1297878069809725/posts" >https://juejin.cn/user/1297878069809725/posts <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
</blockquote>
<hr>
<p><strong>🍬观前小剧场</strong>：</p>
<blockquote>
<p>Q：既然上次我们知道了ELMo、BERT都是芝麻街的东西，那还有什么吗？我觉得这样的命名好有趣！</p>
<p>A：当然！不妨来看看2020年的一篇大鸟，呃不对，是论文——BigBird吧！</p>
</blockquote>
<hr>
<h1 id="一、这是什么鸟？看一下："><a href="#一、这是什么鸟？看一下：" class="headerlink" title="一、这是什么鸟？看一下："></a>一、这是什么鸟？看一下：</h1><h2 id="一句话解释「大鸟」BigBird："><a href="#一句话解释「大鸟」BigBird：" class="headerlink" title="一句话解释「大鸟」BigBird："></a>一句话解释「大鸟」BigBird：</h2><p>通过<strong>稀疏的注意力机制</strong>，将对序列长度的二次依赖<strong>减少到线性</strong>。</p>
<p>基于此，相较于BERT，可以<strong>处理更长的上下文信息</strong>，大大提高了各种NLP任务的性能。</p>
<hr>
<h2 id="BigBird出现的目的和意义："><a href="#BigBird出现的目的和意义：" class="headerlink" title="BigBird出现的目的和意义："></a>BigBird出现的目的和意义：</h2><p>虽然BERT是近年来NLP中最成功的深度学习模型之一，但是由于其全注意力机制，会存在<strong>对序列长度的二次依赖</strong>，从而<strong>不能很好地处理较长的上下文信息</strong>。</p>
<p>而BigBird采用<strong>稀疏的注意力机制</strong>，突破了全注意力机制的<strong>局限</strong>，使得可以<strong>处理更长的上下文信息</strong>。（怎么感觉和上面说的一样）</p>
<hr>
<h2 id="BigBird背景铺垫与介绍："><a href="#BigBird背景铺垫与介绍：" class="headerlink" title="BigBird背景铺垫与介绍："></a>BigBird背景铺垫与介绍：</h2><p>Transformer的广泛应用使得现代NLP模型性能大大优于以前的序列模型，如LSTM，主要是因为<strong>自注意力机制</strong>的引入，这使得我们可以对输入序列的每个令牌去<strong>并行</strong>的评估，而不是像LSTM等循环神经网络的<strong>顺序</strong>依赖性。这样做的好处是可以充分利用GPU和TPU这种现代SIMD硬件加速器的全部功能，从而可以让我们在空前规模的大数据集上训练NLP模型，而这一过程往往是通过预训练以及对下游任务的微调进行的。</p>
<p>但是，正如矛盾是对立而统一的一样，这种改进并不是没有代价的，完全自注意力机制的计算和内存开销是<strong>序列长度的平方</strong>，这降低了它们对更大上下文的任务的直接适用性，最典型的大上下文例子就是QA和文档分类与摘要。如果非要做的话，就会将大字符串拆分更小的段，这种内容碎片化也会导致上下文的严重丢失，从而使其应用程序受到限制。</p>
<p>那么，我们有没有一种可能，可以通过<strong>更少的内积</strong>实现一个完全二次型的自注意力机制呢？这样<strong>稀疏的自注意力机制</strong>能否保留原始网络的表现力和灵活性呢？</p>
<p>答案就是”大鸟”——BigBird！（灵感来源于图的稀疏化方法）</p>
<p>“大鸟”的稀疏注意力机制可以提高需要较长上下文的任务的性能。其复杂性是线性的而非二次的。</p>
<p>“大鸟”主要由三个部分组成：</p>
<ul>
<li>在序列的所有部分上出现的一组（$g$个）全局<code>token</code>的集合</li>
<li>所有与$w$个本地相邻<code>token</code>集合相关的<code>token</code></li>
<li>所有标记都属于$r$个随机标记的集合</li>
</ul>
<p>“大鸟”的三个主要贡献：</p>
<ul>
<li>BigBird满足所有已知的全变换的理论性质，特别是添加额外的记号，允许将所有连续序列表示为线性个内积的序列函数，并且在标准精度下是图灵完备的。这也就意味着，一切可以计算的问题，BigBird都能计算，理论上，它能够用来解决任何算法。</li>
<li>BigBird的扩展上下文有利于各种NLP任务。</li>
<li>一种新的基于注意力机制的模型应用：提取基因组序列（如DNA）的上下文表示。</li>
</ul>
<blockquote>
<p>我们不妨来看一看，各种不同的Transformer变种的效率：</p>
<p>圈越大，时间复杂度越高；</p>
<p>纵坐标越小，空间复杂度越高；</p>
<p>横坐标越大，性能越好；</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2668aa7424b34a929c4b30e2d2200b45~tplv-k3u1fbpfcp-watermark.image"
                      alt="image.png"
                ></p>
</blockquote>
<hr>
<h2 id="BigBird相关工作："><a href="#BigBird相关工作：" class="headerlink" title="BigBird相关工作："></a>BigBird相关工作：</h2><p>相关工作这一部分主要概括了两条限制Transformer的依赖关系，的研究方向（加逗号是为了断句，方便理解）。</p>
<ol>
<li><p>对于长度限制，最简单的方法是使用滑动窗口；更复杂的方法通常重复调用Transformer块，即每次提供不同的上下文。</p>
</li>
<li><p>讨论完全的注意力机制是否是必要的，不完全的注意力机制可以减少内存和计算需求。</p>
</li>
</ol>
<p>除此之外，Longformer引入了局部滑动窗口和少量全局遮罩来减少计算，并将BERT扩展到更长的序列任务。BigBird与Extended Transformers Construction工作密切相关，后者旨在为Transformer编码文本结构。其广泛使用全局令牌来实现目标。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cf44ce15f914426b8ffb41182ab4ec74~tplv-k3u1fbpfcp-watermark.image"
                      alt="image.png"
                ></p>
<blockquote>
<p>注意力机制的示意图如上所示，白色表示注意力不集中；</p>
<p>从左到右分别是r &#x3D; 2的随机注意力、w &#x3D; 3的滑动窗口注意力、g &#x3D; 2的全局注意力以及将之合并的”大鸟”。</p>
</blockquote>
<p>这三种注意力机制结合起来为什么就能接近BERT的全注意力机制呢？我们以图释之：</p>
<blockquote>
<p>BigBird：</p>
</blockquote>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ae25633798ac480687061916991123b3~tplv-k3u1fbpfcp-watermark.image"
                      alt="graph.gif"
                ></p>
<blockquote>
<p>BERT：</p>
</blockquote>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ece3c3e207df40538c46632ef7326e87~tplv-k3u1fbpfcp-watermark.image"
                      alt="full.png"
                ></p>
<hr>
<h1 id="二、林子大了，什么鸟都有："><a href="#二、林子大了，什么鸟都有：" class="headerlink" title="二、林子大了，什么鸟都有："></a>二、林子大了，什么鸟都有：</h1><h2 id="稀疏注意力到底在做什么？"><a href="#稀疏注意力到底在做什么？" class="headerlink" title="稀疏注意力到底在做什么？"></a>稀疏注意力到底在做什么？</h2><p>BigBird使用稀疏注意力机制，这说明注意力机制是逐个<code>token</code>应用的，而不是像BERT那样，只对整个输入进行一次应用。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f16dff0b54f246c2b99e0850431b81a6~tplv-k3u1fbpfcp-watermark.image"
                      alt="image.png"
                ></p>
<p>如图所示，有额外的向左平移和向右平移的句子，就是顶部的<code>them Vasudev is proting BigBird...</code>这些；</p>
<p>这是计算<strong>滑动注意力</strong>时所需要的，而滑动注意力又是BigBird稀疏注意力的一个重要组成部分，也就是图中黄色的那些。</p>
<h2 id="蛋白质是牛肉的八倍？"><a href="#蛋白质是牛肉的八倍？" class="headerlink" title="蛋白质是牛肉的八倍？"></a>蛋白质是牛肉的八倍？</h2><p>BigBird能够处理比以前长8倍的序列。利用 BigBird 及其稀疏注意力机制，研究小组将 BERT 的复杂度$O(n^2)$降到$O(n)$。</p>
<p>这说明，原来的BERT只能处理512个<code>token</code>的输入序列，现在BigBird可以处理4096个<code>token</code>，整整八倍！</p>
<p>然而事实上，BigBird带给我们的惊喜远不止如此，在BigBird的论文中，使用的是4096这一数值，然而实际上的效果可以达到更大的 16K以上！</p>
<h2 id="一较高下？"><a href="#一较高下？" class="headerlink" title="一较高下？"></a>一较高下？</h2><p>用GPT-3举例（2020年还没有GPT-3.5，更别说GPT-4了），BigBird的预训练集远不如GPT-3大，因为GPT-3的训练参数为1750亿，但如下表所示，BigBird比很多模型的性能更好。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9a286fffb71c4e82a4346cb6ca61adc6~tplv-k3u1fbpfcp-watermark.image"
                      alt="image.png"
                ></p>
<h2 id="21世纪是生物的世纪？"><a href="#21世纪是生物的世纪？" class="headerlink" title="21世纪是生物的世纪？"></a>21世纪是生物的世纪？</h2><p>深度学习在基因组学数据处理中的应用越来越多。编码器将 DNA 序列的片段作为输入，用于诸如甲基化分析、预测非编码变体的功能效应等任务。这些任务以DNA序列片段作为输入，既然是序列，那么BigBird想必可以派上用场，果不其然，甚至因为DNA中的许多功能效应是高度非局部的，也就是偏向于更长的范围，所以长序列的处理显得尤为重要。</p>
<h2 id="大显身手！Google搜索引擎："><a href="#大显身手！Google搜索引擎：" class="headerlink" title="大显身手！Google搜索引擎："></a>大显身手！Google搜索引擎：</h2><p>2019年，BERT出现的时候，Google第一时间就把BERT集成到其搜索引擎中，来理解用户的输入，从而为用户呈现更多、更相关的内容了；2020年，”大鸟”飞来，很快啊！Google马上就又放了进去。</p>
<hr>
<h1 id="三、鸟师傅，你是做什么工作的："><a href="#三、鸟师傅，你是做什么工作的：" class="headerlink" title="三、鸟师傅，你是做什么工作的："></a>三、鸟师傅，你是做什么工作的：</h1><h2 id="再看稀疏注意力："><a href="#再看稀疏注意力：" class="headerlink" title="再看稀疏注意力："></a>再看稀疏注意力：</h2><p>在一般的完全注意力，例如BERT中，序列为$X&#x3D;x_1, x_2,…,x_n$，由稠密向量$Q,K,V$计算的注意力为$Z&#x3D;Softmax(QK^T)$；</p>
<p>在BigBird注意力计算中，这样的过程只在个别的query向量和key向量之间计算。</p>
<p>那么，更多的是什么呢？</p>
<p>设：<code>b, r, s, g</code>分别为<code>block_size, num_random_blocks, num_sliding_blocks, num_global_blocks</code>；</p>
<p>当<code>b = 4, r = 1, s = 3, g = 2</code>时，<code>Q</code>和<code>V</code>的块如下所示：</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d8d363c4d8e4329b1a5e22eaba7b995~tplv-k3u1fbpfcp-zoom-1.image"
                      alt="avatar" style="zoom:50%;"
                >

<p>注意力得分$q_1,q_2,q_3,q_{n-2},q_{n-1},q_n$计算过程：</p>
<p>$q_1$由$a_1$来表示：$a_1&#x3D;Softmax(q_1 × K^T)$，$q_1$代表第一个块，$g_i$代表第$i$个块</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9f2fd4486f8b46f8ac8ee4611ed77a74~tplv-k3u1fbpfcp-watermark.image"
                      alt="image.png"
                ></p>
<p>当计算$a_2$时，事情就变得没那么简单了，需要加上另外两种块，即$a_2&#x3D;Softmax(q2×concat(k1, k2, k3, k5, k7))$</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/401e904ae01547ee84f7a3bf2c1421ef~tplv-k3u1fbpfcp-watermark.image"
                      alt="image.png"
                ></p>
<p>类似地，来到$q_3$：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a590cbe45d224c2da609cace940a4a11~tplv-k3u1fbpfcp-watermark.image"
                      alt="image.png"
                ></p>
<p>倒数第二个，$q_{n-1}$：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/35ea6b7a23cd45a68ea936c1a96295d0~tplv-k3u1fbpfcp-watermark.image"
                      alt="image.png"
                ></p>
<p>最后，也如初见，$a_n&#x3D;Softmax(q_n × K^T)$：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b5ecc98a9f384a6a9fad15041580fde7~tplv-k3u1fbpfcp-zoom-1.image"
                      alt="avatar"
                ></p>
<blockquote>
<p>整体效果图：</p>
</blockquote>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fe6ad1ee41764801a7911c3f5051c7ff~tplv-k3u1fbpfcp-watermark.image"
                      alt="block-sparse-attn.gif"
                ></p>
<p>这就是稀疏注意力中最难的部分了。</p>
<blockquote>
<p>实事求是：</p>
</blockquote>
<p>纸上谈兵显然是行不通的，我们需要数据支撑，稀疏注意力到底为什么好：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/11a9a72862534034bc76be7ca2099324~tplv-k3u1fbpfcp-watermark.image"
                      alt="image.png"
                ></p>
<p>嗯！显然好！</p>
<hr>
<h2 id="大鸟大鸟，实战见分晓！"><a href="#大鸟大鸟，实战见分晓！" class="headerlink" title="大鸟大鸟，实战见分晓！"></a>大鸟大鸟，实战见分晓！</h2><blockquote>
<p>像这样，就可以使用BigBird模型了！</p>
</blockquote>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BigBirdModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从预先训练的检查点装载BigBird</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>)</span><br><span class="line"><span class="comment"># 这将初始化模型的默认配置，即attention_type = &quot;block_sparse&quot;, num_random_blocks = 3, block_size = 64</span></span><br><span class="line"><span class="comment"># 但是您可以使用任何检查点自由地更改这些参数</span></span><br><span class="line"><span class="comment"># 这3个参数只会更改每个查询token将要参加的token数量</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>, num_random_blocks=<span class="number">2</span>, block_size=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过将attention_type设置为&quot;original_full&quot;</span></span><br><span class="line"><span class="comment"># BigBird将依赖于n^2复杂度的全部注意力</span></span><br><span class="line"><span class="comment"># 这样，BigBird与BERT的相似度达到99.9%</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>, attention_type=<span class="string">&quot;original_full&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<blockquote>
<p>一个问答任务的例子：</p>
</blockquote>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BigBirdForQuestionAnswering, BigBirdTokenizer</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从预训练的权重中初始化BigBird模型，并在其顶部随机初始化头部</span></span><br><span class="line">model = BigBirdForQuestionAnswering.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>, block_size=<span class="number">64</span>, num_random_blocks=<span class="number">3</span>)</span><br><span class="line">tokenizer = BigBirdTokenizer.from_pretrained(<span class="string">&quot;google/BigBird-roberta-base&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">dataset = <span class="string">&quot;torch.utils.data.DataLoader object&quot;</span></span><br><span class="line">optimizer = <span class="string">&quot;torch.optim object&quot;</span></span><br><span class="line">epochs = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 极小的训练循环</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> dataset:</span><br><span class="line">        model.train()</span><br><span class="line">        batch = &#123;k: batch[k].to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = model(**batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        output[<span class="string">&quot;loss&quot;</span>].backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将最终权重保存在本地目录中</span></span><br><span class="line">model.save_pretrained(<span class="string">&quot;&lt;YOUR-WEIGHTS-DIR&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推至Hub</span></span><br><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> ModelHubMixin</span><br><span class="line">ModelHubMixin.push_to_hub(<span class="string">&quot;&lt;YOUR-WEIGHTS-DIR&gt;&quot;</span>, model_id=<span class="string">&quot;&lt;YOUR-FINETUNED-ID&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用微调模型进行推理</span></span><br><span class="line">question = [<span class="string">&quot;How are you doing?&quot;</span>, <span class="string">&quot;How is life going?&quot;</span>]</span><br><span class="line">context = [<span class="string">&quot;&lt;some big context having ans-1&gt;&quot;</span>, <span class="string">&quot;&lt;some big context having ans-2&gt;&quot;</span>]</span><br><span class="line">batch = tokenizer(question, context, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">batch = &#123;k: batch[k].to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch&#125;</span><br><span class="line"></span><br><span class="line">model = BigBirdForQuestionAnswering.from_pretrained(<span class="string">&quot;&lt;YOUR-FINETUNED-ID&gt;&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    start_logits, end_logits = model(**batch).to_tuple()</span><br><span class="line">    <span class="comment"># 用你想要的策略解码start_logits, end_logits</span></span><br></pre></td></tr></table></figure></div>

<h2 id="BigBird的架构的细节部分："><a href="#BigBird的架构的细节部分：" class="headerlink" title="BigBird的架构的细节部分："></a>BigBird的架构的细节部分：</h2><p>注：这一部分推荐阅读原论文，笔者的翻译水平有限（笑）。</p>
<p>原文链接：<a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.14062" >https://arxiv.org/abs/2007.14062 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="1-使用广义注意力机制来描述BigBird模型："><a href="#1-使用广义注意力机制来描述BigBird模型：" class="headerlink" title="1. 使用广义注意力机制来描述BigBird模型："></a>1. 使用<strong>广义注意力机制</strong>来描述BigBird模型：</h3><p>BigBird使用广义注意力机制和稀疏随机图概念描述模型，以减少自注意机制的复杂度。</p>
<ol>
<li><p>用广义注意力机制描述BigBird模型，该机制用于Transformer的每个层。广义注意力机制由有向图<code>D</code>表示：</p>
<ul>
<li><code>D</code>的节点集是输入序列中的记号；</li>
<li><code>D</code>的弧表示注意力机制将考虑的内积。</li>
</ul>
</li>
<li><p>如果<code>D</code>是完全有向图，可以恢复完全二次注意力机制。为简化表达，BigBird操作<code>D</code>的邻接矩阵<code>A</code>，即使<code>D</code>可能是稀疏的。<code>A(i，j) = 1</code>表示查询<code>i attend</code>到键<code>j</code>，否则为0。当<code>A</code>是全1矩阵时，导致二次复杂度，因为所有记号都<code>attend</code>每个其他记号。</p>
</li>
<li><p>将自注意视为完全连接图允许利用现有图论来减少其复杂度。减少自注意的二次复杂度问题可以视为图稀疏问题。</p>
</li>
<li><p>稀疏随机图注意机制应具有两个理想条件：节点间平均路径长度小和局部性概念。</p>
</li>
<li><p>随机图是展开器，可以在许多不同的上下文中近似完全图，包括在其谱属性方面。</p>
</li>
</ol>
<h3 id="2-Erdos-Renyi随机图模型："><a href="#2-Erdos-Renyi随机图模型：" class="headerlink" title="2. Erdos-Rényi随机图模型："></a>2. Erdos-Rényi随机图模型：</h3><p>在这个模型中，每条边都是独立选定的，出现的概率是固定的。在只有<code>Θ(n)</code>条边的随机图中，任意两个节点之间的最短路径是对数级的。因此，这样的随机图在谱上近似完全图，其第二特征值远离第一个特征值。这一属性导致随机游走在图中的混合时间很快，这暗示信息可以在任意一对节点之间快速流动。因此，BigBird提出了稀疏注意力，其中每个查询仅关注<code>r</code>个随机键。</p>
<h3 id="3-相邻内积与聚类系数："><a href="#3-相邻内积与聚类系数：" class="headerlink" title="3. 相邻内积与聚类系数："></a>3. 相邻内积与聚类系数：</h3><p>大多数NLP和计算生物学中的上下文都具有较高的局部性参考。这意味着一个记号可以从其相邻记号中获取大量信息。BigBird研究自注意模型时得出结论，相邻内积对于NLP任务极其重要。在图论术语中，聚类系数是局部连接性的度量，当图包含许多完全图或近完全图时，聚类系数较高。简单的Erdos-Rényi随机图没有高的聚类系数，但小世界图具有高聚类系数。因此，BigBird定义了滑动窗口注意力，使查询在宽度为<code>w</code>的自注意中attend到<code>i - w/2</code>到<code>i + w/2</code>之间的键。</p>
<hr>
<h1 id="四、结论与尾声："><a href="#四、结论与尾声：" class="headerlink" title="四、结论与尾声："></a>四、结论与尾声：</h1><p>BigBird是一个线性相关的稀疏注意机制，有理论保证和实践检验，是序列到序列函数的通用近似器，并且是图灵完备的。</p>
<p><strong>在理论上：</strong></p>
<ul>
<li>BigBird使用额外的全局令牌来保留模型的表达能力。</li>
<li>BigBird通过移动到稀疏注意机制来减少计算，但同时确实也会产生一些其他的成本。</li>
</ul>
<p><strong>在实际上：</strong></p>
<p>BigBird在许多NLP任务上取得了最先进的性能，如问答和长文档分类。</p>
<p>BigBird进一步引入了基于注意机制的DNA上下文语言模型，并对下游任务进行调优，如启动子区域预测和预测非编码变异的效果。</p>
<p>最后，BigBird虽好，但是正如其名，太Big了，这里是指代码量，实现起来实在是比较复杂。</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul>
        <li><strong>标题:</strong> NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读</li>
        <li><strong>作者:</strong> Conqueror712</li>
        <li><strong>创建于
                :</strong> 2023-05-29 12:32:00</li>
        
            <li>
                <strong>更新于
                    :</strong> 2023-05-29 12:33:38
            </li>
        
        <li>
            <strong>链接:</strong> https://redefine.ohevan.com//post/Paper-BigBird.html
        </li>
        <li>
            <strong>
                版权声明:
            </strong>
            
            本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a> 进行许可。
            

        </li>
    </ul>
</div>

            </div>
        

        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                        rel="prev"
                        href="/post/Paper-Transformer.html"
                        >
                            <span class="left arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-left"></i>
                            </span>
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">再回首划时代的技术 - Transformer丨论文解读</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                        rel="next"
                        href="/post/Paper-BERT.html"
                        >
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">2023了，再来看看NLP经典之作 - BERT丨论文解读</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        


        
            <div class="comment-container">
                <div class="comments-container pjax">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;评论
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-swup-reload-script>
        import { init } from 'https://evan.beee.top/js/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

            </div>
        
    </div>

    
        <div class="toc-content-container">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">此页目录</div>
        <div class="page-title">NLP的林子大了，什么「大鸟」都有 - BigBird丨论文解读</div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%B6%E3%80%81%E5%89%8D%E8%A8%80%EF%BC%9A"><span class="nav-text">零、前言：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E8%BF%99%E6%98%AF%E4%BB%80%E4%B9%88%E9%B8%9F%EF%BC%9F%E7%9C%8B%E4%B8%80%E4%B8%8B%EF%BC%9A"><span class="nav-text">一、这是什么鸟？看一下：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E5%8F%A5%E8%AF%9D%E8%A7%A3%E9%87%8A%E3%80%8C%E5%A4%A7%E9%B8%9F%E3%80%8DBigBird%EF%BC%9A"><span class="nav-text">一句话解释「大鸟」BigBird：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BigBird%E5%87%BA%E7%8E%B0%E7%9A%84%E7%9B%AE%E7%9A%84%E5%92%8C%E6%84%8F%E4%B9%89%EF%BC%9A"><span class="nav-text">BigBird出现的目的和意义：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BigBird%E8%83%8C%E6%99%AF%E9%93%BA%E5%9E%AB%E4%B8%8E%E4%BB%8B%E7%BB%8D%EF%BC%9A"><span class="nav-text">BigBird背景铺垫与介绍：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BigBird%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%EF%BC%9A"><span class="nav-text">BigBird相关工作：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E6%9E%97%E5%AD%90%E5%A4%A7%E4%BA%86%EF%BC%8C%E4%BB%80%E4%B9%88%E9%B8%9F%E9%83%BD%E6%9C%89%EF%BC%9A"><span class="nav-text">二、林子大了，什么鸟都有：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%B0%E5%BA%95%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-text">稀疏注意力到底在做什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%9B%8B%E7%99%BD%E8%B4%A8%E6%98%AF%E7%89%9B%E8%82%89%E7%9A%84%E5%85%AB%E5%80%8D%EF%BC%9F"><span class="nav-text">蛋白质是牛肉的八倍？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E8%BE%83%E9%AB%98%E4%B8%8B%EF%BC%9F"><span class="nav-text">一较高下？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#21%E4%B8%96%E7%BA%AA%E6%98%AF%E7%94%9F%E7%89%A9%E7%9A%84%E4%B8%96%E7%BA%AA%EF%BC%9F"><span class="nav-text">21世纪是生物的世纪？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E6%98%BE%E8%BA%AB%E6%89%8B%EF%BC%81Google%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%9A"><span class="nav-text">大显身手！Google搜索引擎：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E9%B8%9F%E5%B8%88%E5%82%85%EF%BC%8C%E4%BD%A0%E6%98%AF%E5%81%9A%E4%BB%80%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84%EF%BC%9A"><span class="nav-text">三、鸟师傅，你是做什么工作的：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%8D%E7%9C%8B%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%9A"><span class="nav-text">再看稀疏注意力：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E9%B8%9F%E5%A4%A7%E9%B8%9F%EF%BC%8C%E5%AE%9E%E6%88%98%E8%A7%81%E5%88%86%E6%99%93%EF%BC%81"><span class="nav-text">大鸟大鸟，实战见分晓！</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BigBird%E7%9A%84%E6%9E%B6%E6%9E%84%E7%9A%84%E7%BB%86%E8%8A%82%E9%83%A8%E5%88%86%EF%BC%9A"><span class="nav-text">BigBird的架构的细节部分：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BD%BF%E7%94%A8%E5%B9%BF%E4%B9%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%9D%A5%E6%8F%8F%E8%BF%B0BigBird%E6%A8%A1%E5%9E%8B%EF%BC%9A"><span class="nav-text">1. 使用广义注意力机制来描述BigBird模型：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Erdos-Renyi%E9%9A%8F%E6%9C%BA%E5%9B%BE%E6%A8%A1%E5%9E%8B%EF%BC%9A"><span class="nav-text">2. Erdos-Rényi随机图模型：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%9B%B8%E9%82%BB%E5%86%85%E7%A7%AF%E4%B8%8E%E8%81%9A%E7%B1%BB%E7%B3%BB%E6%95%B0%EF%BC%9A"><span class="nav-text">3. 相邻内积与聚类系数：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E7%BB%93%E8%AE%BA%E4%B8%8E%E5%B0%BE%E5%A3%B0%EF%BC%9A"><span class="nav-text">四、结论与尾声：</span></a></li></ol>

    </div>
</div>
        </div>
    
</div>



                

            </div>

            

        </div>

        <div class="main-content-footer">
            <footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2022</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Conqueror712</a>
        </div>
        
            <script data-swup-reload-script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">访问人数</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">总访问量</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a> 驱动</span>
            <span class="text-sm lg:block">主题&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.4.4</a></span>
        </div>
        
        
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex justify-center items-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    

</main>


    
<script src="/js/libs/Swup.min.js"></script>

<script src="/js/libs/SwupSlideTheme.min.js"></script>

<script src="/js/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/libs/SwupScrollPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
        ],
        containers: ["#swup"],
    });

    swup.hooks.on("page:view", () => {
        Global.refresh();
    });

    // if (document.readyState === "complete") {
    //
    // } else {
    //     document.addEventListener("DOMContentLoaded", () => init());
    // }
</script>






<script src="/js/utils.js" type="module"></script>

<script src="/js/main.js" type="module"></script>

<script src="/js/layouts/navbarShrink.js" type="module"></script>

<script src="/js/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/layouts/categoryList.js" type="module"></script>





    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>







<div class="post-scripts" data-swup-reload-script>
    
        
<script src="/js/libs/anime.min.js"></script>

        
<script src="/js/tools/tocToggle.js" type="module"></script>

<script src="/js/layouts/toc.js" type="module"></script>

<script src="/js/plugins/tabs.js" type="module"></script>

    
</div>


</body>
</html>
